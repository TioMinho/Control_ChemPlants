% ---------------------------------------------------------------
% Preamble
% ---------------------------------------------------------------
\documentclass[a4paper,11pt]{book}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1in,a4paper,pdftex]{geometry}
\usepackage[protrusion=true,expansion=true]{microtype} 
\usepackage{amsmath,amsfonts,amsthm,amssymb,bm,mathdots,mathtools,bigints}
\usepackage{rotating}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, anchorcolor=blue, citecolor=red]{hyperref}
\usepackage[all]{hypcap}
\usepackage{color, xcolor}
\usepackage{listings}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{cite}
\usepackage{makecell}
\usepackage[printwatermark]{xwatermark}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{tikz, pgfplots}

\usepackage{framed}
\definecolor{myYellow}{rgb}{1,0.65,0}
\definecolor{myRed}{rgb}{0.84, 0.18, 0.13}
\definecolor{myGreen}{rgb}{0, 0.53, 0.27}
\definecolor{myBlue}{rgb}{0, 0.34, 0.91}
\colorlet{shadecolor}{myBlue!7}

\numberwithin{figure}{chapter}
\numberwithin{equation}{chapter}
\numberwithin{table}{chapter}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]

\newwatermark[allpages,color=red!15,angle=45,scale=5,xpos=-1cm,ypos=2cm]{DRAFT}

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\usepackage{graphicx}
\graphicspath{ {imgs/} }

\newbox{\bigpicturebox}

\lstset{
    backgroundcolor=\color[rgb]{0.86,0.88,0.93},
    language=matlab, keywordstyle=\color[rgb]{0,0,1},
    basicstyle=\footnotesize \ttfamily,breaklines=true,
    escapeinside={\%*}{*)}
}

% --------------------------------------------------------------------
% Tikz Macros
% --------------------------------------------------------------------
\usetikzlibrary{shapes,arrows, backgrounds, positioning, fit, decorations.pathmorphing}

\tikzstyle{sectionBlock} = [draw, fill=blue!20, rectangle, minimum height=3em, minimum width=3em]
\tikzstyle{block} = [draw, fill=white, rectangle, minimum height=3em, minimum width=3em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]
\tikzstyle{mcInput} = [draw, rectangle, line width=0.5mm, minimum width=2em, minimum height=2em, fill=black!10]
\tikzstyle{mcCircle} = [draw, circle, line width=0.5mm, minimum width=2em, minimum height=2em, fill=white]

% --------------------------------------------------------------------
% Definitions
% --------------------------------------------------------------------
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}} 

\makeatletter                       
\def\printtitle{
    {\centering \@title\par}}
\makeatother                                    

\makeatletter 
\def\printauthor{
    {\centering \large \@author}}               
\makeatother                            

\newcounter{boxed-theorem}
\makeatletter
\newenvironment{boxed-theorem}[1]
{\begin{shaded} \begin{theorem}{#1}}
{\end{theorem} \end{shaded}}

\newcounter{boxed-definition}
\makeatletter
\newenvironment{boxed-definition}[1]
{\begin{shaded} \begin{definition}{#1}}
{\end{definition} \end{shaded}}

% ---------------------------------------------------------------
% Metadata 
% ---------------------------------------------------------------
\title{ \normalsize \textsc{Course Conclusion Paper - DRAFT} 
        \\[2.0cm]             
        \HRule{0.5pt} \\              
        \LARGE \textbf{\uppercase{Dynamical Modelling and Control of Chemical Reactive Systems}}
        \HRule{2pt} \\[0.5cm]  
}

\author{
        Otacílio Bezerra Leite Neto\\   
        Federal University of Ceará\\  
        Department of Teleinformatics Engineering\\
        \texttt{minhotmog@gmail.com} \\
}

\begin{document}
% ---------------------------------------------------------------
% Maketitle
% ---------------------------------------------------------------
\thispagestyle{empty}       % Remove page numbering on this page

\printtitle                 % Print the title data as defined above
    \vfill
\printauthor                % Print the author data as defined above
\newpage

% ---------------------------------------------------------------
% Begin document
% ---------------------------------------------------------------
% Set page numbering to begin on this page
\thispagestyle{empty}   
\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}
\tableofcontents

% 1 - Introduction
% ---------------------------------------------------------------
\clearpage
\setcounter{page}{1}
\chapter{Introduction}

This chapter presents the main problem in discussion and the basic concepts concerning its formulation and solutions, which are detailed further in the next chapters. This is a work on Control Theory and its application to Chemical Reactive Systems, therefore the discussion will follow the notation common to the literature of this field, and a ``modern" approach to this theory is explored. 

The sections are organized as follows: Section 1.1 provides general definitions for control systems engineering, Section 1.2 discuss chemical reactive systems and its importance in both industry and academia, Sections 1.3 and 1.4 describes the motivation and justification of this work, respectively, and Section 1.5 details the subsequent chapters in this document.

\section{Control Systems Engineering}

The discipline of Control Systems Engineering deals with the design of devices, named \textit{controllers}, that are integrated to a physical system (a \textit{dynamical system}, in most cases) in order to impose a desired behavior to this system. To achieve this goal, the discipline covers topics ranging from applied mathematics, such as dynamical systems theory and signal processing, to a more engineering discussion, regarding instrumentation and implementation of these controllers in a real-life plant or individual system. 

A system, in a broad physical sense, is defined as a ensemble of interacting components that responds to external stimuli producing a determined dynamical response, and whose individual parts are not able to produce the same functionality by their own. Thus, the first essential element in Control Theory is a mathematical model of the system of interest. One such model is the \textit{Input-Output Representation}, as shown in \textbf{fig!!!}, in which an input stimuli, a signal $u(t)$, acts on the system producing an output response, a signal $y(t)$, described by the following differential equation:
\begin{equation} \label{eq:01-01}
\begin{split}
    \alpha_n \cfrac{d^n y(t)}{dt^n} + \alpha_{n-1} & \cfrac{d^{n-1} y(t)}{dt^{n-1}} + \cdots + \alpha_{1} \cfrac{d y(t)}{dt} + \alpha_{0} y(t) = \\
    & \beta_m \cfrac{d^m u(t)}{dt^m} + \beta_{m-1} \cfrac{d^{m-1} u(t)}{dt^{m-1}} + \cdots + \beta_{1} \cfrac{d u(t)}{dt} + \beta_{0} u(t)
\end{split}
.\end{equation}

[fig]

In this representation, the input $u(t)$ is called the \textit{manipulated variable}, since it represents a arbitrary stimuli that can be given directly by human action or a by an automatic controller, while the output $y(t)$ is called the \textit{controlled variable}, since it can only be modified indirectly through $u(t)$. This also leads to a \textit{cause-and-effect} interpretation of the system.

A model can provide a quantitative understanding of the system that is useful both to access some response specifications and to design controllers to modify them based on some requirements. In the case of the model in Equation \eqref{eq:01-01}, it is possible to calculate the response $y(t)$, and its derivatives, resulting from any specific action $u(t)$. Besides, a model can be used to perform computer simulations, in order to visualize the dynamical behavior of the system without actually manipulating it, since real experiments could be expensive or even damage the system. Consider, for instance, a schematic and a simulation for a model representing a mass-spring-damper system, shown at \textbf{!!!!}.

[fig]

In this simulation, the \textit{rise time}, \textit{peak time}, \textit{overshoot ratio} and \textit{steady-state value} are examples of response specifications that can be defined to describe the system behavior to a external stimuli (in this case, a constant force of unit magnitude). These specified parameters are characteristic to responses of a class of systems known as \textit{underdamped second-order systems}, that will be discussed further in the document. 

A controller is used to calculate, for a time $t \in \left[ t_0, t_N\right)$, the necessary input $u(t)$ to produce an output $y(t)$ as close as possible to a desired reference signal $r(t)$. There are two common configurations, shown in \textbf{Fig!!!}, of how to connect the controller to the system.

[fig]

The configuration in \textbf{Fig!!!.a}, known as \textit{Open-Loop Controller}, calculates the action as a function $u(t) = \pi(r, t)$, given an initial condition $y(t_0) = y_0$. In this case, the controller does not observe the output $y(t)$, and relies on the model to guarantee that the system is driven to the reference. Of course, if there are any external disturbances acting on this configuration, or if the model is not reliable enough, it is not possible to guarantee that the requirements are met. Thus, these type of controllers are not suitable for critical applications, and its use is restricted to systems where deviance from the desired reference can be tolerated. \textbf{[exemplos de aplicações?]}

In contrast, the configuration in \textbf{Fig!!!.b}, known as \textit{Closed-Loop Controller} or \textit{Feedback Controller}, calculates the action as a function $u(t) = \pi(e, t)$, where $e(t) = r(t) - y(t)$ is the error between the reference and the actual response. Now, the controller will observe the system output, trough some sensor device, and compares it to the desired reference in order to calculate a \textit{corrective action}. This feedback property can make the system reject disturbances while still driving it to the desired reference. Thus, the Feedback Controller became the most popular choice of controller configuration in industry for a wide range of applications, even for critical ones.  \textbf{[exemplos de aplicações? + references]}

\section{Chemical Reactive Systems}

A chemical reaction, the transformation of a chemical substance into another, is a process central to chemistry and to nature itself. A reaction equation is a intuitive representation of such transformations. For instance, consider the following equation representing a \textit{synthesis reaction}:
\begin{equation}
    A + 2 B \longrightarrow 3 C + D 
.\end{equation} 

In this equation, the compounds $A$ and $2 B$ forms the set of \textit{reactants}, $\mathbb{R}$, while $3 C$ and $D$ forms the set of \textit{products}, $\mathcal{P}$. The coefficients in such equations are the \textit{stoichiometric numbers}, providing an information about proportionality between the quantity of each substances in the reaction. 

Usually the products can be directly used as reactants in another reaction, in which case they can also be referred as a intermediate product (or byproduct), and the equations can be appended in a ``series" representation. In this case, each $k$-th intermediate product forms a set $\mathcal{I}_k$. In addition to a chain of series reactions, there is also the possibility of different reactions to occur in parallel, in the same system. The combination of these sets of reactants, byproducts, products and reactions are often referred as a \textit{chemical reaction network}, and the associated equation can be represented in general form as:
\begin{equation} \label{eq:chemNetwork}
\left\{ \begin{matrix}
    \mathbb{R}^{(1)}  & \longrightarrow & \mathcal{I}^{(1)}_1  &  \longrightarrow & \cdots & \longrightarrow & \mathcal{I}^{(1)}_{M_1} & \longrightarrow & \mathcal{P}^{(1)} \\
    \mathbb{R}^{(2)} & \longrightarrow & \mathcal{I}^{(2)}_1  &  \longrightarrow & \cdots & \longrightarrow & \mathcal{I}^{(2)}_{M_2} & \longrightarrow & \mathcal{P}^{(2)} \\
    \vdots &  & \vdots &  & \vdots &  & \vdots &  & \vdots \\
    \mathbb{R}^{(N)} & \longrightarrow & \mathcal{I}^{(N)}_1  &  \longrightarrow & \cdots & \longrightarrow & \mathcal{I}^{(N)}_{M_N} & \longrightarrow & \mathcal{P}^{(N)} \\
\end{matrix} \right.
.\end{equation} 

Moreover, chemical reactions displays a dynamical behavior concerning the speed at which a reaction occurs. This rate of reaction, its \textit{kinetics}, are dependent on the conditions in the environment, such as temperature and pressure, and on some properties of the reaction itself. In the case of a \textit{isothermal process}, i.e., when the temperature in the environment remains constant, this rate can be calculated as a constant $K$, leading to a representation on the form:
\begin{equation}
    \mathbb{R} \overset{K}{\longrightarrow} \mathcal{P}
.\end{equation} 

When the temperature in the environment is not constant, the process is said to be \textit{endothermic} or \textit{exothermic} if, respectively, it consumes or produces energy. The kinetics of the reactions in such processes are usually functions of the temperature which, given an activation energy $E$, are assumed to follow the Arrhenius equation:
\begin{equation} \label{eq:arrhenius1}
    K(T) = K_0 e^{-E / T}
.\end{equation}

In practice, these chemical reactions are produced by mixing the reactants in some environment with adequate conditions. In order to control the quantities of these substances, actual processes consists in a manipulation of the concentrations of reactants in some container, usually by providing a mass flow of these substances through some fluid. A major interest is to manipulate the reactants in some way to produce a desired concentration of one or more products in the chemical reaction network, allowing this problem to be addressed by a control engineering perspective. A \textit{chemical reactor system}, depicted in \textbf{Fig!!!}, is a system where a controller can manipulate the concentration of some reactants to produce a desired concentration of some products.

[img]

When the process is not isothermal, the occurrence of a reaction contributes to the entropy of the environment, and consequently affects the kinetics of the subsequent ones. To compensate for this, practical applications also try to control the conditions in the environment using instruments external to the reactions themselves. Because of the use of the Arrhenius equation to model these reaction rates, this control is usually implemented through a cooling or heating system coupled to the original reactor system, resulting in the schematic on \textbf{Fig!!!}.

[img]

\section{Motivation}

The use of automatic controllers to impose a desired behaviour to physical systems is a practice ubiquitous in many engineering fields. In the last years, the price of digital computers have been dropping while their performance have been growing. Consequently, digital controllers have became the central key in developments in important and innovative fields such as aeronautics \textbf{[reference]} and autonomous driving \textbf{[reference]}. In parallel, this theory is also useful to understand and bring inspiration from nature itself since, for instance, the mechanisms for temperature regulation observed in vertebrate animals behave as a feedback controller \cite{Heller:1978}.

Most recent developments in Control Theory focus on using Feedback Controllers to achieve \textit{Robust and Optimal Control}. This theory accounts for the design of controllers that deals with uncertainty, either from the model or from the observation of the system, and are able to achieve the control objectives in a \textit{optimal manner}. Despite being a few decades old, these fields have gained a lot of interest in the last years thanks to recent results in \textit{Machine Learning}, particularly in \textit{Reinforcement Learning}, that are having success in using optimization techniques for artificial agents to control themselves in environments loaded with uncertainty \textbf{[reference]}.

Furthermore, the specific application of controlling chemical reactor systems brings benefits from the fact that chemical reactions are present in most biological and industrial processes. In this sense, controllers can be used to guarantee safety constraints, maximize productivity and minimize the use of resources, in such way that is unfeasible without automatic and high performing machinery.

\section{Objectives}

This works aims to provide a self-contained discussion of modelling and control of chemical reactive systems in the perspective of modern control theory. Therefore, the results are focused on \textit{state feedback controllers} modelled in continuous and discrete time, but analysis in the frequency domain is also considered in order to explain some concepts. Several properties of these models, both in the open-loop and closed-loop regime, are summarized in the document and the intention is to have a generalized framework to understand, evaluate and design those systems. Finally, the theory of more advanced methods such as optimal estimation and optimal control is also developed in the same sense.

\section{Chapters Organization}

The chapters of this document are mainly organized in two parts. The first part, comprised by the chapters 2, 3 and 4, builds the necessary theoretical background and provides the mathematical framework for the applications. The second part, comprised by the chapters 5 and 6, describes the experiments and results of applying these methods in real-world applications.

Individually, the chapters are organized as follows: chapter 2 introduces the dynamical models and its several properties with respect to the real system behavior, chapter 3 discusses classical methods in developing automatic controllers and state observers, chapter 4 presents more advanced methods in optimal estimation and optimal control, chapter 5 describes the practical experiments used to validate the previous discussions, chapter 6 summarizes and discusses the results of the experiments and evaluate the several controllers performances and, finally, chapter 7 provides the conclusion of the document and possible future works.

% 2 - Dynamical System Modelling
% ---------------------------------------------------------------
\clearpage
\chapter{Dynamical System Analysis}

This chapter discusses the mathematical models for dynamical systems and their use in response analysis. The sections starts by introducing a procedure to build models from physical principles and presenting equivalent common representations. Next, the response of systems, in a time domain, are analyzed in the light of such models, relating the mathematical structure with the dynamical behavior. Finally, some important properties are defined and proved using these formulations and the system response in a frequency domain is also presented.

\section{Model from First Principles}

A dynamical system is a physical system whose states evolves with time. For this reason, one can represent a dynamical system using the \textit{first principles} from physics itself, and formulate the evolution in time by calculating the rate of change of the states in respect to time. Thus, dynamical models can be equated using differential equations with time derivatives. 

A straightforward procedure to model a system consists of identifying the variables of interest and relate them using conservation laws, such as conservation of mass, conservation of energy or conservation of momentum. The resulting differential equations are in the form:
\begin{equation} \label{eq:massCons01}
    \begin{pmatrix}
        \text{Rate of} \\ \text{Mass/Energy/Momentum} \\ \text{Accumulation}
    \end{pmatrix} = \begin{pmatrix}
        \text{Mass/Energy/Momentum} \\ \text{entering the System}
    \end{pmatrix} - \begin{pmatrix}
        \text{Mass/Energy/Momentum} \\ \text{leaving the System}
    \end{pmatrix}
.\end{equation}

The choice of which conservation law to use depends on the system itself, since the variables of interest can provide dynamics to the system in many forms. Usually, conservation of mass is used to relate dynamics of concentrations and volumes, or other material variables, while conservation of momentum is often used to relate dynamics of motion. Since energy can be converted on form, the conservation laws of this quantity can be used to model several dynamics, such as the rate of change in heat, electrical charges or velocity of a system.

In the case of a chemical reactor system, the variables of interest are the concentrations of the chemical substances in the system. Hence, the rate of accumulation of a substance can be represented using the mass conservation law, or mass balance:
\begin{equation} \label{eq:massCons02}
\begin{split}
    \begin{pmatrix}
        \text{Accumulation} \\ \text{of mass} \\ \text{in the system}
    \end{pmatrix} &= 
    \begin{pmatrix}
        \text{Mass} \\ \text{entering} \\ \text{the System}
    \end{pmatrix} - \begin{pmatrix}
        \text{Mass} \\ \text{leaving} \\ \text{the System}
    \end{pmatrix} \\
    &= \left[ \begin{pmatrix}
        \text{Mass flow} \\ \text{entering} \\ \text{System}
    \end{pmatrix} + \begin{pmatrix}
        \text{Mass} \\  \text{produced} \\ \text{by reactions}
    \end{pmatrix} \right] - \left[ \begin{pmatrix}
        \text{Mass flow} \\ \text{leaving} \\ \text{System}
    \end{pmatrix} + \begin{pmatrix}
        \text{Mass} \\ \text{consumed} \\ \text{by reactions}
    \end{pmatrix} \right]
\end{split}
.\end{equation}

\begin{boxed-theorem}{(Mass Balance of Reactors)} \label{th:isoReactSys01}
    Consider a closed isothermal reactor system comprised of a diluted solution of constant volume $V$, whose reactions are described by a chemical reaction network as in Definition \ref{eq:chemNetwork}. The change of concentration for any compound $A$ in this reactor is described by the dynamical model:
    \begin{equation} \label{eq:isoModel1}
            \cfrac{d (\rho_A)}{dt} = q (\rho^{(A)}_{in} - \rho^{(A)}_{out}) + \left( \sum_{\alpha X \rightarrow \beta A} \cfrac{1}{\beta} K_{XA} (\rho_X)^{\alpha} \right) - \left(\sum_{\alpha A \rightarrow \beta X} \cfrac{1}{\beta} K_{AX} (\rho_A)^{\alpha} \right)
    ,\end{equation}

    \noindent where $\rho^{(A)}_{in}$ and $\rho^{(A)}_{out}$ are the densities of $A$ in the flows entering and leaving the system, respectively, and where $\alpha X \rightarrow \beta A$ and $\alpha A \rightarrow \beta X$ represents the reactions in the network between $A$ and any other compound $X$ with density $\rho_X$, each occurring with kinetic rates $K_{XA}$ and $K_{AX}$, respectively.
\end{boxed-theorem}

\begin{proof}
    First of all, as denoted in \eqref{eq:massCons02}, the mass flow of any substance $A$ entering and leaving the system, $M_{in}$ and $M_{out}$, respectively, given a fluid inflow $F_{in}$ with density $\rho^{(A)}_{in}$ and a fluid outflow $F_{out}$ with $\rho^{(A)}_{out}$, can be calculated as:
    \begin{equation}
        \begin{matrix}
            M_{in} = \rho^{(A)}_{in} F_{in} & & M_{out} = \rho^{(A)}_{out} F_{out}
        \end{matrix}
    .\end{equation}
    
    To calculate the mass contribution from the reactions, it is necessarily first to formulate the mass contribution for a single reaction. In this case, consider a reaction between two chemical compounds $X$ and $Y$, with stoichiometric numbers $\alpha$ and $\beta$:
    \begin{equation} \label{eq:simpleEq01}
        \alpha X \overset{K_{XY}}{\longrightarrow} \beta Y
    .\end{equation}
    
    Under the assumption that the reactant is in a dilute solution, the rate of this equation obeys the \textit{law of mass action} \cite{Horn:1972}. Given a constant kinetic rate $K_{XY}$, since the system is isothermal, and the volume of the solution as $V$, the mass of $X$ consumed, $M^{(X)}_\text{cons}$, and the mass of $Y$ produced, $M^{(Y)}_\text{prod}$, are given by the power-laws:
    \begin{equation}
        \begin{matrix}
            M^{(X)}_\text{cons} = \cfrac{V}{\beta} K_{XY} (\rho_X)^{\alpha} & & M^{(Y)}_\text{prod} = \cfrac{V}{\alpha} K_{XY} (\rho_X)^{\alpha}
        \end{matrix}
    ,\end{equation}
    
    \noindent where $\rho_X$ and $\rho_Y$ are the respective densities of these compounds. Assuming that the network represents a set of reactions occurring within an chemical solution of volume $V$, the mass of a substance $A$ that is consumed and produced by the reactions, named respectively $M_\text{cons}$ and $M_\text{prod}$, are given by summing over the contribution of each reaction on the network where $A$ is either a reactant or a product to any other compound $X$:
    \begin{equation}
            M_\text{cons} = V \sum_{\alpha A \rightarrow \beta X} \cfrac{1}{\beta} K_{AX} (\rho_A)^{\alpha}\ \ \ \ \  M_\text{prod} = V \sum_{\alpha X \rightarrow \beta A} \cfrac{1}{\beta} K_{XA} (\rho_X)^{\alpha}
    .\end{equation}
    
    Finally, packing all together, the mass balance of any substance $A$ in a isothermal chemical reactive system can be represented by the general dynamical model:
    \begin{equation}
    \begin{split}
        \begin{pmatrix}
        \text{Accumulation} \\ \text{of mass} \\ \text{in the system}
    \end{pmatrix} &= \left[ \begin{pmatrix}
        \text{Mass flow} \\ \text{entering} \\ \text{System}
    \end{pmatrix} + \begin{pmatrix}
        \text{Mass} \\  \text{produced} \\ \text{by reactions}
    \end{pmatrix} \right] - \left[ \begin{pmatrix}
        \text{Mass flow} \\ \text{leaving} \\ \text{System}
    \end{pmatrix} + \begin{pmatrix}
        \text{Mass} \\ \text{consumed} \\ \text{by reactions}
    \end{pmatrix} \right] \\
        \cfrac{d (\rho_A V)}{dt} &= \left[ \rho^{(A)}_{in} F_{in} + V \sum_{\alpha X \rightarrow \beta A} \cfrac{1}{\beta} K_{XA} (\rho_X)^{\alpha} \right] - \left[ \rho^{(A)}_{out} F_{out} + V \sum_{\alpha A \rightarrow \beta X} \cfrac{1}{\beta} K_{AX} (\rho_A)^{\alpha} \right]
    \end{split}
    .\end{equation}
    
    Since the system is closed, i.e., there are no leaks or unknown sources of fluids, the assumptions of a constant volume implies that $F_{in} = F_{out} = F$. Normalizing each term by the volume and substituting a new variable $q = F/V$ results in:
    \begin{equation} \label{eq:isoModel3}
            \cfrac{d (\rho_A)}{dt} = q (\rho^{(A)}_{in} - \rho^{(A)}_{out}) + \left( \sum_{\alpha X \rightarrow \beta A} \cfrac{1}{\beta} K_{XA} (\rho_X)^{\alpha} \right) - \left(\sum_{\alpha A \rightarrow \beta X} \cfrac{1}{\beta} K_{AX} (\rho_A)^{\alpha} \right)
    .\end{equation}
\end{proof}

Notice some important restrictions to the use of the model just presented. First of all, to calculate the mass contribution of an individual reaction was necessary to use a model which assumes that the reactor system is actually comprised of a dilute solution in some closed container. In industry, this means that the reactor system is actually a tank containing the solution. The inflow and outflow of fluid can be represented by flows through pipes which can be manipulated by some valve or pump. An illustration of such physical system is exhibited at Fig. \ref{fig:tank01a}.

Furthermore, this model accounts for a single substance $A$, but the system is actually a solution of several compounds, each one with a specific concentration. From the model presented, it is visible that it is necessary to compute each concentration $\rho_X$ before actually computing the rate of change in $\rho_A$. However, from the same model, the computation of the rate of change of any $\rho_X$ may depend on $\rho_A$ itself. Therefore, the change of concentration inside the whole system is actually the result of a system of differential equations:
\begin{align}   \label{eq:isoSys01}
\begin{cases}
    \hfill \cfrac{d (\rho_{X_1})}{dt} &= f(\rho_{X_1}, \rho_{X_2}, ..., \rho_{X_n}, \rho_{in}^{(X_1)}, \rho_{out}^{(X_1)}, t) \\
    \hfill \cfrac{d (\rho_{X_2})}{dt} &= f(\rho_{X_1}, \rho_{X_2}, ..., \rho_{X_n}, \rho_{in}^{(X_2)}, \rho_{out}^{(X_2)}, t) \\
    & \vdots   \\
    \hfill \cfrac{d (\rho_{X_n})}{dt} &= f(\rho_{X_1}, \rho_{X_2}, ..., \rho_{X_n}, \rho_{in}^{(X_n)}, \rho_{out}^{(X_n)}, t)
\end{cases}
,\end{align}

\noindent where $X_1, X_2, ..., X_n$ are the chemical compounds inside the reactor and $f(\cdot)$ is the dynamical model presented in Theorem \ref{th:isoReactSys01}.

\begin{figure}[ht] 
    \centering
    \begin{subfigure}{0.49\textwidth}   
        \includegraphics[width=\textwidth]{chapter2/tank01}
        \caption{$\ \ \ \ \ $}  \label{fig:tank01a}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}   
        \includegraphics[width=\textwidth]{chapter2/tank02}
        \caption{$\ \ \ \ \ $} \label{fig:tank01b}
    \end{subfigure}
    
    \caption{Schematic representations of industrial reactor tanks for (a) a simple isothermal process and (b) a non-isothermal process with heating/cooling system.} 
    \label{fig:tank01}
\end{figure}

The model discussed so far is very simple and can describe several applications. To account for more complex systems, the same modeling procedure can be applied. For instance, it is possible to extend the description to account for exothermic and endothermic processes, when the temperature inside the system has a dynamical evolution and the dynamics of the reactions starts to depend on it.

When discussing non-isothermal processes, it is also common to discuss heating or cooling systems that tries to impose certain operational conditions to the reactions, as illustrated in Fig. \ref{fig:tank01b}. In exothermic processes, for instance, the heat accumulated in the system tends to grows as the reactions occurs, which can be very dangerous. One approach to regulate the temperature consists in involving the chemical solution, or the container containing it, with a material whose temperature can be manipulated, transferring or absorbing heat by conductance. The temperature of this material can be manipulated by, for instance, running a heated fluid or converting electrical energy to heat energy.

\begin{boxed-theorem}{(Non-isothermal Reactor System)} \label{th:exoReactSys01}
    Consider a closed non-isothermal reactor system comprised of a diluted solution of constant volume $V$. Assume, also, that the system is involved by a cooling/heating system with capacity $Q$ and that the reactions obeys the Arrhenius equation. The change of concentration for any compound $A$ in this reactor is described by the dynamical model:
    \begin{equation} \label{eq:exoModel1}
            \cfrac{d (\rho_A)}{dt} = q (\rho^{(A)}_{in} - \rho^{(A)}_{out}) + \left( \sum_{\alpha X \rightarrow \beta A} \cfrac{1}{\beta} K_{XA} e^{-\frac{E_{XA}}{T}} (\rho_X)^{\alpha} \right) - \left(\sum_{\alpha A \rightarrow \beta X} \cfrac{1}{\beta} K_{AX} e^{-\frac{E_{AX}}{T}} (\rho_A)^{\alpha} \right)
    ,\end{equation}

    \noindent where $E_{XA}$ and $E_{AX}$ are the activation energy needed for each reaction, and the rest of the parameters are the same as defined in Theorem \ref{th:isoReactSys01}. Furthermore, the change of temperature inside the reactor system, $T$, and in the cooling/heating system, $T_C$, are described by the dynamical models:
    \begin{align} \label{eq:exoModel2}
    \begin{cases}
        \cfrac{d(T)}{dt} = q(T_{in} - T_{out}) + \eta (T_C - T) + \delta \sum_{\alpha A \rightarrow \beta X} K_{AX} e^{-\frac{E_{AX}}{T}} (\rho_A)^{\alpha} \Delta H_{AX} \\
        \cfrac{d(T_C)}{dt} = \gamma Q + \beta (T - T_C) 
    \end{cases}
    ,\end{align}
    
    \noindent where $T_{in}$ and $T_{out}$ are the temperatures of the fluid inflow and outflow, respectively, $\Delta H_{AX}$ is the energy change from each reaction $\alpha A \rightarrow \beta X$ and $\eta, \delta, \gamma, \beta \in \mathbb{R}$ are proportionality parameters specific to the system and environmental conditions.
\end{boxed-theorem}

A proof of this theorem can be found in Appendix A. The non-isothermal reactor system is a more general model that accounts for the fact that the temperature of the environment is usually not constant. From this assumption, the flow entering and leaving the system are also not assumed to have the same temperature that the fluid inside the reactor. In practical applications, the temperature of the fluid inflow can either be manipulated or measured, where the temperature of the fluid outflow is actually assumed to be equal to the temperature inside the reactor. In addition, the proportionality constants are not functions of any dynamical variable, so they can be calculated before the operation of the system by using the properties of the materials and containers.

In the case of this model, the rate of changes in the chemical concentrations depends on the temperature through the Arrhenius equation. However, the temperature of the reactor itself depends on those concentrations. So, as noted in Equation \eqref{eq:isoSys01}, the dynamical model of the entire reactor is a system of differential equations relating all those quantities.


\section{Mathematical Models of Systems}

The last section presented the foundation for modeling a dynamical system using first principles from physics. Although it was a well-defined formulation, the resulting models are not guaranteed to be practical in a mathematical sense. This is to due to the fact that the differential equations, as evidenced in Equation \eqref{eq:exoModel1}, are usually nonlinear functions of the variables of interest, and the analysis of such functions are quite more challenging. In the perspective of control theory, that are two main formats for the model of a system: the \textit{Input-Output} (IO) and the \textit{State-Space} (SS) representations.

\begin{boxed-definition}{(Input-Output Representation)} \label{def:IORepr01}
    An Input-Output (IO) representation of a dynamical system with $p \geq 1$ output variables, represented by $\bm{y} : \mathbb{R} \rightarrow \mathbb{R}^{p}$, and $r \geq 1$ input variables, represented by $\bm{u} : \mathbb{R} \rightarrow \mathbb{R}^{r}$, is the system of differential equations:
    \begin{equation} \label{eq:IERepr01}
    \begin{cases}
        h_1 \left( y_1, \dot{y_1}, ..., y_1^{(n_1)}, u_1, \dot{u}_1, ..., u^{(m_{11})}_1, u_2, \dot{u}_2, ..., u^{(m_{12})}_2, ..., u_r, \dot{u}_r, ..., u^{(m_{1r})}_r, t   \right) = 0 & \\
        h_2 \left( y_2, \dot{y_2}, ..., y_2^{(n_2)}, u_1, \dot{u}_1, ..., u^{(m_{21})}_1, u_2, \dot{u}_2, ..., u^{(m_{22})}_2, ..., u_r, \dot{u}_r, ..., u^{(m_{2r})}_r, t   \right) = 0 & \\
         \vdots \hfill & \\
        h_p \left( y_p, \dot{y_p}, ..., y_p^{(n_p)}, u_1, \dot{u}_1, ..., u^{(m_{p1})}_1, u_2, \dot{u}_2, ..., u^{(m_{p2})}_2, ..., u_r, \dot{u}_r, ..., u^{(m_{pr})}_r, t   \right) = 0 &
    \end{cases}
    ,\end{equation}
    
    \noindent where:
    \begin{equation*}
    \begin{matrix}
        \dot{y}(t) = \cfrac{d y(t)}{dt}, & \ddot{y}(t) = \cfrac{d^2 y(t)}{dt^2}, & \cdots, & y^{(n)}(t) = \cfrac{d^n y(t)}{dt^n}
    \end{matrix}
    \end{equation*}

	\noindent and    
    \begin{equation*}
    \begin{matrix}
        \dot{u}(t) = \cfrac{d u(t)}{dt}, & \ddot{u}(t) = \cfrac{d^2 u(t)}{dt^2}, & \cdots, & u^{(n)}(t) = \cfrac{d^n u(t)}{dt^n}
    \end{matrix}
    .\end{equation*}
\end{boxed-definition}

An input-output representation is a simple model that describes the entire system using only two types of variables, and their derivatives. Therefore, the dimension of these variables and the order of derivatives at each differential equation provides the information about the structure of the model. For instance, in the case where $p = r = 1$ the system can be classified as a \textit{Single-Input Single-Output} (SISO) configuration, whereas it is classified as a \textit{Multiple-Input Multiple-Output} (MIMO) configuration if $p,r > 1$. 

This model presents a cause-and-effect interpretation of the system where the direct relationship between the input and output signal, and its derivatives, are equated as if the system was a processing unit. In practice, the input signals $\bm{u}(t)$ are the manipulated variables of the system, where the output signals $\bm{y}(t)$ are the observations of the controlled variables. This representation brings an easy visualization on how a desired system behavior can be achieved by applying a specific input signal, posing as a practical framework for designing controllers. 

\begin{boxed-definition}{(State-Space Representation)} \label{th:SSRepr01}
    A State-Space (SS) representation of a system with $n \geq 1$ states variables, represented by $\bm{x} : \mathbb{R} \rightarrow \mathbb{R}^{n}$, for $p \geq 1$ output variables, represented by $\bm{y} : \mathbb{R} \rightarrow \mathbb{R}^{p}$, and $r \geq 1$ input variables, represented by $\bm{u} : \mathbb{R} \rightarrow \mathbb{R}^{r}$, is given by the systems of state and output equations:
    \begin{align} \label{eq:SSRepr01}
    \begin{split}
    \textbf{State Equations:}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \hfill & \textbf{ Output Equations:} \hfill \\
    \begin{cases}
        \dot{x}_1(t) = f_1(x_1, ..., x_n, u_1, ..., u_r, t) \\
        \dot{x}_2(t) = f_2(x_1, ..., x_n, u_1, ..., u_r, t) \\
        \vdots \\
        \dot{x}_n(t) = f_n(x_1, ..., x_n, u_1, ..., u_r, t)
    \end{cases} \hfill & \begin{cases}
        y_1(t) = g_1(x_1, ..., x_n, u_1, ..., u_r, t) \\
        y_2(t) = g_2(x_1, ..., x_n, u_1, ..., u_r, t) \\
        \vdots \\
        y_p(t) = g_p(x_1, ..., x_n, u_1, ..., u_r, t) \hfill
    \end{cases}
    \end{split}
    ,\end{align}
    
    \noindent or, in the matrix form:
    \begin{align} \label{eq:SSRepr02}
    \begin{cases}
        \dot{\bm{x}}(t) = \bm{f}(\bm{x}(t), \bm{u}(t), t) \\
        \bm{y}(t) = \bm{g}(\bm{x}(t), \bm{u}(t), t)
    \end{cases}
    .\end{align}
\end{boxed-definition}

The State-Space representation is yet another formulation for a dynamical model, but centered in the concept of \textit{state variables}. In a formal definition, the set of state variables is the smallest set of linearly independent variables that can unequivocally determine the value of all the states variables given an initial state $\bm{x}(t_0)$ and a forcing function $\bm{u}(t)$, for any time $t \geq t_0$. In a physical perspective, however, these variables accounts for quantities that can describe the dynamics of the system, such as position or velocities, or they are latent variables that somehow stores intrinsic information about the system behavior. In comparison to the Input-Output representation, this formulation poses a simpler mathematical model, since it is composed by a system of ordinary differential equations and a system of algebraic equations. However, this model presents a semantical improvement over the latter since the inclusion of the state variables expands the internal description of the system. 

\begin{figure}[ht] 
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering  
        \includegraphics[scale=0.7]{chapter2/model01}
        \caption{}  \label{fig:model01a}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}   
    \centering
        \includegraphics[scale=0.7]{chapter2/model02}
        \caption{} \label{fig:model01b}
    \end{subfigure}
    
    \caption{Graphical interpretation of (a) Input-Output models and (b) State-Spaces models.} 
    \label{fig:model1}
\end{figure}

An graphical illustration of both representations is shown at Fig. \ref{fig:model1}. In the light of these formulations, a system can be classified in respect to the model mathematical structure. There are five main properties used for this classification: if the system is causal or non-causal, linear or nonlinear, dynamical or instantaneous, time-invariant or time-varying and with or without delay. The necessary and sufficient conditions for each one of these properties are summarized in Table \ref{table:classes01}.

\begin{table}[!ht]
    \centering
    \begin{tabular}{r | c | c }
     & \textbf{Input-Output} & \textbf{State-Space} \\
    \hline 
        \textbf{Causal}         & \makecell{$m_{ij} \leq n_k$\\$i \in [1,..., p],\ j \in [1,...,r]$} & Always causal \\
    \hline 
        \textbf{Linear}         & \makecell{$h_i(\cdot) = \sum_{j=0}^{n_i} y^{(j)} + ... \ \ \ \ \ \ \ \ $\\$ \ \ \ \ \ \ \ \ \ \ \ \ ... + \sum_{k=1}^{r} \sum_{l=0}^{m_{ik}} u_k^{(l)}$\\$i \in [1,2,...,p]$} & \makecell{$f_i = \bm{a}_i(t) \bm{x}(t) + \bm{b}_i(t) \bm{u}(t), i = 1,2,...,n$ \\ $g_j = \bm{c}_j(t) \bm{x}(t) + \bm{d}_j(t) \bm{u}(t), j = 1,2,...,p$ \\ $\bm{a}_i,\bm{c}_j \in \mathbb{R}^{1 \times n}$ and $\bm{b}_i, \bm{d}_j \in \mathbb{R}^{1 \times r}$} \\
    \hline 
        \textbf{Dynamical}      & \makecell{$n_i > 0$ or $m_{jk} > 0$\\$i, j \in [1,...,p],\ k \in [1,...,r]$} & $n > 0$ \\
    \hline 
        \textbf{Time-Invariant} & \makecell{$h_i(y_i(t),...,u_1(t),...,u_r(t)) = 0$\\$i \in  [1,2,...,p]$} & \makecell{$\dot{\bm{x}}(t) = \bm{f}(\bm{x}(t), \bm{u}(t))$ \\ $\bm{y}(t) = \bm{g}(\bm{x}(t), \bm{u}(t))$} \\
    \hline 
        \textbf{Without-Delay}  & \makecell{All the signals share\\the same arguments} & \makecell{All the signals share\\the same arguments} \\
    \end{tabular} 
    \caption{Necessary and sufficient conditions for different classes of models.}
    \label{table:classes01} 
\end{table} 

This work focus on dynamical linear systems, since its models are the most well studied in the control theory community. In reality, a physical system is always causal, nonlinear and time-varying \cite{Vidyasagar:2002}, but the models can be assumed differently with fairly accuracy. The benefit of linear systems is that it obeys the superposition principle, and a linear combination of the inputs directly causes the exact same linear combination of the individual outputs. Under the assumption of a linear system, a nice result is that the vectorial functions $\bm{f}(\cdot)$ and $\bm{g}(\cdot)$ of the State-Space representation in \eqref{eq:SSRepr03} reduces to simple matrix forms.

\begin{boxed-definition}{(Linear State-Space Representation)}
    A State-Space representation describing a linear system with state vector $\bm{x}(t) : \mathbb{R} \rightarrow \mathbb{R}^{n}$, output vector $\bm{y}(t) : \mathbb{R} \rightarrow \mathbb{R}^{p}$ and input vector $\bm{u}(t) : \mathbb{R} \rightarrow \mathbb{R}^{r}$ is given by the system of equations: 
    \begin{align} \label{eq:SSRepr04}
    \begin{cases}
        \dot{\bm{x}}(t) = \bm{A}(t) \bm{x}(t) + \bm{B}(t) \bm{u}(t) & \\
        \bm{y}(t) = \bm{C}(t) \bm{x}(t) + \bm{D}(t) \bm{u}(t) &
    \end{cases}
    ,\end{align}

    \noindent where $\bm{A}(t) : \mathbb{R} \rightarrow \mathbb{R}^{n \times n}$, $\bm{B}(t) : \mathbb{R} \rightarrow \mathbb{R}^{n \times r}$, $\bm{C}(t) : \mathbb{R} \rightarrow \mathbb{R}^{p \times n}$ and $\bm{D}(t) : \mathbb{R} \rightarrow \mathbb{R}^{p \times r}$. In the case of a time-invariant linear system, these matrices becomes constants. 
\end{boxed-definition}

This formulation has the advantages that the time response of the system can be easily calculated and that the analysis of the dynamics follows well-established results from linear algebra applied to the matrices $\bm{A}(t)$, $\bm{B}(t)$, $\bm{C}(t)$ or $\bm{D}(t)$, as well as for the vectors $\bm{x}(t)$ and $\bm{u}(t)$. Furthermore, the physical interpretation of the system through the state variables becomes straightforward in this model, even in the case of latent variables. 

In addition to the State-Space representation, the linear assumption also benefits Input-Output representations. One major analytical tool that can be used in these cases is to transform this model to a frequency domain, using a linear transform operator, in order to simplify the solution for the differential equations. The most popular choice of transformation is the \textit{Laplace transform}, $\mathcal{L}\{ h(t) \}$, which converts functions in time to functions in complex frequencies. Using the properties of this operator, differential equations are converted to simple algebraic equations.

\begin{boxed-theorem}{(Transfer Function)} \label{th:transFun01}
    Given a linear model for a SISO system, with initial conditions $\bm{y}(0^-) = \bm{u}(0^-) = \bm{0}$, in the Input-Output formulation:
    \begin{equation}
    \begin{split}
        \alpha_n \cfrac{d^n y(t)}{dt^n} + \cdots + \alpha_{1} \cfrac{d y(t)}{dt} + \alpha_{0} y(t) &= \beta_m \cfrac{d^m u(t)}{dt^m} + \cdots + \beta_{1} \cfrac{d u(t)}{dt} + \beta_{0} u(t)
    \end{split}
    .\end{equation}
    
    Its transfer function, in the Laplace domain, is calculated as:
    \begin{equation} \label{eq:transFun01}
         G(s) = \cfrac{Y(s)}{U(s)} = \cfrac{\beta_m s^m + \beta_{m-1} s^{m-1} + \cdots + \beta_{1} s + \beta_0}{\alpha_n s^n + \alpha_{n-1} s^{n-1} + \cdots + \alpha_{1} s + \alpha_0}
    .\end{equation}
\end{boxed-theorem}

An indirect result of this is that the SS representation can be converted to the IO representation using the Laplace transform operator, leading to a notion of equivalence between the two representations. Notice that the extension to the MIMO case is straightforward: just compute the transfer function between each pair of input and output, leading to the matrix $\bm{G} \in \mathbb{C}^{n \times m}$.

\begin{boxed-theorem}{(Passage from SS to IO)} \label{th:SSToIO}
    Consider a linear and time-invariant system in State-Space form with initial states $\bm{x}(0^-) = \bm{0}$ and represented as:
    \begin{align}
    \begin{cases}
        \dot{\bm{x}}(t) = \bm{A} \bm{x}(t) + \bm{u}(t) \\
        \bm{y}(t) = \bm{C} \bm{x}(t) + \bm{D}(t) \\
    \end{cases}     
    .\end{align}
    
    The equivalent system in Input-Output representation is given by the transfer function:
    \begin{align}
    \bm{G}(s) = \bm{Y}(s)\bm{U}^{-1}(s) = \bm{C} (s\bm{I} - \bm{A})^{-1} \bm{B} + \bm{D}
    .\end{align}
\end{boxed-theorem}
 
\begin{proof}
    Applying the Laplace transfer in both sides of the equation:
    \begin{equation} \label{eq:convertSSIO01}
    \begin{split}
        \mathcal{L} \left\{ \dot{\bm{x}}(t) \right\} &= \bm{A} \mathcal{L} \left\{ \bm{x}(t) \right\} + \bm{B} \mathcal{L} \left\{ u(t) \right\} \\
        s \bm{X}(s) - \dot{\bm{x}}(0^-) &= \bm{A} \bm{X}(s) + \bm{B} \bm{U}(s) \\
        (s\bm{I}  - \bm{A}) \bm{X}(s) &=  \bm{B} \bm{U}(s) \\
         \bm{X}(s) &= (s\bm{I}  - \bm{A})^{-1} \bm{B} \bm{U}(s)
    \end{split}
    .\end{equation}
    
    By applying the same procedure in the output equations, using the previous result, the Laplace transform of the output is:
    \begin{equation} \label{eq:convertSSIO02}
    \begin{split}
        \mathcal{L} \left\{ \bm{y}(t) \right\} &= \bm{C} \mathcal{L} \left\{ \bm{y}(t) \right\} + \bm{D} \mathcal{L} \left\{ u(t) \right\} \\
        \bm{Y}(s)  &= \bm{C} \bm{X}(s) + \bm{D} \bm{U}(s) \\
          &= \bm{C} \left( (s\bm{I}  - \bm{A})^{-1} \bm{B} \bm{U}(s) \right) + \bm{D} \bm{U}(s) \\
          &= \left( \bm{C} (s\bm{I} - \bm{A})^{-1} \bm{B}   + \bm{D} \right) \bm{U}(s) \\
    \end{split}
    .\end{equation}
    
    In conclusion, since by definition $\bm{Y}(s) = \bm{G}(s) \bm{U}(s)$, it is possible to obtain the transfer function matrix $\bm{G}(s) = \bm{C} (s\bm{I} - \bm{A})^{-1} \bm{B} + \bm{D}$ as an equivalent representation of the system.
\end{proof}

Despite of the discussion about the benefits of linear models, it is necessary to account for the fact that physical systems will present, in most situations, nonlinear behavior. For this reason, some effort must be done to develop a linear model that can describe the nonlinear behavior with certain accuracy, even if over some small region of the space. With this motivation, a technique for \textit{linearization} of a nonlinear model is detailed below.

\begin{boxed-theorem}{(Linearization by Taylor Expansion)} \label{th:linearization}
    Consider a nonlinear time-invariant system:
    \begin{equation} \label{eq:SSRepr03}
    \begin{cases}
        \dot{\bm{x}}(t) = \bm{f}(\bm{x}(t), \bm{u}(t)) \\
        \bm{y}(t) = \bm{g}(\bm{x}(t), \bm{u}(t))
    \end{cases}
    .\end{equation}
    
    Given steady-state operating points $\bm{x}_o$, $\bm{y}_o$ and $\bm{u}_o$, the dynamics of the system in the neighborhood of these points can be represented by the linear model: 
    \begin{align}
    \begin{cases}
        \Delta \dot{\bm{x}}(t) = \bm{A}\Delta \bm{x}(t) + \bm{B}\Delta \bm{u}(t) & \\
        \hfill \bm{y}(t) = \bm{C}\Delta \bm{x}(t) + \bm{D}\Delta \bm{u}(t) &
    \end{cases}
    ,\end{align}
    
    \noindent where
    \begin{equation}
    \begin{matrix}
        \bm{A} = \left. \cfrac{\partial \bm{f}}{\partial \bm{x}} \right|_{\bm{x}_o, \bm{u}_o}, & \bm{B} = \left. \cfrac{\partial \bm{f}}{\partial \bm{u}} \right|_{\bm{x}_o, \bm{u}_o}, & \bm{C} = \left. \cfrac{\partial \bm{g}}{\partial \bm{x}} \right|_{\bm{x}_o,  \bm{u}_o}, & \bm{D} = \left. \cfrac{\partial \bm{g}}{\partial \bm{u}} \right|_{\bm{x}_o, \bm{u}_o} 
    \end{matrix}
    \end{equation}
    
    \noindent and
    \begin{equation}
    \begin{matrix}
        \Delta \bm{x}(t) = \bm{x}(t) - \bm{x}_o; & & \Delta \bm{u}(t) = \bm{u}(t) - \bm{u}_o
    \end{matrix}
    .\end{equation}
\end{boxed-theorem}

\begin{proof}
    Consider a system represented by state equations $\bm{f}(\cdot)$ and output equations $\bm{g}(\cdot)$, with steady-state points $\bm{x}_o$, $\bm{y}_o$ and $\bm{u}_o$. Now, consider a very small perturbation $\Delta u(t)$ in the input signal around these operation points. This perturbation will result in small changes in the state and output variables:
    \begin{align}
    \begin{matrix}
        \bm{x}(t) = \bm{x}_o + \Delta \bm{x}(t); & &
        \bm{u}(t) = \bm{u}_o + \Delta \bm{u}(t); & &
        \bm{y}(t) = \bm{y}_o + \Delta \bm{y}(t)
    \end{matrix}
    .\end{align}
    
    This results in the following configuration on the State-Space:
    \begin{align}
    \begin{cases}
        \cfrac{d(\bm{x}_o + \Delta \bm{x}(t))}{dt} &= \bm{f}(\bm{x}_o + \Delta \bm{x}(t), \bm{u}_o + \Delta \bm{u}(t) ) \\
        \bm{y}_o + \Delta \bm{y}(t) &= \bm{g}(\bm{x}_o + \Delta \bm{x}(t), \bm{u}_o + \Delta \bm{u}(t) ) \\
    \end{cases}
    ,\end{align}
    
    \noindent where $d(\bm{x}_o + \Delta \bm{x}(t)) / dt = d(\Delta \bm{x}(t)) / dt$, since $\bm{x}_o$ is constant. The perturbed variables are very close to the operation points, hence the functions $\bm{f}(\cdot)$ and $g(\cdot)$ can be approximated by a Taylor series expansion, yielding:
    \begin{align}
    \begin{cases}
        \hfill \cfrac{d(\Delta \bm{x}(t))}{dt} &= \bm{f}(\bm{x}_o, \bm{u}_o) + \left. \cfrac{\partial \bm{f}}{\partial \bm{x}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{x}(t) + \left. \cfrac{\partial \bm{f}}{\partial \bm{u}}\right\vert_{\bm{x}_o, \bm{u}_o}  \Delta \bm{u}(t) + \text{high order terms} \\ \\
        \bm{y}_o + \Delta \bm{y}(t) &= \bm{g}(\bm{x}_o, \bm{u}_o) + \left. \cfrac{\partial \bm{g}}{\partial \bm{x}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{x}(t) + \left. \cfrac{\partial \bm{g}}{\partial \bm{u}}\right\vert_{\bm{x}_o, \bm{u}_o}  \Delta \bm{u}(t) + \text{high order terms} \\
    \end{cases}
    .\end{align}
    
    Since the steady-state condition implies zero variation, it is possible to assume $\bm{f}(\bm{x}_o, \bm{u}_o) = 0$ and $g(\bm{x}_o, \bm{u}_o) = 0$, since they are ordinary differential equations. Truncating in the first order terms and substituting $\bm{y}(t) = \bm{y}_o + \Delta \bm{y}(t)$ results in:
    \begin{align}
    \begin{cases}
        \cfrac{d(\Delta \bm{x}(t))}{dt} =\left. \cfrac{\partial \bm{f}}{\partial \bm{x}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{x}(t) + \left. \cfrac{\partial \bm{f}}{\partial \bm{u}}\right\vert_{\bm{x}_o,  \bm{u}_o}  \Delta \bm{u}(t) \\ \\
        \hfill \bm{y}(t) = \left. \cfrac{\partial \bm{g}}{\partial \bm{x}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{x}(t) + \left. \cfrac{\partial \bm{g}}{\partial \bm{u}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{u}(t) \\
    \end{cases}
    .\end{align}
    
    Finally, since all the Jacobians involved are actually matrices of appropriate dimensions, the final linear approximation of the system is the SS model given by:
    \begin{align}
    \begin{cases}
        \Delta \dot{\bm{x}}(t) = \bm{A}\Delta \bm{x}(t) + \bm{B}\Delta \bm{u}(t) & \\
        \hfill \bm{y}(t) = \bm{C}\Delta \bm{x}(t) + \bm{D}\Delta \bm{u}(t) &
    \end{cases}
    .\end{align}
\end{proof}

Note that this method can be used to convert non-linear models obtained from the first-principle procedure to a desirable State-Space form, since those models are usually in the non-linear State-Space representation. Consider for instance, a reactor system describing an isothermal process that follows the Van de Vusse reaction scheme \cite{VanDeVusse:1964}:
\begin{align}
\begin{matrix}
    \textbf{Reaction Scheme:}  & & \textbf{Flow parameters:} \\ 
    A\overset{K_{AB}}{\rightarrow} B \overset{K_{BC}}{\rightarrow} C & & \rho_{in}^{(B)}(t) = \rho_{in}^{(C)}(t) = \rho_{in}^{(D)}(t) = 0 \\
    2 A \overset{K_{AD}}{\rightarrow} D & &  \rho_{out}^{(k)}(t) = \rho_k(t),\ k \in \{A, B, C, D\}
\end{matrix}    
.\end{align}

Consider $\bm{x} = [\rho_A, \rho_B, \rho_C, \rho_D]^T$ and $u = q$, and consider also that the states are perfectly observed, so that $\bm{y} = \bm{x}$. Using Theorem \ref{th:isoReactSys01},  the state-equation for the nonlinear State-Space representation of this reactor is:
\begin{align}
\begin{cases}
    \dot{x_1}(t) = u(t) (\rho_{in}^{(A)} - x_1(t)) - (K_{AB} x_1(t) + K_{AD} (x_1(t))^2)\\
    \dot{x_2}(t) = -u(t) x_2(t) + K_{AB} x_1(t) - K_{BC} x_2(t) \\
    \dot{x_3}(t) = -u(t) x_3(t) + K_{BC} x_2(t) \\
    \dot{x_4}(t) = -u(t) x_4(t) + \cfrac{1}{2} K_{AD} (x_1(t))^2
\end{cases}
.\end{align}

 To select steady-state points $\bm{x}_o$ and $\bm{u}_o$, it is necessary to find values for $\bm{x}(t)$ and $u(t)$ that satisfies the system $\dot{\bm{x}}(t) = \bm{0}$. This can be done by analytical formulas, in some cases, by numerical methods or by simply simulating the nonlinear system and measuring these quantities when the states displays zero variation. Finally, the linear matrices $(\bm{A}, \bm{B}, \bm{C}, \bm{D})$ are obtained by deriving each one of these equations in respect to each state and input, resulting:
\begin{align}   \label{eq:isoReact01}
\left\{ \begin{matrix*}[l]
    \bm{A} = \begin{bmatrix}
        -u_o - K_{AB} - 2K_{AC} x_{1o} & 0 & 0 & 0 \\ K_{AB} & -u_o - K_{BC} & 0 & 0 \\ 0 & K_{BC} & -u_o & 0 \\ K_{AD} x_{1o} & 0 & 0 & -u_o
    \end{bmatrix} \hfill & &
    \bm{B} = \begin{bmatrix}
        \rho_{in}^{(A)}-x_{1o} \\ -x_{2o} \\ -x_{3o} \\ -x_{4o} 
    \end{bmatrix} \hfill \\  \\ 
    \bm{C} = \begin{bmatrix}
        1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1
    \end{bmatrix} \hfill & &
    \bm{D} = \begin{bmatrix}
        0 \\ 0 \\ 0 \\ 0
    \end{bmatrix} \hfill
\end{matrix*} \right.
.\end{align}

Notice that the response of this model represents the deviations around the given operation points. If the errors in approximating the non-linearities of the actual system are somehow tolerable, this model can still be used as a nominal model to represent all the operations, given that the response is vertically ``correted" as $\bm{x}(t) = \Delta \bm{x}(t) + \bm{x}_o$ (and for initial states $\Delta \bm{x}_0 = \bm{x}(t_0) - \bm{x}_o$ and input signal $\Delta \bm{u}(t) = \bm{u}(t) - \bm{u}_o$). The simulation of both the nonlinear and the linear systems are displayed in Fig. \ref{fig:linResp01}, with the dashed line the linear response of a system given $\bm{x}_o = [6.19, 1.09, 0.60, 1.05]^T$ and $u_0 = 3.03$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/linResp01}
    \caption{Comparison between the nonlinear and linear response of the reactor system.}
    \label{fig:linResp01}
\end{figure}

\section{Response Analysis in Time Domain}

Once that a model is well-established, it is possible both to simulate the system and to analyze its response, both for a natural or forced regime. This section, then, focus on developing a quantitative understanding of a system behavior through a linear model. The results are focused on continuous-time response of linear and time-invariant (LTI) systems in the form:
\begin{align}
\begin{cases}
    \dot{\bm{x}}(t) = \bm{A} \bm{x}(t) + \bm{B} \bm{u}(t) \\
    \bm{y}(t) = \bm{C} \bm{x}(t) + \bm{D} \bm{u}(t)
\end{cases}
.\end{align}

First of all, it is necessary to access some properties of the matrix $\bm{A}$, which describes the natural evolution of the states. 

\begin{boxed-definition}{(State-Transition Matrix)} \label{def:stateTransM}
    Consider a system in State-Space representation with matrix $\bm{A} \in \mathbb{R}^{n \times n}$. Its \textit{State-Transition Matrix}, $e^{\bm{A} t} \in \mathbb{R}^{n \times n}$ , is the converging series:
\begin{equation}
    e^{\bm{A} t} = \bm{I} + \bm{At} + \cfrac{\bm{A}^2 t^2}{2!} + \cfrac{\bm{A}^3 t^3}{3!} + \cdots = \sum_{k=0}^{\infty} \cfrac{A^k t^k}{k!}
.\end{equation} 
\end{boxed-definition}

This matrix is central to the computation of the system time response, as it will be shown shortly. Since the matrix $A$ is a square matrix, this operation leads to some useful properties.

\begin{boxed-theorem}{} \label{th:stateTransMProp}
    Consider a matrix exponential as in Definition \ref{def:stateTransM}, for a matrix $\bm{A}  \in \mathbb{R}^{n \times n}$. Then, the following properties holds:
\begin{align}
        1)\ \cfrac{d ( e^{\bm{A} t} )}{dt} = \bm{A} e^{\bm{A} t} & &
        2)\ e^{\bm{A} t} e^{\bm{A} \tau} = e^{\bm{A} (t + \tau)} & &
        3)\ e^{-\bm{A} t} e^{\bm{A} t} = e^{\bm{A} t} e^{-\bm{A} t} = \bm{I}
.\end{align}
\end{boxed-theorem}

The proof is direct from Definition \ref{def:stateTransM}, and a detailed discussion of these properties is omitted. Given these results, the calculation of the time response of a system in SS representation becomes straightforward.

\begin{boxed-theorem}{(Lagrange Formula)} \label{th:lagrangeForm}
    Consider a LTI system in State-Space representation. Its response for any time $t \geq t_0$, initial state $\bm{x}(t_0)$ and input signal $\bm{u}(t)$ is given by the solutions of the state and output equations:
    \begin{align}
    \begin{cases}
        \bm{x}(t) = e^{\bm{A}(t - t_0)} \bm{x}(t) + \int_{t_0}^{t} e^{\bm{A}(t - \tau)} \bm{B} \bm{u}(\tau) d \tau \hfill & \\
        \bm{y}(t) = \bm{C} e^{\bm{A}(t - t_0)} \bm{x}(t) + \bm{C} \int_{t_0}^{t} e^{\bm{A}(t - \tau)} \bm{B} \bm{u}(\tau) d \tau + \bm{D} \bm{u}(t) &
    \end{cases}
    .\end{align}
\end{boxed-theorem}

\begin{proof}
    First of all, consider a system in State-Space representation with state equation as defined in Equation \eqref{eq:SSRepr03}. Multiplying both sides by $e^{-\bm{A} t}$:
    \begin{equation} \label{eq:lagrTH01}
    \begin{split}
        e^{-\bm{A} t} \dot{\bm{x}}(t) &= e^{-\bm{A} t} (\bm{A} \bm{x}(t) + \bm{B} \bm{u}(t)) \\
        e^{-\bm{A} t} \dot{\bm{x}}(t) - \bm{A}e^{-\bm{A} t} \bm{x}(t)  &=  e^{-\bm{A} t} \bm{B} \bm{u}(t)
    \end{split}
    .\end{equation}
    
    Using the first assertive in Theorem \ref{th:stateTransMProp}, it is easy to see that $d[e^{-\bm{A} t} x(t)]/dt = e^{-\bm{A} t} \dot{\bm{x}}(t) - \bm{A} e^{-\bm{A} t} \bm{x}(t)$. Substituting this result in \eqref{eq:lagrTH01} and integrating both sides from $t_0$ to $t$:
    \begin{equation} \label{eq:lagrTH02}
    \begin{split}
        \cfrac{d(e^{-\bm{A} t} \bm{x}(t))}{dt} &= e^{-\bm{A} t} \bm{B} \bm{u}(t) \\
        \left. e^{-\bm{A} t} \bm{x}(t) \right|^{t}_{t_0}  &= \int_{t_0}^{t} e^{-\bm{A} \tau} \bm{B} \bm{u}(\tau) d\tau \\
        e^{-\bm{A} t} \bm{x}(t) - e^{-\bm{A} t_0} \bm{x}(t_0)  &= \int_{t_0}^{t} e^{-\bm{A} \tau} \bm{B} \bm{u}(\tau) d\tau
    \end{split}
    .\end{equation}
    
    Multiplying both sides by $e^{\bm{A} t}$ and using the second and third assertive from Theorem \ref{th:stateTransMProp}, the state response can be calculated as:
    \begin{equation} \label{eq:lagrTH03}
    \begin{split}
        e^{\bm{A} t} \left( e^{-\bm{A} t} \bm{x}(t) \right. &- \left. e^{-\bm{A} t_0} \bm{x}(t_0) \right) = e^{\bm{A} t} \int_{t_0}^{t} e^{-\bm{A} \tau} \bm{B} \bm{u}(\tau) d\tau \\
        \bm{I} \bm{x}(t) &- e^{\bm{A} (t - t_0)} \bm{x}(t_0) = \int_{t_0}^{t} e^{\bm{A}(t - \tau)} \bm{B} \bm{u}(\tau) d\tau \\
        \bm{x}(t) &= e^{\bm{A} (t - t_0)} \bm{x}(t_0) + \int_{t_0}^{t} e^{\bm{A}(t - \tau)} \bm{B} \bm{u}(\tau) d\tau \\
    \end{split}
    .\end{equation}
    
    Finally, substituting \eqref{eq:lagrTH03} into the output equation leads to:
    \begin{equation} \label{eq:lagrTH04}
    \begin{split}
        \bm{y}(t) &= \bm{C} \left( e^{\bm{A} (t - t_0)} \bm{x}(t_0) + \int_{t_0}^{t} e^{\bm{A}(t - \tau)} \bm{B} \bm{u}(\tau) d\tau \right) + \bm{D} \bm{u}(t) \\ 
        \bm{y}(t) &= \bm{C} e^{\bm{A}(t - t_0)} \bm{x}(t) + \bm{C} \int_{t_0}^{t} e^{\bm{A}(t - \tau)} \bm{B} \bm{u}(\tau) d \tau + \bm{D} \bm{u}(t) 
    \end{split}
    .\end{equation}
\end{proof}

When discussing the response of a system, the focus is actually directed to the state equation describing its dynamics, since the output equation only represents an observation of the system through the states. In this sense, the Lagrange formula exposes the nice characteristic of linear systems that the total response is a composition of two separated actions:
\begin{equation}
    \bm{x}(t) = \bm{x}_{\text{n}}(t) + \bm{x}_{\text{f}}(t) 
,\end{equation}

\noindent where the natural response, $\bm{x}_{\text{n}}(t)$, corresponds to the state-transition matrix multiplication term and the forced response, $\bm{x}_{\text{f}}(t)$, corresponds to the integral term. This concept is visualized in Fig. \ref{fig:linResp02} for the total response of the first two states of the model derived in Equation \eqref{eq:isoReact01}, given the operation points $\bm{x}_o = [6.19, 1.09]$ and $u_o = 3.03$, excited with a step input $u(t) = 3$, $t \in [0, 1.2]$. In this plot, the solid line represents the total response, whereas the dashed and dotted lines represents the natural and forced response, respectively. It is easy to verify that the decomposition of the total response is equal to the sum of those independent components.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/linResp02}
    \caption{Simulation of the total response decomposition for the two first states of the model from \eqref{eq:isoReact01} shown separately (left) and in a single visualization (right) in which the steady-state point is indicated by a circle.}
    \label{fig:linResp02}
\end{figure}

The only problem remaining to fully characterize the dynamical response of a system is to compute the state-transition matrix, which can be done in several ways \cite{Moler_VanLoan:2003}. A specific method, known as the \textit{Sylvester expansion}, is an analytical solution that brings an interesting understanding of the system behavior through the state-state matrix $A$.

\begin{boxed-theorem}{(Sylvester expansion)} \label{th:sylvester01}
    Consider a matrix exponential function $\bm{f}(\bm{A}) = e^{\bm{A} t}$ for any square matrix $\bm{A} \in \mathbb{R}^{n \times n}$, whose distinct eigenvalues are $\bm{\lambda} \in \mathbb{R}^{m},\ m \leq n,$ with associated multiplicity vector $\bm{\nu} \in \mathbb{R}^m$ such that $\sum_j \nu_j = n$. The result of this function can be expanded as:
    \begin{equation} \label{eq:sylvester01}
        f(\bm{A}) = e^{\bm{A} t} = \beta_0(t) \bm{I} + \beta_1(t) \bm{A} + \beta_2(t) \bm{A}^2 + ... + \beta_{n-1}(t) \bm{A}^{n-1} = \sum_{i=0}^{n-1} \beta_i(t) \bm{A}^i
    ,\end{equation} 
    
    \noindent where $\beta_i(t) : \mathbb{R} \rightarrow \mathbb{R}$, $i \in [1,2,...,n-1]$, are scalar functions of time that solves the linear system:
    \begin{equation} \label{eq:sylvester02}
        \bm{V} \bm{\beta} = \bm{\eta}
    \end{equation} 
    
    \noindent for the parameter matrix $\bm{\beta} = [\beta_0(t), \beta_1(t), ..., \beta_n(t)]^T$, and the vector of modes $\bm{\eta} = [\eta_1, \eta_2, ..., \eta_m]^T$ and the confluent Vandermonde matrix $\bm{V} = [V_1, V_2, ..., V_m]^T$ given as
    \begin{align*}
        \eta_i & = \begin{bmatrix} e^{\lambda_i t} & te^{\lambda_i t} & t^2e^{\lambda_i t} & \cdots & t^{\nu_i-1}e^{\lambda_i t} \end{bmatrix}^T \\ \\
        V_j     & = \begin{bmatrix}
            1 & \lambda_j & \lambda_j^2 & \cdots & \lambda_j^{(\nu_j - 1)} & \cdots & \lambda_j^{n-1} \\
            0 & 1 & 2\lambda_j & \cdots & (\nu_j - 1) \lambda_j^{(\nu_j - 1)} & \cdots & (n-1)\lambda_j^{n-2} \\
            \vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \cdots & (\nu_j - 1)! & \cdots & \cfrac{(n - 1)!}{(n - \nu_j)!}\lambda_j^{n-\nu_j} \\
        \end{bmatrix}\
    \end{align*} 
\end{boxed-theorem}

The proof of this expansion is somewhat extensive, but a detailed discussion can be found in \cite{Chen:1998}. Basically, the expansion in Equation \eqref{eq:sylvester01} is a direct application of the Cayley-Hamilton theorem \cite{Atiyah:2018} and the linear system in Equation \eqref{eq:sylvester02} is a result of the Sylvester's matrix theorem  \cite{Horn:2012}.

From the expansion presented in Theorem \ref{th:sylvester01}, it is possible to understand the relationship between the state-transition matrix $e^{\bm{At}}$ and each eigenvalue $\lambda$ of the matrix $\bm{A}$, also known as the \textit{poles} of the system. First of all, notice that the formulation of the linear system that defines the parameters $\beta_0(t), \beta_1(t), ..., \beta_{n-1}(t)$ implies that each one of these functions are linear combinations of the exponentials $e^{\lambda_i t}$ for each eigenvalue $\lambda_i,\ i=1,2,...m$. These exponentials are known as the \textit{modes} of the matrix $\bm{A}$. Since the Sylvester expansion is linear in those parameters, it is possible to conclude that the state-transition matrix, and consequently the response of a system, is a linear combination of the modes.

Consider, for the sake of illustration, the same reactor model from Equation \eqref{eq:isoReact01}, but considering only the first two states (since they are independent of the others). Let $\bm{x}_o = [6.19, 1.09]^T$ and $u_0 = 3.03$, just as before. The resulting matrix $\bm{A}$ and state-transition matrix $e^{\bm{A} t}$ are shown below, where is easy to see that $\bm{\lambda} = [-5.93, -4.70]$, since $\bm{A}$ is diagonal:
\begin{align} \label{eq:stateRespEx01}
    \bm{A} = \begin{bmatrix} 
        -5.93  &        0 \\
        0.83   &  -4.70 
    \end{bmatrix} && e^{\bm{A} t} = \begin{bmatrix} 
        e^{-5.93 t}  &        0 \\
        0.68 e^{-4.7 t} - 0.68 e^{-5.93 t}  &  e^{-4.70 t} 
    \end{bmatrix}
.\end{align}

A simulation in time, shown in Fig \ref{fig:stateTrans01}, shows the evolution of each element of $e^{\bm{A} t}$ for a given time interval. Considering only the natural response $\bm{x}_n(t)$, i.e., setting $\bm{u}(t) = \bm{0},\ t \in [t_0, t]$, it is evident that the actual response of each state in the system is a row-wise weighted sum of these elements, where the weights are given by the initial state $\bm{x}(t_0)$. Therefore, the $(i, j)$ element of this matrix describes how the $j$-th state affects the response of the $i$-th state.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/stateTrans01}
    \caption{Simulation of the state-transition matrix $e^{\bm{A} t}$ in Equation \eqref{eq:stateRespEx01} for $t \in [0,2]$}
    \label{fig:stateTrans01}
\end{figure}

Now, attention must be drawn to the case where the eigenvalues are not real, but complex and conjugate. Despite that the Sylvester expansion is still defined as in Theorem \ref{th:sylvester01}, this case introduces a slightly different interpretation of the modes contributions to the natural response.

\begin{boxed-theorem}{} \label{th:sylvester02}
    Consider the same expansion defined in Theorem \ref{th:sylvester01}. Consider, now, that the matrix $\bm{A}$ has two distinct eigenvalues $\lambda_c, \lambda_c' \in \mathbb{C}$ in the form $\lambda_c, \lambda_c' = \alpha \pm j \omega$. In this case, the linear system solved by the parameters $\beta_0(t), \beta_1(t), ..., \beta_{n-1}(t)$ will have two equations:
    \begin{align} \label{eq:sylvEq01}
    \begin{cases}
        \beta_0 + \alpha \beta_1 + \alpha^{2} \beta_2 + ... + \alpha^{n-1} \beta_{n-1} = e^{\alpha t} cos(\omega t) & \\
        \hfill 0 + \omega \beta_1 + \omega^{2} \beta_2 + ... + \omega^{n-1} \beta_{n-1} = e^{\alpha t} sin(\omega t) &
    \end{cases}
    .\end{align}
\end{boxed-theorem}

\begin{proof}
    Consider the matrix $\bm{A}$ with eigenvalues as specified and the Sylvester expansion as presented. In this case, there will be two equations in the system:
    \begin{align} 
    \begin{cases}
        \beta_0 + \lambda_c \beta_1 + \lambda_c^2  \beta_2 + ... + \lambda_c^{n-1} \beta_{n-1} = e^{\lambda_c t} & \\
        \beta_0 + \lambda_c' \beta_1 + (\lambda_c')^{2} \beta_2 + ... + (\lambda_c')^{n-1} \beta_{n-1} = e^{\lambda_c' t} &
    \end{cases}
    .\end{align}
    
    Since the eigenvalues are complex and conjugate, it has that $\lambda_c + \lambda'_c = 2 \mathbb{R}e[\lambda_c] = 2 \alpha$ and $\lambda_c - \lambda'_c = 2j \mathcal{I}m[\lambda_c] = 2j \omega$. Moreover, the Euler identity $e^{\alpha \pm j \omega} = e^{\alpha t}(cos(\omega) \pm j sin(\omega))$ shows that $e^{\lambda} + e^{\lambda'} = 2 e^{\alpha t} cos(\omega t)$ and $e^{\lambda} - e^{\lambda'} = 2 e^{\alpha t} sin(\omega t)$. In this case, summing the two rows and subtracting the first row by the second one results in:
    \begin{align} 
    \begin{cases}
        \beta_0 + 2 \alpha \beta_1 + 2 \alpha^2  \beta_2 + ... + 2 \alpha^{n-1} \beta_{n-1} = 2 e^{\alpha t} cos(\omega t) & \\
        0 + 2j \omega \beta_1 + 2j \omega^{2} \beta_2 + ... + 2j \omega^{n-1} \beta_{n-1} = 2j e^{\alpha t} sin(\omega t) &
    \end{cases}
    .\end{align}
    
    Finally, dividing the first row by $2$ and the second row by $2j$ results in \eqref{eq:sylvEq01}.
\end{proof}

From the same reasons stated before, this result implies that the actual response of the system will have sinusoidal components that produces oscillations in the response. The modes associated with complex and conjugate eigenvalues are classified as \textit{pseudo-periodic}, since they are composed by an exponential growth (or decay) enveloping a sinusoidal function. Consider, again for the sake of illustration, the following toy example:
\begin{align} \label{eq:stateRespEx02}
    \bm{A} = \begin{bmatrix} 
         -0.1  &  \hfill 0.5 \\
        -0.5   &  -0.1 
    \end{bmatrix} && e^{\bm{A} t} = \begin{bmatrix} 
        e^{-0.1 t} cos(0.5t)  &    e^{-0.1 t} sin(0.5t) \\
        -e^{-0.1 t} sin(0.5t)  &  e^{-0.1 t} cos(0.5t)
    \end{bmatrix}
.\end{align}

The elements of the resulting state-transition matrix are simulated for a specific time-interval and shown in Fig. \ref{fig:stateTrans02}. It is possible to see, in this case, the pseudo-periodic behavior of the complex conjugate modes, where the dashed lines represents the exponential envelope. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/stateTrans02}
    \caption{Simulation of the state-transition matrix $e^{\bm{A} t}$ in Equation \eqref{eq:stateRespEx02} for $t \in [0,50]$}
    \label{fig:stateTrans02}
\end{figure}

The previous discussion on the modes of matrix $\bm{A}$ introduced the importance of the eigenvalues of this matrix in analyzing the system response. The analysis, however, was focused on the natural response $\bm{x}_n(t)$, whereas the forced response $\bm{x}_f(t)$ was deliberately omitted. Now, the analysis focus the opposite case. Consider, for instance, the response of the first state for the two systems described by the matrices $(\bm{A}^{(1)}, b^{(1)})$ and $(\bm{A}^{(2)}, b^{(2)})$ given below, which are more complete descriptions of the systems in \eqref{eq:stateRespEx01} and \eqref{eq:stateRespEx02}. Considering initial states $\bm{x}^{(1)}(0) = \bm{x}^{(2)}(0) = \bm{0}$ and the input signals $\bm{u}^{(1)}(t) = \bm{u}^{(2)}(t) = \bm{1}$, for time $t \in [0, 60)$, the evolution of these states are shown in Fig. \ref{fig:forcedResponse}.
\begin{equation} \label{eq:isoSys02}
\begin{matrix}
    \bm{A}^{(1)} = \begin{bmatrix} 
        -5.93  &      0 \\
        \hfill 0.83   &  -4.70 
    \end{bmatrix} & b^{(1)} = \begin{bmatrix} 3.81 \\ -1.09 \end{bmatrix} & & & \bm{A}^{(2)} = \begin{bmatrix} 
         -0.1  &  \hfill 0.5 \\
        -0.5   &  -0.1 
    \end{bmatrix} & b^{(2)} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
\end{matrix}
.\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/forcedResponse}
    \caption{One-state forced response of the two systems from \eqref{eq:isoSys02}.}
    \label{fig:forcedResponse}
\end{figure}

Since $\bm{A}^{(1)}$ is diagonal and $\bm{A}^{(2)}$ has a single pair of complex conjugate eigenvalues, it is possible to consider that those are respectively the characteristic responses of aperiodic and pseudo-periodic modes to an unitary step input. Since any signal can be decomposed by a sequence of step signals, the forced response to an unitary step is the default evolution used in literature to characterize the behavior of modes (and ultimately, of systems) in respect to some specifications of the transient response defined below.

\begin{boxed-definition}{(Unit-Step Response Specifications)} \label{def:responseParameters}
    Given a mode $e^{\lambda t}$ associated with the eigenvalue $\lambda \in \mathbb{R}$, its contribution to the response has a time constant ($\tau$) defined as
    \begin{equation}
        \tau = - \cfrac{1}{\lambda}
    .\end{equation}  
    
    Furthermore, given pseudo-periodic modes $e^{\lambda t}$ and $e^{\lambda' t}$ of the matrix $\bm{A}$ associated with the eigenvalues $\lambda,\lambda' = \alpha \pm j \omega$, their contribution to the response has a time constant ($\tau$), a natural frequency ($\omega_n$) and a damping coefficient ($\zeta$) defined as:
    \begin{equation}
    \begin{matrix}
        \tau = - \cfrac{1}{\alpha} & & & \omega_n = \sqrt{\alpha^2 + \omega^2} & & & \zeta = - \cfrac{\alpha}{\omega_n}
    \end{matrix}
    .\end{equation}
    
\end{boxed-definition}   

The specifications just defined provides a quantitative way to describe the response of a system in terms of time and frequencies. The time constant, for instance, is a quantity that represents the time needed for the mode to lost $63\%$ of its initial value, since $e^{\lambda \tau} = e^{-1} = 0.37$. A greater value of a time constant indicates that the system is able to ``discharge" energy faster. The damping coefficient, in turn, provides an information about the intensity of the peak in the pseudo-periodic responses, which is known as \textit{overshoot} (or \textit{undershoot} in the case that of a negative peak) and the natural frequency represents the oscillation of the response before reaching steady-state. From the perspective of control theory, these are some of the specifications used to define desirable transient responses to a controlled system.

Notice that the response specifications are always functions of the real and imaginary parts of the discussed eigenvalues. This brings the possibility of a visualization in the complex plane to interpret how each eigenvalue contributes to the total response. A straightforward notion is that the closer an eigenvalue is to the imaginary axis, the faster is its contribution. Similarly, the furthest an eigenvalue is to the real axis, the more oscillatory is its contribution. Finally, a vector from the origin of the plane to a complex eigenvalue has a norm equal to the natural frequency ($\omega_n$) and the cosine of the angle formed with the imaginary axis is equal to the damping factor ($\zeta$). A simulation of the contributions from different eigenvalues are shown in Fig. \ref{fig:eigen01}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/eigen01}
    \caption{Eigenvalues of a system and the forced response associated with their modes.}
    \label{fig:eigen01}
\end{figure}

\section{Similarity Transformations}

A State-Space representation can be interpreted through a system of coordinates. A state, in this context, represents a vector as visualized through this reference. Under the assumption of a linear time-invariant system, there is an intuition that is possible to change the representation of the states by changing this system of coordinates through some linear transformation, obtaining a different model to the same system. This is the motivation for the discussion in this section.

First of all, consider a brief reflection about the geometrical interpretation of a State-Space model for a physical system. Let a state-vector in an arbitrary time $t$ be $\bm{x}(t) = [1, 3]^T$, defined in the $\mathbb{R}^{2}$ space, as shown in the left side of Fig. \ref{fig:similarity01} in a Cartesian coordinate system. It is possible to associate to this vector an orthonormal basis $\bm{I} \in \mathbb{R}^2$, the 2-dimensional identity matrix, such that the vector $\bm{x}(t)$ observed through this basis standard basis is equal to itself. This concept provides the possibility to associate any other arbitrary basis to represent a state-vector and visualize the states through this perspective. When the basis is not orthogonal, a change in the state-vector with a direction parallel to a component of the basis, i.e., a change in only one element of this vector, produces a change in other directions if observed through the original orthonormal basis. This is an interesting result, since it shows the interactions between two state variables through a basis, making it possible to observe mutual changes in those variables from a single direction. The right side of Fig. \ref{fig:similarity01} illustrates the vector $\bm{x}(t)$ as referenced through the basis $\bm{Q} = [\bm{q}_1, \bm{q}_2]^T$, for $\bm{q}_1 = [3, 1]^T$ and $\bm{q}_1 = [2, 2]^T$, given as: 
\begin{equation}
    \bm{x}(t) = \begin{bmatrix} 1 \\ 3 \end{bmatrix} = \begin{bmatrix} 3 & 2 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} -1 \\ 2 \end{bmatrix} = \bm{G}\bm{z}(t)
.\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/similarity01}
    \caption{Visualization of a basis transformation applied to a vector.}
    \label{fig:similarity01}
\end{figure}

In the State-Space formulation, the matrix $\bm{A}$ represents a linear function that maps state-vectors from $\mathbb{R}^{n}$ to itself. When applying a new basis to represent the state-vectors, it is intuitive that the mapping performed by this function also changes so that it still represents the same linear combination of the states.

\begin{boxed-theorem}{(Similarity Transformation)}
    Consider a system in SS representation described by the matrices $(\bm{A}, \bm{B}, \bm{C}, \bm{D})$ and a nonsingular transformation matrix $\bm{P} \in \mathbb{R}^{n \times n}$. An equivalent representation for the transformation $\bm{z}(t) = \bm{P} \bm{x}(t)$ is:
    \begin{align}
    \begin{cases}
        \dot{\bm{z}}(t) = \tilde{\bm{A}} \bm{z}(t) + \tilde{\bm{B}} \bm{u}(t) & \\
        \bm{y}(t) = \tilde{\bm{C}} \bm{z}(t) + \tilde{\bm{D}} \bm{u}(t)
    \end{cases} 
    ,\end{align}
    
    \noindent where:
    \begin{equation}
        \begin{matrix}
            \tilde{\bm{A}} = \bm{P} \bm{A} \bm{P}^{-1} & & \tilde{\bm{B}} = \bm{P} \bm{B} & & \tilde{\bm{C}} = \bm{C} \bm{P}^{-1} & & \tilde{\bm{D}} = \bm{D}
        \end{matrix}
    .\end{equation}
\end{boxed-theorem}

\begin{proof}
    Consider a SS representation and any nonsingular matrix $\bm{P} \in \mathbb{R}^{n \times n}$. Making $\bm{z}(t) = \bm{P} \bm{x}(t)$ leads to $\bm{x}(t) = \bm{P}^{-1} \bm{z}(t)$. Substituting this in the state equation results in:
    \begin{equation}
    \begin{split}
        \dot{\bm{x}}(t) &= \bm{A} \bm{x}(t) + \bm{B} \bm{u}(t) \\
        \bm{P}^{-1} \dot{\bm{z}}(t) &= \bm{A} \bm{P}^{-1} \bm{z}(t) + \bm{B} \bm{u}(t) \\
        \bm{I} \dot{\bm{z}}(t) &= \bm{P} \bm{A} \bm{P}^{-1} \bm{z}(t) + \bm{P} \bm{B} \bm{u}(t) \\
        \dot{\bm{z}}(t) &= \tilde{\bm{A}} \bm{z}(t) + \tilde{\bm{B}} \bm{u}(t)
    \end{split}
    .\end{equation}
    
    Thus, substituting $\bm{x}(t) = \bm{P}^{-1} \bm{z}(t)$ in the output equation results in:
    \begin{equation}
    \begin{split}
        \bm{y}(t) &= \bm{C} \bm{x}(t) + \bm{D} \bm{u}(t) \\
        \bm{y}(t) &= \bm{C} \bm{P}^{-1} \bm{y}(t) + \bm{D} \bm{u}(t) \\
        \bm{y}(t) &= \tilde{\bm{C}} \bm{z}(t) + \tilde{\bm{D}} \bm{u}(t)
    \end{split}
    .\end{equation}
\end{proof}

By this theorem is it clear that, after transforming the state-vector, the entire dynamical model $(\bm{A}, \bm{B}, \bm{C}, \bm{D})$ changes. This would imply that the analysis of the original system, for its properties and time response, is not valid for the similar transformed system. However, as discussed before, these transformations accounts for the same system when observed through a different reference basis. Therefore, it is expected that the model presents the same analysis results, as demonstrated below.

\begin{boxed-theorem}{} \label{th:simTrans01}
    Consider a system in State-Space form with matrix $\bm{A}$. Consider also a similarity transformation $\bm{z}(t) = \bm{P} \bm{x}(t)$ that results in a similar matrix $\tilde{\bm{A}} = \bm{P} \bm{A} \bm{P}^{-1}$. In this case, $\bm{A}$ and $\tilde{\bm{A}}$ have the same set of eigenvalues.
\end{boxed-theorem}

\begin{proof}
    The eigendecomposition problem is defined as $\bm{A} \bm{v} = \bm{\lambda} \bm{v}$. From the similarity transformation, $\tilde{\bm{A}} = \bm{P} \bm{A} \bm{P}^{-1}$ leads to $\bm{A} = \bm{P}^{-1} \tilde{\bm{A}} \bm{P}$. Substituting this in the eigendecomposition:
    \begin{align}
    \begin{split}
        \bm{P}^{-1} \tilde{\bm{A}} \bm{P} \bm{v} & = \bm{\lambda} \bm{v} \\
        \tilde{\bm{A}} \bm{P} \bm{v} & =  \bm{\lambda} \bm{P} \bm{v}
    \end{split}
    .\end{align}
   
    Considering the transformed eigenvector $\tilde{\bm{v}} = \bm{P} \bm{v}$, it is clear that $\tilde{\bm{A}} \tilde{\bm{v}} = \bm{\lambda} \tilde{\bm{v}}$, implying that the matrices $\bm{A}$ and $\tilde{\bm{A}}$ shares the same set of eigenvalues $\bm{\lambda}$.
\end{proof}

It is clear from past results that the fact that both the matrices shares the same set of eigenvalues directly implies that they provide the same dynamical responses and, as will be shown later, the same general properties. Therefore, the similarity transformation consists in a method to produce new State-Spaces representations that emphasizes some geometrical perspective of the model, hopefully in some perspective that helps analyze a specific property, without actually changing the relationship of the original model with the physical system.

The use of similarity transformations can also benefits the computation of functions of the matrices of the State-Space representation, given that they impose a desirable structure to this matrix. With this motivation, a popular transformation that provides a new representation with computational advantages is the Similarity transformation.

\begin{boxed-theorem}{(Diagonalization)}
    Consider a $n$-dimensional system in State-Space form represented by the matrices $(\bm{A}, \bm{B}, \bm{C}, \bm{D})$, such that the matrix $\bm{A}$ has $n$ distinct real eigenvalues, i.e., $\bm{\lambda} \in \mathbb{R}^n$. Performing the transformation $\bm{z}(t) = \bm{V} \bm{x}(t)$, where $\bm{V} = [\bm{v}_1, \bm{v}_2, ..., \bm{v}_n]$ is the modal matrix composed by the eigenvectors $\bm{v}_i \in \mathbb{R}^n$, $i \in [1, ..., n]$, of matrix $\bm{A}$, the resulting transformed matrix $\bm{\Lambda} = \bm{V} \bm{A} \bm{V}^{-1}$ is diagonal.
\end{boxed-theorem} 

\begin{proof}
    Since the eigenvalues are real and distinct, the eigenvectors must be linearly independent, proving that the inverse $\bm{V}^{-1}$ always exist and that it is a feasible transformation matrix. Using the identity for the eigendecomposition of matrix $\bm{A}$:
    \begin{align}
    \begin{split}
        \bm{\lambda} \bm{v} &= \bm{A} \bm{v} \\
        \begin{bmatrix} \lambda_1 \bm{v}_1 & \lambda_2 \bm{v}_2 & \cdots & \lambda_n \bm{v}_n \end{bmatrix} &= \begin{bmatrix} \bm{A} \bm{v}_1 & \bm{A} \bm{v}_2 & \cdots & \bm{A} \bm{v}_n  \end{bmatrix} \\
        \begin{bmatrix} \bm{v}_1 & \bm{v}_2 & \cdots & \bm{v}_n \end{bmatrix} \begin{bmatrix} \lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n  \end{bmatrix} & = \bm{A} \begin{bmatrix} \bm{v}_1 & \bm{v}_2 & \cdots & \bm{v}_n \end{bmatrix} \\
        \begin{bmatrix} \lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n  \end{bmatrix} & = \begin{bmatrix} \bm{v}_1 & \bm{v}_2 & \cdots & \bm{v}_n \end{bmatrix}^{-1} \bm{A} \begin{bmatrix} \bm{v}_1 & \bm{v}_2 & \cdots & \bm{v}_n \end{bmatrix} \\
        \bm{\Lambda}  &= \bm{V}^{-1} \bm{A} \bm{V}
    \end{split}
    .\end{align}
    
    Concluding that $\bm{\Lambda}$ is a diagonal matrix which elements are the eigenvalues of matrix $\bm{A}$.
\end{proof}

This result can be easily extended to the case where the eigenvalues are conjugate complex pairs, but each pair is distinct. A model with a diagonal matrix $\bm{A}$ has the nice property that the evolution of the states are decoupled, in the sense that each state evolution is a linear function of itself. A geometrical interpretation of this transformation is that the eigenvectors of this matrix produces a basis that encode information about the interaction between those states, while the interaction in the original formulation was the linear combination of the modes. 

In this diagonalization procedure, the resulting elements of the matrix $\bm{\Lambda}$ are the very own eigenvalues of the original matrix $\bm{A}$, which allows for a direct interpretation of the system response by just visualizing this matrix. Furthermore, it is easy to verify that the state-transition matrix for the transformed matrix, $e^{\bm{\Lambda} t}$, is also a diagonal matrix whose elements are the modes of the system, and can be easily computed:

\begin{align}
\begin{split}
    e^{\bm{\Lambda} t} & = \sum_{k=0}^{\infty} \cfrac{A^k t^k}{k!   }  = \sum_{k=0}^{\infty} \cfrac{t^k}{k!} \begin{bmatrix} \lambda_1^k & 0 & \cdots & 0 \\ 0 & \lambda_2^k & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n^k  \end{bmatrix} = \begin{bmatrix} \cfrac{t^k\lambda_1^k}{k!} & 0 & \cdots & 0 \\ 0 & \cfrac{t^k\lambda_2^k}{k!} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \cfrac{t^k\lambda_n^k}{k!} \end{bmatrix} \\ 
    &= \begin{bmatrix} e^{\lambda_1 t} & 0 & \cdots & 0 \\ 0 & e^{\lambda_2 t} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & e^{\lambda_n t} \end{bmatrix}
\end{split}
.\end{align}

In the case that the eigenvalues are not all distinct, it may be not possible to design a modal matrix in the same way, since the eigenvectors could not be all linearly independent, and the matrix would not form a basis. In those cases, however, it is still possible to design a generalized modal matrix that transform the original matrix $\bm{A}$ to a quasi-diagonal matrix $\bm{J}$, where there will be decoupled block of states. This similarity transformation is known as the Jordan form \cite{Strang:2016}, and it generalizes the notion of diagonalization for any arbitrary matrix.

\section{Stability, Controlability and Observability}

When moving from a discussion of the models in the perspective of dynamical system analysis to a perspective of control theory, it is necessary to define and analyze some important properties of a system. These properties are characteristic to the system, analyzed through a model, but they account directly to questions relating the control objectives and the instrumentation, as it will be shown later.

The first property to be discussed consists in the stability of a system. An instable system, as the name suggests, is a system whose response does not converge to a specific value and rather oscillates or grows unbounded. In physical scenarios, unstable systems are problematic, since their response to external stimuli can result in dangerous situations to itself and, maybe, to the environment around it. Because of this, determining the stability of a system is a crucial procedure into analyzing a system that will be controlled. Under the several quantitative methods to determine if a system is indeed stable, given a mathematical model, a popular and practical one is the Bounded-Input Bounded-Output (BIBO) stability criteria.

\begin{boxed-definition}{(BIBO Stability)} \label{def:BIBOStab}
    A dynamical system is defined as BIBO stable if every bounded input stimuli $| \bm{u}(t) | \leq \epsilon < \infty$ produces in it a bounded output response $| \bm{y}(t) | \leq \delta < \infty$.
\end{boxed-definition}

The main result behind this criteria is that the natural response of a system should vanish as time evolves, i.e., $\bm{x}(t) = \bm{0}$ as $t \to \infty$. This result is very intuitive from Theorem \ref{th:lagrangeForm}, since the vanishing of the natural response implies that $e^{\bm{A} t} = \bm{0}$ as $t \to \infty$ and the forced response is expected to be bounded, if $\bm{u}(t)$ is bounded. Since the state-transition matrix is a linear combination of the modes, and the modes are exponential functions of the eigenvalues of the matrix $\bm{A}$, it is possible to determine a condition for stability in the light of these quantities.

\begin{boxed-theorem}{(BIBO Stability in SS)} \label{th:BIBOStab}
    A system in State-Space form, represented by a matrix $\bm{A} \in \mathbb{R}^{n \times n}$ with eigenvalues $\bm{\lambda} \in \mathbb{R}^n$, is BIBO stable if and only if $\mathbb{R}e[\lambda_i] < 0,\ \forall i \in [1,2,...,n]$.
\end{boxed-theorem}

A detailed proof of this theorem can be found in Appendix A. First of all, note that this criteria depends only on the eigenvalues of matrix $\bm{A}$, so the stability property of a system is invariant to any similarity transformation, since $\bm{A}$ and any transformed matrix $\tilde{\bm{A}} = \bm{P} \bm{A} \bm{P}^{-1}$ shares the same eigenvalues. From the previous results it is also known that the real part of the eigenvalues, independent of their multiplicity or domain, appears as the arguments of the exponential functions that are the system modes. Therefore, an eigenvalue with a negative real part will produce a mode that is a exponential decay, as this theorem indicates. By the same argument, if the matrix $\bm{A}$ has at least one eigenvalue $\lambda_j = 0$ such that $\mathbb{R}e[\lambda_i] \leq 0,\ \forall i \in [1,2,...,n]$, then the mode associated with this eigenvalue is a constant and the natural response becomes bounded. This configuration is known as a marginally stable condition, in the BIBO perspective. The time response of a $2$-nd order system is shown Fig. \ref{fig:stability01} for three different poles configurations, given an unitary step.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/stability01}
    \caption{Stability of forced responses given the positions of the system poles.}
    \label{fig:stability01}
\end{figure} 

Later chapters will discuss the possibility of stabilize an unstable system through a controller. There are, however, some restrictions to the possibility of controlling or not the states of a system, which includes the necessity of discussing \textit{controllability}. The controllability of a system states whether it is possible to calculate an input signal that drive the system to any arbitrary state or not, given some time restriction. This property accounts exclusively for this possibility, in the sense that it does not account for the operational feasibility of actually applying this input signal into a physical system, since it may need more energy than an actuator can produce. 

\begin{boxed-definition}{(Controllability)}
    A system in State-Space form with matrices $(\bm{A}, \bm{B})$ is said to be controllable if, for any initial state $\bm{x}(t_0) = \bm{x}_0$ and terminal state $\bm{x}(T) = \bm{x}_T$, $T < \infty$, there exists an input signal $\bm{u}(t)$, $t \in [t_0, T]$, that can transfer $\bm{x}(t_0)$ to $\bm{x}(T)$. Otherwise, the system is said to be uncontrollable. 
\end{boxed-definition}

There are several methods to analyze the controllability of a system given a mathematical model and the definition above. A popular criteria introduces the concept of a controllability matrix and has a nice geometrical interpretation.

\begin{boxed-theorem}{(Controllability in SS)}
    Consider a system in linear State-Space form with matrices $(\bm{A}, \bm{B})$ and the controllability matrix $\bm{\mathcal{C}} \in \mathbb{R}^{n \times nr}$ defined as:
    \begin{equation}
        \bm{\mathcal{C}} = \begin{bmatrix} \bm{B} & \bm{A} \bm{B} & \bm{A}^2 \bm{B} & \cdots & \bm{A}^{n-1} \bm{B} \end{bmatrix}
    .\end{equation}
    
    The system is controllable if and only if $\bm{\mathcal{C}}$ has full row rank.
\end{boxed-theorem}   

An intuition behind this theorem is that the full row rank condition implies that $\mathcal{C}$ has $n$ linearly independent columns, therefore these columns can be used as a basis for what is known as the \textit{controllable subspace}. To better understand that, consider the following forced solution $\bm{x}_f(t)$ given by the Lagrange formula:
\begin{equation}
    \bm{x}_f(t) = \int_{0}^{t} e^{\bm{A} (t-\tau)} \bm{B} \bm{u}(\tau) d\tau
.\end{equation}

From the Cayley-Hamilton theorem, shown in the Sylvester expansion at Theorem \ref{th:sylvester01}, it is possible to represent $e^{\bm{A}(t-\tau)}$ as a linear combination of scalars $\beta_i(t-\tau)$ and powers of the matrix $\bm{A}^i$, $i \in [0, 1,..., n-1]$. Thus, using this theorem and substituting $\tau_2 = t - \tau$ for an easier manipulation, the forced response can be represented as:
\begin{align}
\begin{split}
    \bm{x}_f(t) &= \int_{0}^{t} \left( \sum_{i=0}^{n-1} \beta_i(\tau_2) \bm{A}^i \right) \bm{B} \bm{u}(t - \tau_2) d\tau_2 = \sum_{i=0}^{n-1} \left( \bm{A}^i \bm{B} \right) \int_{0}^{t} \beta_i(\tau_2) \bm{u}(t - \tau_2)  \\
        &= \sum_{i=0}^{n-1} \left( \bm{A}^i \bm{B} \right) \tilde{\beta}_i(\bm{u}, t)
\end{split}
,\end{align}

\noindent where $\bm{A}^i \bm{B}$ are the columns of the matrix $\bm{\mathcal{C}}$ and $\tilde{\beta}_i(\bm{u}, t)$ is a function that depends only on the input signal $\bm{u}(t)$ and time $t$. This result implies that the unforced response $\bm{x}_f(t)$ is a linear combination given by the columns of $\bm{\mathcal{C}}$. If $\bm{\mathcal{C}}$ has $n$ linearly independent columns, than this linear combination spans the entire $n$-dimensional space, i.e., the entire state space, and thus any desirable state vector $\bm{x}^*$ can be reached. If the column rank of $\bm{\mathcal{C}}$ is less than $n$, then only a subspace of smaller dimension can be reached through the forced response.

While the discussion on controllability concerns the possibility of driving a system to a desirable state through an actuator signal, there is also the necessity to discuss the possibility of determining the internal state of a system given the output signal $\bm{y}(t)$. This property, known as \textit{observability}, comes from the fact that any output of a system, related to the states through the matrix $\bm{C}$, may be a combination of states, and that some states may not even be present in the output signal. In conclusion, it is necessary to know if it is possible to reconstruct $\bm{x}(t)$ directly through $\bm{y}(t)$. 

\begin{boxed-definition}{Observability}
    A system in State-Space form with matrices $(\bm{A}, \bm{C})$ is said to be observable if, given an input signal $\bm{u}(t)$ and output signal $\bm{y}(t)$, over the interval $t \in [t_0, T]$, it is possible to uniquely determine the value of the initial state $\bm{x}(t_0)$. Otherwise, the system is said to be unobservable.
\end{boxed-definition}

Similarly to the controllability property, there are several ways to analyze the observability of a system, given a mathematical model. A popular criteria introduces the concept of a observability matrix.

\begin{boxed-theorem}{(Observability in SS)}
    Consider a system in linear State-Space form with matrices $(\bm{A}, \bm{C})$ and the observability matrix $\bm{\mathcal{O}} \in \mathbb{R}^{nq \times n}$ defined as:
    \begin{equation}
        \bm{\mathcal{O}} = \begin{bmatrix} \bm{C} \\ \bm{C} \bm{A} \\ \bm{C} \bm{A}^2 \\ \vdots \\ \bm{C} \bm{A}^{n-1} \end{bmatrix}
    .\end{equation}
    
    The system is observable if and only if $\bm{\mathcal{O}}$ has full column rank.
\end{boxed-theorem}   

The interpretation of this theorem follows the same intuition of before: if the matrix $\bm{\mathcal{O}}$ has full column rank, then it can be used as a basis to span a subspace with the same dimension as the State-Space. In fact, the proof of both theorems follows the same procedures and there is a direct relationship between controllability and observability, known as the Theorem of Duality.

The concepts of controllability and observability just presented, together with the conditions to fulfill these properties, were first introduced by \cite{Kalman:1960}. The geometrical interpretations of both theorems may suggest a practical solution for the cases where a system is uncontrollable or unobservable. In the first case, the solution would be to add specific actuators to the system, in the condition that they are linearly independent between themselves and the ones actually in operation. The same procedure can be done to solve the unobservable problem, but adding more sensors instead. Those procedures would change the matrices $\bm{B}$ and $\bm{C}$, without changing the system dynamics, and could ensure the necessary conditions in matrices $\bm{\mathcal{C}}$ and $\bm{\mathcal{O}}$. Of course, the implementation of such instrumentation may not be practical, due to technical or economical constraints. Of course, if including these additional devices is not feasible, one can still control and observe a given subset of the state variables.

\section{Response Analysis in the Frequency Domain}

Although the response of dynamical systems are naturally perceived in time, there are advantages of analyzing the models in a frequency domain perspective. This analysis differs from a simple time domain analysis from the fact that, in a steady-state regime, the response of a linear system for a sinusoidal input is itself sinusoidal, with the same frequency but different amplitude and phase, as illustrated at Fig. \ref{fig:freq01}.

\begin{figure}[ht] 
    \centering
    \begin{subfigure}{0.25\textwidth}   
        \centering
        $u(t) = M_i cos(\omega t + \phi_i)$\par\medskip
        \includegraphics[scale=0.55]{chapter2/freq01_1}
    \end{subfigure}
    \begin{subfigure}{0.38\textwidth}   
        \centering
        \includegraphics[scale=0.6]{chapter2/freq01_2}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}   
        \centering
        $y(t) = M_o cos(\omega t + \phi_o)$\par\medskip
        \includegraphics[scale=0.55]{chapter2/freq01_3}
    \end{subfigure}
    
    \caption{Illustration of the steady-state response of LTI systems to sinusoidal inputs.}
    \label{fig:freq01}
\end{figure}

A common representation is the phasor representation, where $M \angle \phi = M cos(\omega t + \phi)$. In the context of SISO Input-Output models, the response of the system can be summarized by a transfer function which is also a phasor:
\begin{equation}
    g(t) = \cfrac{y(t)}{u(t)} = \cfrac{M_o \angle \phi_o}{M_i \angle \phi_i} = M_g \angle \phi_g
,\end{equation}

\noindent where $M_g = M_o / M_i$ and $\phi_g = \phi_o - \phi_i$. Notice that this formulation makes the time-dependance implicit in the system response, since it is periodic in this case. For this reason, the system response can be visualized as a function of frequency rather than a function of time, and the properties of the system can be accessed in this way. The two most popular techniques for frequency response analysis are Bode plots \cite{Bode:1945} and Nyquist diagrams \cite{Nyquist:1932}. The first is a direct plot of $M_g(\omega)$ and $\phi_g(\omega)$ for several values of $\omega$, making $M_g(\omega) = 20 log \left| G(j\omega) \right|$ and $\phi_g(\omega) = \angle G(j\omega)$, where $G(j\omega)$ is a transfer function of a system evaluated for an input signal with exclusively oscillatory components. The Nyquist diagram, in the other hand, is a direct phasor visualization obtained by applying the Argument Principle to a contour containing the entire right-hand side of the complex plane. Both visualizations are depicted in Fig. \ref{fig:freq02}, for a Transfer function obtained by applying the transformation of Theorem \ref{th:SSToIO} into the system from \eqref{eq:stateRespEx01}:
\begin{equation} \label{eq:IOSys01}
    \bm{G}(s) = \begin{bmatrix}
        \cfrac{3.81}{s + 5.92} & \cfrac{-1.09s - 3.33}{s^2 + 10.62s + 27.84}
    \end{bmatrix}
.\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter2/freq02}
    \caption{Visualization of the two-states system in \eqref{eq:IOSys01} in Bode plots (left) and Nyquist diagrams (right). The blue and orange lines represents the states $x_1$ and $x_2$, respectively.}
    \label{fig:freq02}
\end{figure}

The immediate advantage of these visualizations is that is not necessary to compute an entire simulation of this system, until it reaches steady-state, to be able to perform analysis, which is really critic for high-dimensional systems with slow time-constants. In addition to that, these visualizations (specially the Bode plot) can be easily sketched by hand with fairly accuracy, allowing for some understanding of the system without need for solving differential equations or the inverse Laplace transforms. For these reasons, frequency responses methods for analyzing systems were ubiquitous for many years in industry applications, and some properties assessments, such as closed-loop stability, are still better understood under this formulation, as it will be shown in later chapters.

% 3 - Controller Design
% ---------------------------------------------------------------
\clearpage
\chapter{Controller Synthesis}

This chapter discusses the general results and properties for the design of dynamical controllers, focusing on feedback architectures. The devices are motivated and formulated using the State-Space model for dynamical systems, so the feedback is performed on the state response rather than the system output. For this reason, the results in this section focus on these equations, where the output equations is made implicit.

\section{State Feedback Controllers}

In modern control theory, advances in computer performance and in the methods themselves have made the design of controllers using State-Space models feasible for real-world applications. This is usually desirable since, as shown in the latter chapter, these kind of models provides a practical solution to understand dynamical system response and properties, so it is natural to want a controller design technique that accounts for that representation. The most basic, yet most popular, feedback controller used in those settings is the \textit{Full-State Feedback Controller}, defined below. 

\begin{boxed-definition}{Full-State Feedback}
    Given a linear system in State-Space representation, the input signal $\bm{u}(t)$ is calculated through feedback of the states as application of the linear control law:
    \begin{equation}
        \bm{u(t)} = \pi(\bm{r}, \bm{x}, t) = \bm{r}(t) - \bm{K} \bm{x}(t)
    ,\end{equation}
    
    \noindent where $\bm{r} : \mathbb{R} \rightarrow \mathbb{R}^{n}$ is a state reference signal that the system must follows and $\bm{K} \in \mathbb{R}^{r \times n}$ is the \textit{feedback gain matrix}.
\end{boxed-definition}

The control law $\pi(\cdot)$ is linear and time-invariant, which makes the analysis of the closed-loop system similar to the one used in open-loop configurations. A schematic of the closed-loop system is shown at Fig. \ref{fig:feedback01}. Of course, this is a choice of control law, and feedback controllers can also be defined using nonlinear or time-dependent functions. Notice that this new definition for the calculation of $\bm{u}(t)$ allows for the following closed-loop representation of the system:
\begin{align}
\begin{split}
    \dot{\bm{x}}(t) &= \bm{A} \bm{x}(t) + \bm{B} \left( \bm{r}(t) - \bm{K} \bm{x}(t) \right) \\
    \dot{\bm{x}}(t) &= \left( \bm{A} - \bm{B} \bm{K} \right) \bm{x}(t) + \bm{B} \bm{r}(t)
\end{split}
.\end{align}

\begin{figure}[ht]
    \centering
    \resizebox{!}{!}{
    \begin{tikzpicture}[auto, node distance=2cm,>=latex', scale=0.2]
        % We start by placing the blocks
        \node [input, name=input] {};
        \node [sum, right of=input, node distance=4em] (fbckSum) {};
        \node [block, right of=fbckSum] (inputMatrix) {$\bm{B}$};
        \node [sum, right of=inputMatrix, node distance=4em] (stateSum) {};
        \node [block, right of=stateSum, node distance=4em] (integral) {$\int   $};
        \node [block, right of=integral, node distance=8em] (outputMatrix) {$\bm{C}$};
        \node [output, right of=outputMatrix] (output) {};
        \node [block, below of=integral] (stateMatrix) {$\bm{A}$};
        \node [block, below of=stateSum, node distance=10em, label={below:Controller}] (fbckGain) {$\bm{K}$};
        
        \begin{scope}[on background layer]
            \node [fit=(inputMatrix) (stateMatrix) (outputMatrix), fill= myBlue!10, rounded corners, inner sep=.4cm, label={[xshift=3.5em, myBlue!90]above left:Open-Loop System}] {};
        \end{scope}     
        
        % Once the nodes are placed, connecting them is easy.       
        \draw [draw,->] (input) -- node[pos=0.1] {$\bm{r}$} node[pos=0.8] {$+$}  (fbckSum);
        \draw [->] (fbckSum) -- node[pos=0.3]{$\bm{u}$} (inputMatrix);
        \draw [->] (inputMatrix) -- node[pos=0.8]{$+$} (stateSum);
        \draw [->] (stateSum) -- node[pos=0.5]{$\dot{\bm{x}}$} (integral);
        \draw [->] (integral) -- node[name=bk1,pos=0.5]{} node[name=bk2,pos=0.5]{} node[pos=0.5]{$\bm{x}$} (outputMatrix);
        \draw [->] (outputMatrix) -- node[pos=0.6]{$\bm{y}$} (output);
        \draw [->] (bk1) |- (stateMatrix);
        \draw [->] (bk2) |- (fbckGain);
        \draw [->] (stateMatrix) -| node[pos=0.95]{$+$} (stateSum);
        \draw [->] (fbckGain) -| node[pos=0.97]{$-$} (fbckSum);
    \end{tikzpicture} 
    }
    \caption{Block diagram of a state-feedback closed-loop system.}
    \label{fig:feedback01}
\end{figure}

From this is clear that the inclusion of a feedback controller in the loop is equivalent to transform an open-loop system into a new system $(\bm{A}_{cl}, \bm{B})$, with $\bm{A}_{cl} = \bm{A} - \bm{B} \bm{K}$, whose manipulated variables is a reference signal $\bm{r}(t)$ but the controlled variables are still the same. Since $\bm{K}$ is an arbitrary matrix, it is possible to change the behavior of the closed-loop system, and consequently the controlled variables response, by designing this matrix. To understand better the capabilities of the state feedback, consider the following theorems.

%\begin{boxed-theorem}
%   Given any matrix $\bm{K} \in \mathbb{R}^{r \times n}$, the closed-loop system described by matrices $(\bm{A}-\bm{B}\bm{K}, \bm{B})$ is controllable if and only if the open-loop system $(\bm{A}, \bm{B})$ is controllable.
%\end{boxed-theorem} 
%
%\begin{proof}
%   From definition, the controllability matrix $\bm{\mathcal{C}}_{cl}$ of the closed-loop system is given by:
%   \begin{equation}
%       \bm{\mathcal{C}}_{cl} = \begin{bmatrix} \bm{B} & (\bm{A}-\bm{B}\bm{K})\bm{B} & (\bm{A}-\bm{B}\bm{K})^2 \bm{B} & \cdots & (\bm{A}-\bm{B}\bm{K})^{n-1} \bm{B} \end{bmatrix}
%   \end{equation}
%   
%   It is possible to factorize this matrix as:
%   \begin{equation}
%       \bm{\mathcal{C}}_{cl} = \bm{\mathcal{C}} 
%       \begin{bmatrix} \bm{I} & -\bm{K} \bm{B} & -\bm{K} (\bm{A} - \bm{B}\bm{K})\bm{B} & \cdots & -\bm{K} (\bm{A} - \bm{B}\bm{K})^{n-3}\bm{B} & -\bm{K} (\bm{A} - \bm{B}\bm{K})^{n-2}\bm{B} \\ 
%       0 & \bm{I} & -\bm{K} \bm{B} & \cdots & -\bm{K} (\bm{A} - \bm{B}\bm{K})^{n-4}\bm{B} & -\bm{K} (\bm{A} - \bm{B}\bm{K})^{n-3}\bm{B} \\ 
%       \vdots & \vdots & \vdots & \iddots & \vdots & \vdots \\
%       0 & 0 & 0 & \cdots & -\bm{K} \bm{B} & -\bm{K} (\bm{A} - \bm{B}\bm{K})\bm{B} \\
%       0 & 0 & 0 & \cdots & \bm{I} & -\bm{K} \bm{B} \\
%       0 & 0 & 0 & \cdots & 0 & \bm{I} \end{bmatrix}
%   \end{equation}
%   
%   Notice that the multiplication $\bm{K} \bm{B} \in \mathbb{R}^{1 \times r}$ and $\bm{I} \in \mathbb{R}^{r \times r}$. Since the rightmost matrix is upper-triangular, and has no zeros in the main diagonal, the rank of $\bm{\mathcal{C}}$ and $\bm{\mathcal{C}}_{cl}$ are equal. Therefore, if $\bm{\mathcal{C}}$ has full row rank, so has $\bm{\mathcal{C}}_{cl}$, proving the statement.
%\end{proof}

\begin{boxed-theorem}{(Controller Canonical Form)} \label{th:controlCanon}
    If a SISO system in State-Space representation is controllable, then by applying the transformation $\bm{z}(t) = \bm{P}\bm{x}(t)$, for a matrix $\bm{P}$ calculated as the inverse of:
    \begin{equation}
        \bm{P}^{-1} = \bm{\mathcal{C}} \begin{bmatrix}
        1 & \alpha_1 & \cdots  & \alpha_{n-1} \\
        0 & 1 & \cdots & \alpha_{n-2} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots &1
        \end{bmatrix}
    ,\end{equation}
    
    \noindent where $[\alpha_1, \alpha_2, ..., \alpha_{n-1}]$ are the $n-1$ first coefficients of the characteristic polynomial $\Delta(s) = det(s\bm{I} - \bm{A})$, the resulting representation is in the \textit{controller canonical form} given as:
    \begin{align}
    \begin{cases}
        \dot{\bm{z}}(t) = \begin{bmatrix}
            -\alpha_1 & -\alpha_2 & \cdots & -\alpha_{n-1} & -\alpha_n \\
            1 & 0 & \cdots & 0 & 0 \\
            \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & \cdots & 1 & 0 
        \end{bmatrix} \bm{z}(t) + \begin{bmatrix}
            1 \\ 0 \\ \vdots \\ 0
        \end{bmatrix} u(t) \\
        y(t) = \begin{bmatrix} \beta_1 & \beta_2 & \cdots & \beta_n \end{bmatrix} \bm{z}(t)
    \end{cases}
    .\end{align}
\end{boxed-theorem} 

Details for the proof of this theorem can be found in \textbf{[reference]}. An equivalent result can be proved for MIMO systems \textbf{[reference]}, but the result is very verbose and does not highlight the main results for the state-feedback. Using the canonical form just presented, it is possible to discuss a strong result for the state feedback controller.

\begin{boxed-theorem}{(Pole-Placement Method)} \label{th:polePlace}
    If a system in State-Space representation is controllable, then by state feedback using a gain matrix $\bm{K} \in \mathbb{R}^{r \times n}$ the eigenvalues of $\bm{A}_{cl}=\bm{A}-\bm{B}\bm{K}$, the poles of the closed-loop system, can arbitrarily be assigned anywhere in the complex plane, given that complex conjugate eigenvalues are assigned in pairs.
\end{boxed-theorem} 

\begin{proof}
    Consider that the system is controllable. In this case, it can be converted to the controller canonical form of Theorem \ref{th:controlCanon}. Substituting $\bm{z}(t) = \bm{P} \bm{x}(t)$ results in the following control law for the state feedback:
    \begin{equation}
        u(t) = r(t) - \bm{K} \left( \bm{P}^{-1} \bm{z}(t) \right) = r(t) - \tilde{\bm{K}} \bm{z}(t)
    .\end{equation}
    
    Applying the state feedback, the transformed closed-loop is given by:
    \begin{equation} \label{eq:fdbckContrCanon}
    \tilde{\bm{A}}_{cl} = \bm{P} (\bm{A} - B \bm{K}) \bm{P}^{-1} = \bm{P} \bm{A} \bm{P}^{-1} - \bm{P} B \bm{K} \bm{P}^{-1} = \tilde{\bm{A}} - \tilde{B} \tilde{\bm{K}}
    .\end{equation}   
    
    From Theorem \ref{th:simTrans01} it is known that $\bm{A}_{cl}$ and $\tilde{\bm{A}}_{cl}$ shares the same set of eigenvalues, and, therefore, the same characteristic equations. Consider this characteristic equation in the form:
    \begin{equation}
        \Delta(s) = det(s\bm{I} - \bm{A}) = s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2} + \cdots + \alpha_{n-1} s + \alpha_n
    .\end{equation}
    
    Given a desired a set of coefficients $[\tilde{\alpha}_1, \tilde{\alpha}_2, ..., \tilde{\alpha}_n]$ of a characteristic polynomial whose roots are the desired closed-loop eigenvalues, define the transformed feedback gain matrix as:
    \begin{equation}
        \tilde{\bm{K}} = \begin{bmatrix} \tilde{\alpha}_1 - \alpha_1 & \tilde{\alpha}_2 - \alpha_2 & \cdots & \tilde{\alpha}_n - \alpha_n \end{bmatrix}
    .\end{equation}
    
    Plugging this in \eqref{eq:fdbckContrCanon}, it is easy to see that the resulting representation is:
    \begin{align} \label{eq:SSFdbckControlCanon}
    \begin{cases}
        \dot{\bm{z}}(t) = \begin{bmatrix}
            -\tilde{\alpha}_1 & -\tilde{\alpha}_2 & \cdots & -\tilde{\alpha}_{n-1} & -\tilde{\alpha}_n \\
            1 & 0 & \cdots & 0 & 0 \\
            \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & \cdots & 1 & 0 
        \end{bmatrix} \bm{z}(t) + \begin{bmatrix}
            1 \\ 0 \\ \vdots \\ 0
        \end{bmatrix} u(t) \\
        y(t) = \begin{bmatrix} \beta_1 & \beta_2 & \cdots & \beta_n \end{bmatrix} \bm{z}(t)
    \end{cases}
    ,\end{align} 
    
    \noindent whose characteristic polynomial is now described by the designed coefficients to yield the desirable eigenvalues. Since $\tilde{\bm{A}}_{cl}$ and $\bm{A}_{cl}$ shares the same set of eigenvalues, it is concluded that it is possible to assign the system poles directly through matrix $\tilde{\bm{K}}$.
\end{proof}

Notice, from that procedure, that an ``original" feedback gain matrix can be obtained as $\bm{K} = \tilde{\bm{K}} \bm{P}$ and still yield the same eigenvalues assignment directly in $\bm{A}_{cl}$ (since $\bm{P}$ is just a linear transformation). This theorem has the direct result that, under full-state feedback, the transient response of a linear system can be completely determined by including a controller, which is described by this matrix $\bm{K}$. This result is still preserved for MIMO systems, although the design of $\bm{K}$ is not so straightforward since it is not unique for a desired set of eigenvalues in this case \textbf{[reference]}. 

Using the parameters from Definition \ref{def:responseParameters}, the positions of the closed-loop system poles can be evaluated given desirable operations, and the matrix $\bm{K}$ can be hand-designed to meet these requirements. This method is known as the \textit{Pole-Placement method} for control synthesis. The algorithm below summarizes a simple procedure of designing an appropriate feedback gain matrix given desirable pole positions.

\begin{algorithm}[ht]
    \caption{Pole-Placement Method for SISO Systems}    
    \SetAlgoLined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
    \Input{state-space model $(\bm{A}, B)$ and a set of $n$ desired eigenvalues $\bm{\lambda}^*$.}
    \Output{feedback gain matrix $\bm{K}$.}
    \vskip0.25cm
    
    Calculate $[\alpha_1, \alpha_2, ..., \alpha_n]$ as the coefficients of the polynomial $\Delta(s) = det(s\bm{I} - \bm{A})$;
    
    Let $\bm{P}^{-1} = \begin{bmatrix} B & \bm{A} B & \bm{A}^2 B & \cdots & \bm{A}^{n-1} B \end{bmatrix} \begin{bmatrix}
        1 & \alpha_1 & \cdots  & \alpha_{n-1} \\
        0 & 1 & \cdots & \alpha_{n-2} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots &1
        \end{bmatrix}$;
    
    Let $\bm{P} = \left( \bm{P}^{-1} \right)^{-1}$;
    
    Calculate $[\tilde{\alpha}_1, \tilde{\alpha}_2, ..., \tilde{\alpha}_n]$ as the coefficients of the polynomial $\Delta_{cl}(s) = \prod_{i=1}^n (s - \lambda^*_i)$;
    
    Let $\tilde{\bm{K}} = \begin{bmatrix} \alpha_1 - \tilde{\alpha}_1 & \alpha_2 - \tilde{\alpha}_2 & \cdots & \alpha_n - \tilde{\alpha}_n \end{bmatrix}$;
    
    Return $\bm{K} = \tilde{\bm{K}} \bm{P}$
\end{algorithm} 

Albeit being a simples formula, this method can be used in several applications to yield controllers capable of matching performance requisites. Of course, the designer must take into account that, besides the eigenvalue assignment allows for the whole complex plane, a careless choice of eigenvalues could result in unpractical controllers, with very aggressive or oscillatory input signals. For this reason, it is necessary some knowledge of the instruments limits before designing the matrix $\bm{K}$.

\section{Regulation and Reference Tracking}

When discussing controller synthesis it is also necessary to account for which objective this device is expected to fulfill. In this case, there are two main classifications of controllers based on the operation that they impose to the system: regulators and tracking (or servo) controllers. In the case of state-feedback, these two classes of controllers differs only by what type of reference signal $\bm{r}(t)$, for an operation in a time interval $t \in [t_0, t_f]$, the system is expected to follow. The following statements gives a formal definition of a controller for regulation:

\begin{boxed-definition}{(Regulator)} \label{def:regulator}
    If a state-feedback controller has to make a system follows the reference $\bm{r}(t) = \bm{0}$, as $t \to \infty$, it is said to be a \textit{regulator}. In this case, the closed-loop state equation and equivalent solutions reduces to the following:
        \begin{align}
    \begin{matrix*}[l]
    \textbf{State Equation:} \hfill & & & \textbf{Lagrange solution:} \hfill \\
    \dot{\bm{x}}(t) = \left( \bm{A} - \bm{B} \bm{K} \right) \bm{x}(t)  & & &
    \bm{x}(t) = e^{(\bm{A} - \bm{B} \bm{K}) t} \bm{x}(t_0) \hfill
    \end{matrix*}
    .\end{align}
\end{boxed-definition}

Of course, if the feedback gain matrix impose that all poles of the system are in the left-half plane, the closed-loop is stable and the natural response will eventually converge to zero (Theorem \ref{th:BIBOStab}). Therefore, all stable feedback controllers are able to impose regulation to a system, and the characteristics of the transient response can be fully determined by the matrix $\bm{K}$. These type of controllers are used to make systems goes from nonzero initial states to the zero-state $\bm{x}(t) = \bm{0}$ and stays there, meaning that the the controller can also be used to reject disturbances. Now, one may wonder if this operation is too restrictive in the sense that the zero-state is not the desirable state in many control objectives, as is the case of reactor systems: a zero-state means that no chemical substances are being produced. However, it is common to have linear systems that are   linearized versions of nonlinear models, using the approximation from Theorem \ref{th:linearization}, meaning that the regulator actually is to impose $\Delta \bm{x}(t) = \bm{x}(t) - \bm{x}_o = \bm{0}$, i.e., drives the system to the steady-state point $\bm{x}_o$ and reject disturbances. For the sake of illustration, considers the first system of \eqref{eq:isoSys02}, repeated here:
\begin{equation} \label{eq:isoSys02Rep}
\begin{matrix}
    \bm{A} = \begin{bmatrix} 
        -5.93  &      0 \\
        \hfill 0.83   &  -4.70 
    \end{bmatrix} & b = \begin{bmatrix} 3.81 \\ -1.09 \end{bmatrix}
\end{matrix}    
.\end{equation}


Since this system was obtained by the linearized model in \eqref{eq:isoReact01} for $\bm{x}_o = [6.19, 1.09]^T$ and $u_o = 3.03$, the regulator will impose the zero-state $\Delta \bm{x} = \bm{0}$ which actually means imposing $\bm{x}(t) = [6.19, 1.09]^T$ as $t \to \infty$. A simulation is shown in Fig. \ref{fig:regulator01} for three different regulators, considering initial state $\delta \bm{x}(t) = \bm{0} - \bm{x}_o$ and vertically correcting the response and input signal by $\bm{x}_o$ and $\bm{u}_o$, respectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter3/report_ch3_1}
    \caption{Simulation of three closed-loop systems showing the input signal (left) and state responses (right) for gains $K_1 = [-0.99, 1.22]$, $K_2 = [0.78, 0.55]$ and $K_3 = [0.81, 5.20]$. The reference signal is indicated by the black dashed line.}
    \label{fig:regulator01}
\end{figure}

In contrast, the control objective could be to follow a non-constant signal $\bm{r}(t)$, or to follow a constant signal different from the zero-state, giving rise to the tracking or servomechanism controllers. A more complete discussion is needed in such cases, since there is a possibility that state-feedback is not capable of actually performing this tracking. To understand this question, consider, for simplicity, that the system must follow a constant reference $\bm{r}(t) = a$. Consider, now, an Input-Output conversion of a closed-loop SISO State-Space model, which from \eqref{eq:SSFdbckControlCanon} directly results in the transfer function:
\begin{equation}
    G(s) = \cfrac{Y(s)}{R(s)} = \cfrac{\beta_1 s^{n-1} + \beta_2 s^{n-2} + \cdots + \beta_{n-1} s + \beta_n}{s^{n} + \tilde{\alpha}_1 s^{n-1} + \tilde{\alpha}_2 s^{n-2} + \cdots + \tilde{\alpha}_{n-1} s + \tilde{\alpha}_n}
.\end{equation}

From that formulation it is clear that the response $Y(s) = G(s)R(s)$ will yield a perfect tracking if $G(s) = 1$. Moreover, if the system has to track asymptotically track this reference, this operation can be evaluated as time $t \to \infty$ or, equivalently, as the frequency $s \to 0$. Plugging this limit in the transfer function implies that a perfect tracking is always possible if $G(0) = \beta_n /  \tilde{\alpha}_n = 1$, which is not guaranteed a priori. A possible solution is to transform the reference with as $\tilde{r}(t) = F r(t)$, so that $Y(s) = G(s)\tilde{R}(s) = G(s) F R(s)$, resulting that:
\begin{equation}
    G(0) F = 1 \Rightarrow F = \cfrac{\tilde{\alpha}_n}{\beta_n}
,\end{equation}

\noindent which allows for perfect asymptotically tracking in all cases but when $\beta_n = 0$. This same reasoning can easily be extended to MIMO systems (the gain $F$ turns into a matrix). In the case of non-constant references, the same intuition could still be used, but the analysis and design of $F$ becomes more complex \textbf{[reference]}. This, however, allows for the definition of tracking controllers.

\begin{boxed-definition}{(Tracking Controllers)} \label{def:tracking}
    If a state-feedback controller has to make a system track any step reference $\bm{r}(t) \neq \bm{0}$, as $t \to \infty$, it is said to be a \textit{tracking controller}. In this case, one has to apply the \textit{feedforward gain} $F$ to correct the reference as $\tilde{\bm{r}}(t) = \bm{F}\bm{r}(t)$, resulting in the following closed-loop state equation and equivalent solution:
    \begin{align}
    \begin{matrix*}[l]
    \textbf{State Equation:} \hfill & & \textbf{Lagrange solution:} \hfill \\
    \dot{\bm{x}}(t) = \left( \bm{A} - \bm{B} \bm{K} \right) \bm{x}(t) + \bm{B} \bm{F} \bm{r}(t)  & &
    \bm{x}(t) = e^{(\bm{A} - \bm{B} \bm{K}) t} \bm{x}(t_0) + \int_{t_0}^{t} e^{(\bm{A} - \bm{B} \bm{K}) (t - \tau)} \bm{B} \bm{F} \bm{r}(\tau) d \tau \hfill
    \end{matrix*}
    .\end{align}
\end{boxed-definition}

Despite being a feasible solution, there are still problems with this definition of tracking controllers. For instance, if the system is subject to a \textit{constant additive disturbance}, which as not anticipated in the model, the resulting operation will not yield a perfect tracking. 

The problem of the previous formulation for a tracking controller is that it is not robust to actions that happens outside the model. A direct cause of this is the fact that the feedforward gain $\bm{F}$ does not benefits from the real-time corrective action of the state-feedback, but rather is calculated \textit{off-line} using the model properties. Therefore, a way to ensure a more robust operation could be to insert real-time information about the tracking error directly to the feedback corrective action. With this motivation, a new formulation of the tracking controller is given below.

\begin{boxed-definition}{(Robust Tracking Controllers)} \label{def:robustTracking}
    Given a State-space system and augmented state $\bm{x}_a(t)$ defined as:
    \begin{equation}
        \bm{x}_a(t) = \int_{0}^{t} \bm{r}(\tau) - \bm{C} \bm{x}(\tau) d\tau \Longrightarrow \dot{\bm{x}}_a(t) = \bm{r}(t) - \bm{C} \bm{x}(t)
    .\end{equation}
    
    A robust tracking (or servo) controller, defined by the gain $\tilde{\bm{K}} = \begin{bmatrix} \bm{K} & \bm{K}_a \end{bmatrix}$, is the one which operates on the following augmented version of the original system:
    \begin{align} \label{eq:augmentedSystem}
    \begin{cases}
        \begin{bmatrix}
            \dot{\bm{x}}(t) \\
            \dot{\bm{x}}_a(t)
        \end{bmatrix} &= \begin{bmatrix}
            \bm{A} - \bm{B} \bm{K} & -\bm{B} \bm{K}_a \\ - \bm{C} & \bm{0}
        \end{bmatrix} \begin{bmatrix}
            \bm{x}(t) \\
            \bm{x}_a(t)
        \end{bmatrix} + \begin{bmatrix}
            \bm{0} \\
            \bm{I}
        \end{bmatrix} \bm{r}(t)
        \\
        \hfill \bm{y}(t) &= \begin{bmatrix}
            \bm{C} & \bm{0}
        \end{bmatrix} \begin{bmatrix}
            \bm{x}(t) \\
            \bm{x}_a(t)
        \end{bmatrix}
    \end{cases}
    \end{align}
    
    or, equivalently:
    \begin{align}
    \begin{cases}
        \tilde{\bm{x}}(t) = \tilde{\bm{A}} \tilde{\bm{x}}(t) + \tilde{\bm{B}} \tilde{\bm{r}}(t) \\
        \bm{y}(t) = \tilde{\bm{C}} \tilde{\bm{x}}(t) \hfill
    \end{cases}
    .\end{align}
\end{boxed-definition}

\begin{figure}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}[auto, node distance=2cm,>=latex']
        % We start by placing the blocks
        \node [input, name=input] {};
        \node [sum, right of=input, node distance=4em] (intFbckSum) {};
        \node [block, right of=intFbckSum, node distance=4em] (integralAction) {$\int$};
        \node [block, right of=integralAction, node distance=6em] (intActGain) {$\bm{K}_a$};
        \node [sum, right of=intActGain, node distance=4.5em] (fbckSum) {};
        \node [block, right of=fbckSum] (inputMatrix) {$\bm{B}$};
        \node [sum, right of=inputMatrix, node distance=4em] (stateSum) {};
        \node [block, right of=stateSum, node distance=4em] (integral) {$\int   $};
        \node [block, right of=integral, node distance=8em] (outputMatrix) {$\bm{C}$};
        \node [output, right of=outputMatrix] (output) {};
        \node [block, below of=integral] (stateMatrix) {$\bm{A}$};
        \node [block, below of=stateSum, node distance=10em, label={below:Controller}] (fbckGain) {$\bm{K}$};
        
        \begin{scope}[on background layer]
            \node [fit=(inputMatrix) (stateMatrix) (outputMatrix), fill= myBlue!10, rounded corners, inner sep=.4cm, label={[xshift=3.5em, myBlue!90]above left:Open-Loop System}] {};
        \end{scope}     
        
        \begin{scope}[on background layer]
            \node [fit=(integralAction) (intActGain), fill= myGreen!10, rounded corners, inner sep=.4cm, label={[xshift=4.3em, myGreen!90]above left:Integral Action}] {};
        \end{scope} 
        
        % Once the nodes are placed, connecting them is easy.       
        \draw [draw,->] (input) -- node[pos=0.1] {$\bm{r}$} node[pos=0.8] {$+$}  (intFbckSum);
        \draw [->] (intFbckSum) -- (integralAction);
        \draw [->] (integralAction) -- node[pos=0.5]{$\bm{x}_a$} (intActGain);
        \draw [->] (intActGain) -- node[pos=0.8]{$-$} (fbckSum);
        \draw [->] (fbckSum) -- node[pos=0.3]{$\bm{u}$} (inputMatrix);
        \draw [->] (inputMatrix) -- node[pos=0.8]{$+$} (stateSum);
        \draw [->] (stateSum) -- node[pos=0.5]{$\dot{\bm{x}}$} (integral);
        \draw [->] (integral) -- node[name=bk1,pos=0.5]{} node[name=bk2,pos=0.5]{} node[pos=0.5]{$\bm{x}$} (outputMatrix);
        \draw [->] (outputMatrix) -- node[name=bk3,pos=0.5]{} node[pos=0.6]{$\bm{y}$} (output);
        \draw [->] (bk1) |- (stateMatrix);
        \draw [->] (bk2) |- (fbckGain);
        \draw [->] (stateMatrix) -| node[pos=0.95]{$+$} (stateSum);
        \draw [->] (fbckGain) -| node[pos=0.97]{$-$} (fbckSum);
        \draw [->] (bk3) -- ++(0, -14em) -| node[pos=0.97]{$-$} (intFbckSum);
    \end{tikzpicture} 
    }
    \caption{Block diagram of a state-feedback closed-loop system with integral action.}
    \label{fig:tracking02}
\end{figure}

Since the augmented state $\bm{x}_a(t)$ represents an integral of the tracking error until a time $t$, this formulation is usually characterized as imposing ``integral action" to the controller. The schematic at Fig. \ref{fig:tracking02} illustrates how an integrator can be included to the block diagram of the control loop. Therefore, it must be discussed if the new gain $\tilde{\bm{K}}$ still preserves the eigenvalue assignment property of regular state-feedback gains, so that this imposed regulator would work.

\begin{boxed-theorem}{} \label{th:augmentedCtrb}
    If the SISO system described by $(\bm{A}, B)$ is controllable and its transfer function $G(s)$ has no zero at $s = 0$, then the eigenvalues of the augmented matrix $\tilde{\bm{A}}$ can be assigned arbitrary by the feedback gain $\tilde{\bm{K}}$.
\end{boxed-theorem}

\begin{proof}
    Consider a SISO controllable system. After the augmentation, the controllability matrix $\tilde{\bm{\mathcal{C}}}$ is calculated as:
    \begin{align}
    \begin{split}
        \tilde{\bm{\mathcal{C}}} &= \begin{bmatrix}
            B & \bm{A} B & \bm{A}^2 B & \bm{A}^3 B & \cdots & \bm{A}^{n-1} B \\
            0 & -C B & -C \bm{A} B & -C \bm{A}^2 B & \cdots & -C \bm{A}^{n-2} B
        \end{bmatrix} \\
        &=
            \begin{bmatrix}
                1 & -\alpha_1 & -\alpha_1^2 - \alpha_2 & -\alpha_1(\alpha_1^2 - \alpha_2) + \alpha_2 \alpha_1 - \alpha_3 & \cdots & \Delta_2(\alpha_1,...,\alpha_n) \\
                0 & 1 & -\alpha_1 & -\alpha_1^2 - \alpha_2  & \cdots & \Delta_3(\alpha_1,...,\alpha_n) \\
                0 & 0 & 1 & -\alpha_1  & \cdots & \Delta_1(\alpha_1,...,\alpha_n) \\
                0 & 0 & 0 & 1 &  \cdots & \Delta_4(\alpha_1,...,\alpha_n) \\
                \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & 1 & -\beta_1 & -\beta\alpha_1 - \alpha_2  & \cdots & \Delta_n(\alpha_1,...,\alpha_n) \\
            \end{bmatrix}
    \end{split}
    ,\end{align}
    
    \noindent where $\Delta_i(\alpha_1,...,\alpha_n)$ is a polynomial created to save space in this representation. By inspection of this matrix, it is possible to discover a pattern between the rows. Since elementary operations between the rows ${r_1, r_2, ..., r_n}$ doesn't change the matrix row rank, the last row of the matrix can be transformed as $r_n = r_n + r_{n-1}\beta_{n-2} + r_{n-2}\beta_{n-3} + \cdots + r_{2}\beta_{1}$. The result is the triangular matrix in the form:
    \begin{align}
    \begin{split}
        \tilde{\bm{\mathcal{C}}} &= 
            \begin{bmatrix}
                1 & -\alpha_1 & -\alpha_1^2 - \alpha_2 & -\alpha_1(\alpha_1^2 - \alpha_2) + \alpha_2 \alpha_1 - \alpha_3 & \cdots & \Delta_2(\alpha_1,...,\alpha_n) \\
                0 & 1 & -\alpha_1 & -\alpha_1^2 - \alpha_2  & \cdots & \Delta_3(\alpha_1,...,\alpha_n) \\
                0 & 0 & 1 & -\alpha_1  & \cdots & \Delta_1(\alpha_1,...,\alpha_n) \\
                0 & 0 & 0 & 1 &  \cdots & \Delta_4(\alpha_1,...,\alpha_n) \\
                \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & 0 & 0  & \cdots & \beta_n \\
            \end{bmatrix}
    \end{split}
    .\end{align}
    
    Since $\bm{G}(s)$ has no zeros at $s = 0$, then $\beta_n \neq 0$, meaning that $\tilde{\bm{\mathcal{C}}}$ is nonsingular and, therefore has full row rank. This concludes that the augmented system $(\tilde{\bm{A}}, \tilde{\bm{B}})$ is controllable and, from Theorem \ref{th:polePlace}, the eigenvalues of $\tilde{\bm{A}}$ can be assigned anywhere in the complex plane.
\end{proof}

This result does not scale naturally to MIMO systems, but the main intuition is the same. Basically, if the augmented matrices are controllable, the robust tracking can be achieved by the same formulation. A possible intuition on how this system performs the robust tracking and disturbance rejection can be taken from the fact that the first row of \eqref{eq:augmentedSystem} is basically a regulator, albeit from the term $\bm{B} \bm{K}_a \bm{x}_a(t)$. Because of this, a control action will always be applied whenever $\bm{x}_a(t) \neq \bm{0}$, i.e., when there is an error between the reference and the output signal, following the direction that minimizes this difference. When $\bm{x}_a(t) = \bm{0}$, this equation reduces to a simple regulator, and the disturbances are expected to be rejected. A more quantitative analysis on why this controller yields both robust tracking and disturbance rejection can be found in \textbf{[reference]}. 

For the sake of illustration, consider the same system used in \eqref{eq:isoSys02Rep}. Unfortunately, the augmented version of this \textit{single-input multiple-output} (SIMO) formulation is not controllable. However, the SISO version obtained by letting $C = \begin{bmatrix} 0, 1 \end{bmatrix}$ obeys Theorem \ref{th:augmentedCtrb}, so a robust tracking controller can be designed by state-feedback. Some simulations of closed-loop systems for this system to track are shown in Fig. \ref{fig:tracking03} for a non-constant reference consisting of a sequence of step signals.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapter3/report_ch3_2}
    \caption{Simulation of three closed-loop systems showing the input signal (left) and state response (right) for augmented gains $\tilde{K}_1 = [-0.99, 1.22, -25]$, $\tilde{K}_2 = [0.78, 0.55, -25]$ and $\tilde{K}_3 = [0.81, 5.20, -50]$. The reference signal is indicated by the black dashed line.}
    \label{fig:tracking03}
\end{figure}


\section{Deterministic State Observers}

Until now, the state feedback was discussed in the perspective that the device has direct to the real value of all states of the system. This is assumption is actually unrealistic, since the states are only observed through the output signal $\bm{y}(t)$, that maps the states through the matrix $\bm{C}$, which itself is not assumed to be always equal to the identity matrix. In practice, this means that some states could not be measured, due to technical difficulties or economic reasons, or that the instrumentation available is not perfect, and the observations are prone to deviate from the real value. In reactor systems, for instance, is hard to measure the actual value of chemical compounds concentrations in small scales, and the measuring process itself could be very slow or simply unpractical in operational conditions \textbf{[reference]}. Since the state-vector is necessary for the state-feedback to compute the input to the system, this section discusses how to develop devices that can reconstruct information about the states from the observations of the available sensor.

A device that generates a state-vector $\bm{x}(t)$ from the output signal $\bm{y}(t)$ is known as \textit{state observer}, or \textit{state estimator} in some cases. Amongst the several possible configurations, a very practical and popular one is the \textit{Luenberger observer}, which is defined below.

\begin{boxed-definition}{(Luenberger Observer)} \label{def:luenberger}
    Given a system in State-Space with output signal $\bm{y}(t) : \mathbb{R} \rightarrow \mathbb{R}^p$ and a observer gain $\bm{L} \in \mathbb{R}^{n \times p}$, the estimated state-vector $\hat{\bm{x}}(t)$ is represented by the observer system:
    \begin{equation}
        \dot{\hat{\bm{x}}}(t) = \bm{A} \hat{\bm{x}}(t) + \bm{B} \bm{u}(t) + \bm{L} \left( \bm{y}(t) - \bm{C} \hat{\bm{x}}(t) \right)
    ,\end{equation}
    
    \noindent or, equivalently:
    \begin{equation} \label{eq:luenberger02}
        \dot{\hat{\bm{x}}}(t) = \left( \bm{A} - \bm{L} \bm{C} \right) \hat{\bm{x}}(t) + \bm{B} \bm{u}(t) + \bm{L} \bm{y}(t)
    .\end{equation}
\end{boxed-definition}

The observer system works as a parallel system that is simulated alongside with the actual system, as illustrate in Fig. \ref{fig:luenberger01}. The expected result is that the observer yields $\hat{\bm{x}}(t) = \bm{x}(t)$, as time $t \to \infty$. Alternatively, it is possible to create a variable $\bm{e}(t) = \bm{x}(t) - \hat{\bm{x}}(t)$ such that, using \eqref{eq:luenberger02}:
\begin{align}
\begin{split}
    \dot{\bm{e}} &= \bm{x} - \hat{\bm{x}} \\ 
        &= \left( \bm{A} \bm{x} + \bm{B} \bm{u} \right) - \left( \left( \bm{A} - \bm{L} \bm{C} \right) \hat{\bm{x}} + \bm{B} \bm{u} + \bm{L} \bm{C} \bm{x} \right) \\
        &= \left( \bm{A} - \bm{L} \bm{C} \right) \bm{x} - \left( \bm{A} - \bm{L} \bm{C} \right) \hat{\bm{x}} \\
        &= \left( \bm{A} - \bm{L} \bm{C} \right) \left( \bm{x} - \hat{\bm{x}} \right) \\
        &= \left( \bm{A} - \bm{L} \bm{C} \right) \bm{e}
\end{split}
,\end{align}

\noindent which implies that the observer asymptotically tracks the actual state-vector if $\bm{e}(t) = \bm{0}$ as $t \to \infty$. Analyzing the equation above, it is intuitive to notice that this result can be guaranteed if all the eigenvalues of matrix $\bm{A}_{obs} = \bm{A} - \bm{L} \bm{C}$ have negative real parts. The following theorem relates this statement with the choice of a gain $\bm{L}$.

\begin{figure}[ht]
    \centering
    \resizebox{!}{!}{
    \begin{tikzpicture}[auto, node distance=2cm,>=latex']
        % We start by placing the blocks
        \node [input](input) {};
        \node [block, right of=input, minimum height=4em, node distance=18em, fill=myBlue!20] (openLoopSystem) {Open-Loop System};
        \node [output, right of=openLoopSystem, node distance=18em] (output) {};

        \node [block, below of=openLoopSystem, node distance=6em] (integralLU) {$\int   $};     
        \node [sum, left of=integralLU, node distance=4em] (stateSumLU1) {};                
        \node [sum, left of=stateSumLU1, node distance=3em] (stateSumLU2) {};       
        \node [block, left of=stateSumLU2, node distance=4em] (inputMatrixLU) {$\bm{B}$};
        \node [block, right of=integralLU, node distance=8em] (outputMatrixLU) {$\bm{C}$};
        \node [block, below of=integralLU] (stateMatrixLU) {$\bm{A}$};
        \node [block, below of=stateMatrixLU] (luenbergerGain) {$\bm{L}$};      
        \node [sum, right of=outputMatrixLU, node distance=4em] (fbckOutput) {};    
        
        \begin{scope}[on background layer]
            \node [fit=(inputMatrixLU) (luenbergerGain) (fbckOutput), fill= myRed!10, rounded corners, inner sep=.4cm, label={[xshift=-4em, myRed!90]below right:Luenberger Observer}] {};
        \end{scope} 
        
        % Once the nodes are placed, connecting them is easy.       
        \draw [draw,->] (input) -- node[pos=0.1]{$\bm{u}$} node[name=bk1,pos=0.25]{} (openLoopSystem);
        \draw [->] (openLoopSystem) -- node[name=bk2,pos=0.75]{} node[pos=0.9]{$\bm{y}$}  (output);
        
        \draw [->] (bk1) |-  (inputMatrixLU);
        \draw [->] (bk2) |- node[pos=0.8]{$+$}  (fbckOutput);
        
        \draw [->] (inputMatrixLU) -- node[pos=0.8]{$+$} (stateSumLU2);
        \draw [->] (stateSumLU2) -- node[pos=0.8]{$+$} (stateSumLU1);
        \draw [->] (stateSumLU1) -- node[pos=0.5]{$\dot{\hat{\bm{x}}}$} (integralLU);
        \draw [->] (integralLU) -- node[name=bk1,pos=0.5]{} node[pos=0.5]{$\hat{\bm{x}}$} (outputMatrixLU);
        \draw [->] (outputMatrixLU) -- node[pos=0.8]{$-$} (fbckOutput);
        \draw [->] (bk1) |- (stateMatrixLU);
        \draw [->] (fbckOutput) |- (luenbergerGain);
        \draw [->] (stateMatrixLU) -| node[pos=0.95]{$+$} (stateSumLU1);
        \draw [->] (luenbergerGain) -| node[pos=0.95]{$+$} (stateSumLU2);
    \end{tikzpicture} 
    }
    \caption{Block diagram of a State-Space system with Luenberger observer.}
    \label{fig:luenberger01}
\end{figure}

\begin{boxed-theorem}
    If a system in State-Space representation is observable, then by a Luenberger observer with gain matrix $\bm{L} \in \mathbb{R}^{n \times p}$ the eigenvalues of $\bm{A}_{obs} = \bm{A} - \bm{L} \bm{C}$ can arbitrarily be assigned anywhere in the complex plane, given that complex conjugate eigenvalues are assigned in pairs.
\end{boxed-theorem} 

\begin{proof}
    Consider that a State-Space with matrices $(\bm{A}, \bm{C})$ is observable. From the Duality Theorem \textbf{[reference]} if the pair $(\bm{A}, \bm{C})$ is observable then the pair $(\bm{A}^T, \bm{C}^T)$ is controllable. In this case, it is possible to design     a gain matrix $\bm{K}$ to assign the eigenvalues of $\tilde{\bm{A}}_{obs} = \bm{A}^T - \bm{C}^T \bm{K}$ in any desirable points in the complex space. Since the eigenvalues of a matrix are invariant to the transpose operation, the design of $K$ can also place the eigenvalues of the matrix $(\tilde{\bm{A}}_{obs})^T = \bm{A} - \bm{K}^T \bm{C}$. Therefore, making $\bm{L} = \bm{K}^T$ establishes the theorem.
\end{proof}

The procedure stated in this proof presents the similarities between closed-loop observers, such as the Luenberger observer, and closed-loop controllers as the state-feedback. Basically, the same design considerations that concerns state-feedback are important in the design of the observer gain $\bm{L}$. For instance, the eigenvalues of $\tilde{\bm{A}}_{obs} = \bm{A}^T - \bm{C}^T \bm{K}$ can be assigned such that the time evolution of the error $\bm{e}(t)$ has a desirable time constant, damping coefficient or natural frequency. In conclusion, the state-vector $\bm{x}(t)$ can be reconstructed by using a observer gain such that each eigenvalue of $\bm{A}_{obs}$ is on the left-half side of the complex plane. 

One of the main reasons to develop an observer is to allows for state-feedback controllers to access the values of the state-vector, thus being able to calculate an appropriate action to follow the reference signal. If a controller can only access the estimated state-vector $\tilde{\bm{x}}(t)$, it is possible to define a State-Space formulation for a closed-loop based on feedback from estimated states given data from a, possibly non-linear and time-varying, disturbed system.

\begin{boxed-definition}{(Feedback from Estimated States)} \label{def:fdbckLuenberger}
    Given a system in State-Space representation whose state-vector is reconstructed from a Luenberger observer of gain $\bm{L} \in \mathbb{R}^{n \times p}$ and input signal is calculated through state-feedback with gain $\bm{K} \in \mathbb{R}^{n \times r}$, its time evolution can be represented through the model:
    \begin{align} \label{eq:fdbckLuenberger01}
    \begin{cases}
        \dot{\bm{x}}(t) = \bm{A} \bm{x}(t) - \bm{B} \bm{K} \hat{\bm{x}}(t) + \bm{B} \bm{r}(t) \\
        \dot{\hat{\bm{x}}}(t) = \left(\bm{A} - \bm{L} \bm{C} - \bm{B} \bm{K} \right) \hat{\bm{x}}(t) + \bm{B} \bm{r}(t) + \bm{L} \bm{C} \bm{x}(t) 
    \end{cases}
    .\end{align}
\end{boxed-definition} 

A schematic for this formulation of feedback from estimated states is shown Fig. \ref{fig:observer02}, including the possibility for integral action. In fact, this schematic summarizes several different control architectures that includes both feedback action and state estimation, and it will be a reference whenever this work mentions physical control loops and instrumentation.

\begin{figure}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}[auto, node distance=2cm,>=latex']
        % We start by placing the blocks
        \node [input, name=input] {};
        \node [sum, right of=input, node distance=3em] (intFbckSum) {};
        \node [block, right of=intFbckSum, node distance=4em] (integralAction) {$\int$};
        \node [block, right of=integralAction, node distance=6em] (intActGain) {$\bm{K}_a$};
        \node [sum, right of=intActGain, node distance=4.5em] (fbckSum) {};
        \node [block, right of=fbckSum, node distance=6em] (inputMatrix) {$\bm{B}$};
        \node [sum, right of=inputMatrix, node distance=4em] (stateSum) {};
        \node [block, right of=stateSum, node distance=4em] (integral) {$\int   $};
        \node [block, right of=integral, node distance=8em] (outputMatrix) {$\bm{C}$};
        \node [output, right of=outputMatrix, node distance=7em] (output) {};
        \node [block, below of=integral] (stateMatrix) {$\bm{A}$};
        
        \node [block, below of=stateMatrix, node distance=12.5em] (integralLU) {$\int$};        
        \node [sum, left of=integralLU, node distance=4em] (stateSumLU) {};             
        \node [block, left of=stateSumLU, node distance=4em] (inputMatrixLU) {$\bm{B}$};
        \node [block, right of=integralLU, node distance=8em] (outputMatrixLU) {$\bm{C}$};
        \node [block, below of=integralLU] (stateMatrixLU) {$\bm{A}$};
        \node [block, above of=integralLU] (luenbergerGain) {$\bm{L}$};     
        
        \draw [->] (outputMatrix) -- node[name=bk3,pos=0.33]{} node[name=bk5,pos=0.8]{} node[pos=0.9]{$\bm{y}$} (output);
        \node [sum, anchor=base] (fbckOutput) at (bk3.base |- luenbergerGain) {};
        
        \node [block, below of=stateSumLU, node distance=11em, label={below:Controller}] (fbckGain) {$\bm{K}$};     
        
        \begin{scope}[on background layer]
            \node [fit=(inputMatrix) (stateMatrix) (outputMatrix), fill= myBlue!10, rounded corners, inner sep=.4cm, label={[xshift=3.5em, myBlue!90]above left:Open-Loop System}] {};
        \end{scope}     
        
        \begin{scope}[on background layer]
            \node [fit=(integralAction) (intActGain), fill= myGreen!10, rounded corners, inner sep=.4cm, label={[xshift=4.3em, myGreen!90]above left:Integral Action}] {};
        \end{scope} 
        
        \begin{scope}[on background layer]
            \node [fit=(inputMatrixLU) (luenbergerGain) (stateMatrixLU) (fbckOutput), fill= myRed!10, rounded corners, inner sep=.4cm, label={[xshift=5.8em, myRed!90]above left:Luenberger Observer}] {};
        \end{scope} 
        
        % Once the nodes are placed, connecting them is easy.       
        \draw [draw,->] (input) -- node[pos=0.1] {$\bm{r}$} node[pos=0.8] {$+$}  (intFbckSum);
        \draw [->] (intFbckSum) -- (integralAction);
        \draw [->] (integralAction) -- node[pos=0.5]{$\bm{x}_a$} (intActGain);
        \draw [->] (intActGain) -- node[pos=0.8]{$-$} (fbckSum);
        \draw [->] (fbckSum) -- node[pos=0.3]{$\bm{u}$} node[name=bku1, pos=0.5]{} (inputMatrix);
        \draw [->] (inputMatrix) -- node[pos=0.8]{$+$} (stateSum);
        \draw [->] (stateSum) -- node[pos=0.5]{$\dot{\bm{x}}$} (integral);
        \draw [->] (integral) -- node[name=bk1,pos=0.5]{} node[name=bk2,pos=0.5]{} node[pos=0.5]{$\bm{x}$} (outputMatrix);
        \draw [->] (bk1) |- (stateMatrix);
        \draw [->] (stateMatrix) -| node[pos=0.95]{$+$} (stateSum);
        \draw [->] (fbckGain) -| node[pos=0.98]{$-$} (fbckSum);
        \draw [->] (bk5) |- ++(0, -33em) -| node[pos=0.98]{$-$} (intFbckSum);
        
        
        \draw [->] (bku1) |- (inputMatrixLU);
        \draw [->] (inputMatrixLU) -- node[pos=0.8]{$+$} (stateSumLU);
        \draw [->] (stateSumLU) -- node[pos=0.5]{$\dot{\hat{\bm{x}}}$} (integralLU);
        \draw [->] (integralLU) -- node[name=bk4,pos=0.5]{} node[pos=0.5]{$\hat{\bm{x}}$} (outputMatrixLU);
        \draw [->] (outputMatrixLU) -| node[pos=0.95]{$-$} (fbckOutput);
        \draw [->] (bk4) |- (stateMatrixLU);
        \draw [->] (fbckOutput) -- (luenbergerGain);
        \draw [->] (stateMatrixLU) -| node[pos=0.95]{$+$} (stateSumLU);
        \draw [->] (luenbergerGain) -| node[pos=0.95]{$+$} (stateSumLU);
        \draw [->] (bk3) -- node[pos=0.95]{$+$} (fbckOutput);
        
        \draw [->] (bk4) |- (fbckGain);
    \end{tikzpicture} 
    }
    \caption{Block diagram of a state-feedback closed-loop system with integral action and Luenberger observer.}
    \label{fig:observer02}
\end{figure}

Notice, now, that the formulation just defined imposes that the dynamics of the estimated state $\hat{\bm{x}}(t)$ is dependent both in $\bm{K}$ and $\bm{L}$, which are arbitrary matrices chosen by the control designer. This leads to the possible conclusion that the choice of $\bm{K}$ is now restricted by the effect that it will produce in the choice of $\bm{L}$, which is not true. The following theorem, known as the Separation Principle \textbf{[reference]}, states that design of these two gains are independent.

\begin{boxed-theorem}{(Separation Principle)} \label{th:separationPrinciple}
    Given a system in State-Space with a Luenberger observer of gain $\bm{L}$ and state-feedback controller of gain $\bm{K}$, the closed-loop eigenvalues contributions of $(\bm{A} - \bm{B}\bm{K})$ are independent from those of $(\bm{A} - \bm{L}\bm{C})$.
\end{boxed-theorem}

\begin{proof}
    Consider a feedback from estimated states as defined in \eqref{eq:fdbckLuenberger01}. That controller-estimator system can be rewritten as a single state equation:
    \begin{equation}
    \begin{bmatrix} \dot{\bm{x}} \\ \dot{\hat{\bm{x}}}  \end{bmatrix}
    =
    \underbrace{\begin{bmatrix}
        \bm{A} & - \bm{B} \bm{K} \\
        \bm{L} \bm{C} & \bm{A} - \bm{L} \bm{C} - \bm{B} \bm{K}
    \end{bmatrix}}_{\tilde{\bm{A}}} \begin{bmatrix} \bm{x} \\ \hat{\bm{x}} \end{bmatrix}
    +
    \underbrace{\begin{bmatrix} \bm{B} \\ \bm{B} \end{bmatrix}}_{\tilde{\bm{B}}} \bm{r}
.\end{equation} 

Consider, now, the following similarity transformation $\bm{z}(t) = \bm{P} \bm{x}(t)$:
\begin{equation}
    \underbrace{\begin{bmatrix}
        \bm{I} & 0 \\ \bm{I} & -\bm{I}
    \end{bmatrix}}_{\bm{P}} \begin{bmatrix}
        \bm{x} \\ \hat{\bm{x}}
    \end{bmatrix} = \begin{bmatrix}
        \bm{x} \\ \bm{x} - \hat{\bm{x}}
    \end{bmatrix} = \begin{bmatrix}
        \bm{x} \\ \bm{e}
    \end{bmatrix} 
.\end{equation}

Since $\bm{P} = \bm{P}^{-1}$, and this is a valid similarity transformation that does not alters the system eigenvalues, the equivalent system for state $\bm{z}(t)$ is obtained as:
\begin{equation}
    \begin{bmatrix} \dot{\bm{x}} \\ \dot{\bm{e}}    \end{bmatrix}
    =
    \begin{bmatrix}
        \bm{A} - \bm{B} \bm{K} & - \bm{B} \bm{K} \\
        \bm{0} & \bm{A} - \bm{L} \bm{C}
    \end{bmatrix} \begin{bmatrix} \bm{x} \\ \bm{e} \end{bmatrix}
    +
    \begin{bmatrix} \bm{B} \\ \bm{0} \end{bmatrix} \bm{r}
.\end{equation} \vskip0.2cm

Since the matrix obtained is block triangular, it is possible to conclude that the system in such configuration has eigenvalues that are contributions from the eigenvalues of $\left( \bm{A} - \bm{B} \bm{K} \right)$ and $\left( \bm{A} - \bm{L} \bm{C} \right)$ separately.
\end{proof}

The Separation Principle is a nice results that further motivates the topology of Fig. \ref{fig:observer02}, since it explicitly states that the design of the controller and the state observer can be done separately. Thus, any structure that obeys the state feedback formulation can be used as a controller and the same is valid for the observer device. In the next chapter, this result will be explored to motivate the use of more advanced control and state estimation techniques without having to redefine the analytical tools and intuitions built for traditional state-feedback controllers from pole-placement methods.

\section{Properties of State-Feedback Controllers}

The last section introduces the first considerations into applying state-feedback in real-world systems, given limitations on the instruments and uncertainty on the environment. Basically, a mathematical analysis of such closed-loop controllers allows for a full characterization of the system behavior, but the real system will exhibit a different response due to these limitations and, essentially, due to possible external disturbances. Therefore, it is desirable to anticipate these variations and discuss the properties of the closed-loop system in the sense of robustness and stability margins. Since this goal requires that the system response is evaluated in as general as possible context, the discussion in this section relies on frequency response analysis, which consider a broader class of input signals: sinusoids of any frequency. Consider the representation of closed-loop systems shown in Fig. \ref{fig:properties01}, which consists of a simplified version of Fig. \ref{fig:feedback01} in the Laplace frequency domain considering only the state response.

\begin{figure}[ht]
    \centering
    \resizebox{!}{!}{
    \begin{tikzpicture}[auto, node distance=2cm,>=latex']
        % We start by placing the blocks
        \node [input](input) {};
        \node [sum, right of=input](fbckSum) {};
        \node [block, right of=fbckSum, node distance=8em, minimum width=5em, fill=myBlue!20, label={below:Open-Loop System}] (openLoopSystem) {$\bm{G}(s)$};
        \node [block, node distance=8em, right of=openLoopSystem, label={below:Controller}] (fbckGain) {$K^T$};
        \node [sum,  right of=fbckGain, node distance=4em](distXSum) {};
        \node [input, above of=distXSum, node distance=3em](distX) {};      
        \node [text width=1em, above of=distX, node distance=1em] {$\bm{D}_x$};     
        \node [output, node distance=4em, right of=distXSum] (output) {};
        \draw [->] (distXSum) -- node[name=bk1, pos=0.4]{} node[pos=0.8]{}  (output);
        \node [sum, node distance=4em, below of=bk1](distESum) {};
        \node [input, right of=distESum, node distance=3em](distE) {};
        \node [text width=1em, right of=distE, node distance=1em] {$\bm{D}_k$};
        
        % Once the nodes are placed, connecting them is easy.       
        \draw [draw,->] (input) -- (fbckSum);
        \draw [->] (fbckSum) -- node[pos=0.75]{$\bm{U}$}  (openLoopSystem);
        \draw [->] (openLoopSystem) -- node[pos=0.25]{$\bm{X}$} (fbckGain);
        \draw [->] (fbckGain) -- node[pos=0.8]{$+$}  (distXSum);
        \draw [->] (bk1) -- node[pos=0.8]{$+$}  (distESum);
        \draw [->] (distESum) |- ++(0,-3em) -| node[pos=0.95]{$-$} (fbckSum);       
        
        \draw [->] (distX) -- node[pos=0.85]{$+$} (distXSum);       
        \draw [->] (distE) -- node[pos=0.85]{$+$} (distESum);       
        
    \end{tikzpicture} 
    }
    \caption{Simplified block diagram of a perturbed state-feedback closed-loop.}
    \label{fig:properties01}
\end{figure}

Using block diagram algebra and Theorem \ref{th:SSToIO}, it is possible to associate a forward-path transfer function $\bm{G}_{f}(s)$ given as \textbf{[reference]}:
\begin{equation} \label{eq:fdbckForwardTrnsfer}
    \bm{G}_{f}(s) = -\bm{K}^T G(s) =  -\bm{K}^{T} \left(s \bm{I} - \bm{A} \right)^{-1} B 
.\end{equation}

Now, it is possible to characterize the properties of this quantity which relates the state-feedback gain $\bm{K}$ with the system disturbance $\bm{D}_x(s)$ and gain disturbance $\bm{D}_k(s)$. To facilitate the definitions and discussion, the results are shown for single-state single-input systems, so $\bm{G}_f(s)$ is a scalar function, where the extension for $n > 1$ states is intuitive in most cases. The first property to be discussed, then, considers the stability of closed-loop feedback controllers. Consider the following closed-loop stability criterion from a Bode plot visualization.

\begin{boxed-theorem}{(Bode stability criterion)}
    Consider a feedback system whose closed-loop transfer function is defined, assuming perfect measuring sensors, as:
    \begin{equation}
        T(s) = \cfrac{K G(s)}{1 + K G(s)}
    .\end{equation}
    
    The closed-loop system is said to be stable if $| G(j \omega_{pc}) | \leq 0 $, where $\omega_{pc}$ is the \textit{phase crossover frequency} obtained such that $\angle G(j \omega_{pc}) = -180º$.
\end{boxed-theorem}

Additionally, it is possible to define a stability criterion through a Nyquist diagram of the closed-loop system.

\begin{boxed-theorem}{(Nyquist criterion)}
    Consider a system with feedforward transfer function as defined in \eqref{eq:fdbckForwardTrnsfer}. Now, let $P$ and $Z$ be respectively the number of poles of $G_{f}(s)$ and zeros of $1 + G_{f}(s)$ that are in the right-half plane. In this case, the Nyquist contour shall clockwise encircle the point $s = -1$ a number of times $N$ such that $N = Z - P$.
\end{boxed-theorem}

A detailed proof of both criterion can be found in \textbf{[reference]}. The introduction of these stability evaluation techniques may seem redundant, given that the closed-loop BIBO stability can still be characterized from Theorem \ref{th:BIBOStab}. However, their graphical nature allows for an easy understanding of how disturbances can affect the stability of state-feedback systems. For instance, a system subject to a sinusoidal disturbance of constant magnitude, but with the same frequency as the natural frequency of the system, will show a response with higher magnitude for the phase crossover frequency than the one visualized in the Bode plot. This phenomenon is widely known in Physics as ``resonance". Therefore, the influence of disturbances can inflict instability to a stable system.

Of course, not all disturbances observed in real operations are strong enough to bring any reasonable stable controller to an unstable condition. However, depending on the choice of the gain $\bm{K}$, some closed-loop systems can be more prone to these undesired problems than others. This motivates the discussion on stability margins.

\begin{boxed-definition}{(Stability Margins)}
    Given a closed-loop system with transfer function $T(s)$, the \textit{Gain Margin} ($GM$) is defined as a factor of how much a gain can be increased before the system becomes unstable, and is equated as:
    \begin{equation}
        GM = \cfrac{1}{|T(j \omega_{pc})|}
    ,\end{equation}
    
    \noindent where $\omega_{pc}$ is the phase crossover frequency such that $\angle T(j\omega_{pc}) = -180º$ (or the point where a Nyquist diagram crosses the real axis for $-1 < s < 0$). In addition, the \textit{Phase Margin} ($PM$) is defined as how much phase lag can be added to $T(s)$ the system becomes unstable, and is equated as:
    \begin{equation}
        PM = \angle T(j \omega_{gc})
    ,\end{equation}
    
    \noindent where $\omega_{gc}$ is the \textit{gain crossover frequency} such that $| T(j\omega_{gc}) | = 0 dB$ (or the angle when a Nyquist diagram crosses the unit circle centered at $s = 0$). 
\end{boxed-definition}

A graphical representation of these margins, for both Bode plots and Nyquist diagrams is shown in Fig. \ref{fig:properties03}, for the same system as the one from last figure. If a closed-loop system has small Gain Margin, it is clear that its stability is not robust to gain uncertainties, while a small Phase Margins implies that its stability is not robust to time delay uncertainties in the control actions. Despite the fact that high gain controllers are usually beneficial for performance requirements, it is clear that they also can lead to disastrous operations in uncertain environments \textbf{[reference]}. Therefore, the design of state-feedback gains must account for these quantities, and a trade-off between performance and robustness is always necessary for this formulation. 

\begin{figure}
    \centering
    \begin{minipage}[t]{.48\textwidth}
	\begin{tikzpicture}
		\node (image) {\includegraphics[width=1\textwidth]{chapter3/report_ch3_3_1}};
		\node (Wgc) at (-.8,0.5) {\small $\omega_{gc}$};
		\node (Wpc) at (.6,-0.5) {\small $\omega_{pc}$};		
		
		\node (GM) at (1.5,2.2) {\small $GM$};
	    \draw [-latex, ultra thick, black] (GM) -- (0.3,2.2);
		
		\node (PM) at (-1.6,-1.9) {\small $PM$};	    
	    \draw [-latex, ultra thick, black] (PM) -- (-0.5,-1.9);
	\end{tikzpicture}
	\end{minipage}%
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
	\begin{tikzpicture}
		\node (img) {\includegraphics[width=1\textwidth]{chapter3/report_ch3_3_2}};
		\node (Wpc) at (3.1,0.5) {\small $\omega_{pc}$};
		\node (Wgc) at (-1.9,-2.1) {\small $\omega_{gc}$};		
		
		\node (GM) at (1.2,1.4) {\small $1 - \cfrac{1}{GM}$};
		\draw [-latex, <->] (-2.05,0.8) -- (2.65,0.8);
		\draw [-latex, -, dashed] (2.72,0.3) -- (2.72,1.1);
		\draw [-latex, -, dashed] (-2.12,0.3) -- (-2.12,1.1);
	    		
		\node (PM) at (-0.4, -0.5) {\small $PM$};	  
		\draw [-latex, <->] (0.5,-1.15) to[bend left] (0,0.2);  
	\end{tikzpicture}
	\end{minipage}%	
    
    \caption{Stability margins visualizations given Bode plots (left) and Nyquist diagrams (right) of closed-loop dynamical systems.}
    \label{fig:properties03}
\end{figure}

% The first property to be discussed is that of sensitivity \textbf{[reference]}. 

% \begin{boxed-definition}{(Sensitivity)} \label{def:sensitivity}
%   Consider a SISO system with state-feedback controller of gain $\bm{K}$ whose forward transfer function $\bm{G}_{f}(s)$ is given by \eqref{eq:fdbckForwardTrnsfer}. The \textit{sensitivity} function of this controlled system, denoted by the matrix $\bm{S}$, is defined as the transfer function from the system disturbances $\bm{D}_x(s)$ to the system total response, and it is equated as:
%   \begin{equation}
%       \bm{S}(s) = \left( \bm{I} + \bm{G}_f(s) \right)^{-1} = \cfrac{1}{1+G_f(s)}
%   \end{equation}
    
%   Additionally, the \textit{complementary sensitivity} function, denoted by the matrix $\bm{T}$, is defined as the transfer function from the reference to the state response, and it is equated as:
%   \begin{equation}
%       \bm{T}(s) = \left( \bm{I} + \bm{G}_f(s) \right)^{-1} \bm{G}_f(s) = \cfrac{G_f(s)}{1+G_f(s)}
%   \end{equation}
% \end{boxed-definition}

% This quantity states an important trade-off that has to be made when the feedback gain $\bm{K}$ is applied to a system subject to disturbances. To understand this, notice that:
% \begin{equation}
%   \bm{S} + \bm{T} = I
% \end{equation}

% Since both these quantities are transfer functions, a value of either one that is equal to the identity implies a perfect operation. Moreover, those values are directly affect by the increase or decrease of $\bm{K}$. This leads to the following observations:

% \begin{enumerate}
%   \item a
%   \item b
%   \item c
%   \item d
% \end{enumerate}

% 4 - Optimal Control and Estimation
% ---------------------------------------------------------------
\clearpage
\chapter{Optimal Control and Estimation}

This chapter introduces the vast field of optimal control and optimal estimation of dynamical systems. The developments are focused in optimize a cost function that produces an optimal state trajectory, and the control actions that causes it, given some conditions. The chapter starts by discussing a general formulation of the optimal control problem and then specializing this formulation to a case of a linear system with quadratic cost functions. After that, the dual optimal state estimation problem is discussed and a result that merges the optimal control with the optimal estimator is presented. Finally, the main stability and robustness properties of the controllers derived in this chapter are accessed.

\section{General Formulation}

The last chapter introduced the notion of controller synthesis as an engineering procedure to be done ``by hand" from a designer with some knowledge about the system and the control environment. This approach, despite being very popular and practical, introduces some design issues that make it difficult to generalize a controller to different systems within a same class or, more clearly, to MIMO configurations. Thus, a more general and automatic procedure to control synthesis is desirable.

With this motivation, the concept of \textit{Optimal Control} \textbf{[references]} was introduced as an alternative strategy for controlling dynamical systems that determines the necessary action by optimizing a cost function (or maximizing a reward function). Usually, these formulations are data-driven methods that autonomously produces optimal input signals given a desired objective and restrictions, and can be easily generalized to different systems.

\begin{boxed-definition}{(Optimal Control)} \label{def:optimalControl}
Given a system in State-Space formulation, with state-vector signal $\bm{x} : \mathbb{R} \rightarrow \mathbb{R}^{n}$, and a reference signal $\bm{r} : \mathbb{R} \rightarrow \mathbb{R}^{n}$, the input signal $u(t) \in \mathbb{R}^r$, for any time $t$, is calculated by an optimal control through an optimal control law $\pi^* : \mathbb{R}^{n \times n \times 1} \rightarrow \mathbb{R}^r$ as:
    \begin{equation}
        \bm{u}(t) = \pi^*(\bm{x}, \bm{r}, t) = \min_{\bm{u}} J(\bm{x}, \bm{r}, t)
    ,\end{equation}
    
\noindent where $J : \mathbb{R}^{n \times n \times 1} \rightarrow \mathbb{R}$ is known as a \textit{cost function} of the states and reference signals.
\end{boxed-definition}

First of all, note that this optimization can be converted to maximizing a function $\bm{V}$ by making $\bm{V} = -\bm{J}$, so this document will only refers to optimization as minimizing some cost function. Basically, the problem of finding an optimal control law, or optimal control policy, $pi^*(\cdot)$ cares both for the choice of the cost function $J$ and for what optimization technique will be used to determine the value of $u(t)$ that achieve this minimum. This problem differs from standard optimization problems in data science because not only the data is usually obtained online from the system, but it is dependent through time and constrained by the dynamics of the model and optimal policy action. Thus, the solutions for this optimization are usually obtained through Calculus of Variations \textbf{[reference]} or, as is the approach used in this work, through Dynamic Programming \textbf{[reference]}.

To facilitate the discussion and analysis of optimal controllers, consider a subclass of these controllers (that is still very general) defined below by a specific choice of functional for the cost function.

\begin{boxed-definition}{(Finite-Horizon Optimal Regulators)} \label{def:finiteHorizonOC}
    Consider a controller setup as in Definition \ref{def:optimalControl}. A \textit{Finite-Horizon Optimal Regulators} is defined as any controller whose optimal policy over a time interval $t \in [t_0, T]$ minimizes the cost functional:
    \begin{equation}
        J = \int_{t_0}^T l(\bm{x}, \bm{u}, \tau) d \tau + l_f(\bm{x}, T)
    ,\end{equation}
    
    \noindent where $l(\cdot) : \mathbb{R}^{n \times r \times 1} \rightarrow \mathbb{R}$ and $l_f(\cdot) : \mathbb{R}^{n} \rightarrow \mathbb{R}$ are, respectively, the trajectory and terminal \textit{loss functions}. In the case that $t_0 = 0$, $T$ is also known as the \textit{control horizon}.
\end{boxed-definition}

This formulation presents a notion of, basically, optimize for an trajectory $P^* = [\bm{x}(t_0), \cdots, \bm{x}(T)]$ which is optimal given the loss functions, and then find the control input that can achieve this trajectory. As will be shown later, this is a very feasible strategy for controllable linear systems. In a optimization notation, this problem can also be presented as the following \textit{program}: 
\begin{align}
\begin{matrix*}[l]
    \textbf{minimize} & \int_{t_0}^T l(\bm{x}, \bm{u}, \tau) d \tau + l_f(\bm{x}, T) \\
    \hfill \textbf{s.t.} & \dot{\bm{x}}(t) = \bm{f}(\bm{x}, \bm{u}, t)  \\
                & \bm{u}(t) = \pi(\bm{x}(t_0), \cdots, \bm{x}(t))
\end{matrix*}
.\end{align}

Now, the discussion turns to how to solve this general problem. In this work, the solution for the optimal control will follow the same dynamic programming formulation as in \textbf{[reference]}. The first necessary effort, then, is to define an important partial differential equation known as the Hamilton-Jacobi equation.

\begin{boxed-theorem}{(Hamilton-Jacobi equation)} \label{th:hamiltonJacobi}
    Consider a finite-horizon cost function in the form of Definition \ref{def:finiteHorizonOC}, for a system described by the state-equation $\dot{\bm{x}} = \bm{f}(\bm{x}, \bm{u}, t)$. Consider also that the loss $l(\cdot)$ and state function $\bm{f}$ are smooth on their parameters. Then, minimizing any functional in the form of $J$ is equivalent to minimize the solution of the \textit{Hamilton-Jacobi equation}, which is given by the partial differential equation:
    \begin{equation} \label{eq:HJEquation}
        \cfrac{\partial V^*}{\partial t} = - \min_{u(t)} \left[ l(\bm{x}, \bm{u}, t) + \left[ \cfrac{\partial V^*}{\partial x} \right]^T \bm{f}(\bm{x}, \bm{u}, t)  \right]
    \end{equation}
    
    \noindent and the boundary condition:
    \begin{equation}
        V^*(\bm{x}, T) = l_f(\bm{x}(T))
    .\end{equation}
\end{boxed-theorem}

\begin{proof}
    First of all, consider the restatement of the cost functional as the function $V(\cdot)$:
    \begin{equation}
        V(\bm{x}, \bm{u}, t_0) = \int_{t_0}^T l(\bm{x}, \bm{u}, \tau) d \tau + l_f(\bm{x}(T))
    .\end{equation}
    
    Minimizing this cost functional over control inputs $\bm{u}(t_0), \cdots, \bm{u}(T)$ consists in evaluating the optimal cost:
    \begin{equation}
        V^*(\bm{x}, t_0) = \min_{\bm{u}(t_0), \cdots, \bm{u}(T)} \left[ \int_{t_0}^T l(\bm{x}, \bm{u}, \tau) d \tau + l_f(\bm{x}(T)) \right]
    .\end{equation}
    
    Now, consider any $t \in [t_0, T]$ and $t_r \in [t, T]$. Since the original control action $\bm{u}(t), \cdots, \bm{u}(T)$ can be obtained through the concatenation of $\bm{u}(t), \cdots, \bm{u}(t_r)$ and $\bm{u}(t_r), \cdots, \bm{u}(T)$, the optimal cost in this interval can be represented in the recursive form:
    \begin{equation} \label{eq:HJERecursive}
    \begin{split}
    V^*(\bm{x}, t) &= \min_{\bm{u}(t), \cdots, \bm{u}(T)} \left[ \int_{t}^T l(\bm{x}, \bm{u}, \tau) d \tau + l_f(\bm{x}(T)) \right] \\
        &= \min_{\bm{u}(t), \cdots, \bm{u}(t_r)} \left\{ \min_{\bm{u}(t_r), \cdots, \bm{u}(T)} \left[ \int_{t}^{t_r} l(\bm{x}, \bm{u}, \tau) d \tau + \int_{t_r}^{T} l(\bm{x}, \bm{u}, \tau) d \tau + l_f(\bm{x}(T)) \right] \right\} \\
        &= \min_{\bm{u}(t), \cdots, \bm{u}(t_r)} \left\{ \int_{t}^{t_r} l(\bm{x}, \bm{u}, \tau) d \tau + \min_{\bm{u}(t_r), \cdots, \bm{u}(T)} \left[ \int_{t_r}^{T} l(\bm{x}, \bm{u}, \tau) d \tau + l_f(\bm{x}(T)) \right] \right\} \\
        &= \min_{\bm{u}(t), \cdots, \bm{u}(t_r)} \left\{ \int_{t}^{t_r} l(\bm{x}, \bm{u}, \tau) d \tau + V^*(\bm{x}, t_r) \right\}
    \end{split}
    .\end{equation}
    
    Without loss of generalization, let $t_r = t + \delta t$, where $\delta t$ is a small number. Since $l(\cdot)$ is a smooth function, the right-hand side of the recursive form above can be expanded by a Taylor series expansion:
    \begin{equation}
    \begin{split}
        V^*(\bm{x}, t) = \min_{\bm{u}(t), \cdots, \bm{u}(t+\delta t)} & \biggl\{ \delta t l(\bm{x}, \bm{u}, t + \delta t) \biggr. \\ 
        & \left. + V^*(\bm{x}, t) + \left[ \cfrac{\partial V^*(\bm{x}, t)}{\partial \bm{x}} \right]^T \cfrac{d \bm{x}(t)}{dt} \delta t + \cfrac{\partial V^*(\bm{x}, t)}{\partial t} \delta t + O(\delta t)^2  \right\}
    \end{split}
    ,\end{equation}
    
    \noindent where $O(\delta t)^2$ denotes the high order terms. Since the terms $V^*(\bm{x}, t)$ and $(\partial V^*/\partial t) \delta t$ does not depend on $\bm{u}(t)$, they can be taken out of the minimization. Rearranging the terms and substituting $d \bm{x} / dt = \bm{f}(\bm{x}, \bm{u}, t)$ results in:
    \begin{equation}
        \cfrac{\partial V^*}{\partial t}(\bm{x}, t) =  - \min_{\bm{u}(t), \cdots, \bm{u}(t+\delta t)} \left\{ l(\bm{x}, \bm{u}, t + \delta t)  + \left[ \cfrac{\partial V^*(\bm{x}, t)}{\partial \bm{x}} \right]^T \bm{f}(\bm{x}, \bm{u}, t) + O(\delta t)^2  \right\}
    .\end{equation}
    
    Finally, making $\delta t \to 0$ results in:
    \begin{equation}
        \cfrac{\partial V^*}{\partial t} =  - \min_{\bm{u}(t)} \left\{ l(\bm{x}, \bm{u}, t)  + \left[ \cfrac{\partial V^*}{\partial \bm{x}} \right]^T \bm{f}(\bm{x}, \bm{u}, t) \right\}
    .\end{equation}
    
    To establish the theorem it remains to derive the boundary condition. This result, however, is direct from the form of the cost function, since $V^*(\bm{x}, T) = l_f(\bm{x}(T))$ can not be changed through any more control action inside the time horizon.
\end{proof}

The Hamilton-Jacobi equation implies that finite-horizon optimal controllers can be optimized in a recursive manner, starting from the boundary condition in $t = T$ to the beginning of the horizon at $t = t_0$. This is a result from the fact that an optimal action $u(t)$ depends on the loss function $l(\cdot)$ at time $t$ and on the optimal cost $V^*(\cdot)$ for a time immediately after that (for instance, $t+\delta t$), but the last action $u(T)$ depends only on $l_f(\cdot)$ since it no longer affects the system inside the control horizon. The recursive property, directly evidenced in \eqref{eq:HJERecursive}, is a statement of the Bellman's Principle of Optimality \textbf{[reference]} and, for this reason, the Hamilton-Jacobi equation in the context of optimal control theory is known as the Hamilton-Jacobi-Bellman equation. A illustration of this principle is shown in Fig. \ref{fig:generalOC01}

\begin{figure}[ht]
    \centering
    \resizebox{!}{!}{
    \begin{tikzpicture}[auto, node distance=1.75cm,>=latex', scale=0.2]
        % Nodes
        \node [mcCircle] (x0) {$x_0$};
        
        \node [mcCircle, right of=x0, node distance=8em] (x12) {};
        \node [mcCircle, above of=x12] (x11) {};
        \node [mcCircle, below of=x12] (x13) {};
        
        \node [mcCircle, right of=x12, node distance=12em] (xT) {$x_T$};
        
        % Scopes
        \begin{scope}[on background layer]
            \node [fit=(x11) (x13), fill= black!10, rounded corners, inner sep=.4cm, label={[xshift=0em, black!90]above:$x_1$}] {};
        \end{scope}         
        
        % Lines
        \draw [->, color=myRed!80, line width=.75mm] (x0) -- node[pos=0.7]{$70$} (x11);
        \draw [->] (x0) -- node[pos=0.6]{$40$} (x12);
        \draw [->] (x0) -- node[pos=0.45]{$130$} (x13);
        
        \draw [->, color=myRed!80, line width=.75mm, decorate, decoration={snake,amplitude=1.5mm, segment length=8mm}] (x11) -- node[pos=0.3]{$220$} (xT);
        \draw [->, decorate, decoration={snake,amplitude=1.5mm, segment length=8mm}] (x12) -- node[pos=0.4]{$270$} (xT);
        \draw [->, decorate, decoration={snake,amplitude=1.5mm, segment length=8mm}] (x13) -- node[pos=0.53]{$190$} (xT);
    \end{tikzpicture} 
    }
    \caption{Illustration of the Bellman's Principle of Optimality. Each column represents a discrete time instance and each node represents a possible discrete state. The straight lines represents state transitions given an action with associated costs, whereas the curves represents the trajectory from that state to the terminal state with associated optimal cost. The optimal trajectory between the initial and terminal state is shown in red.}
    \label{fig:generalOC01}
\end{figure}

\section{Linear Quadratic Regulator (LQR)}

The last section introduced a general condition for solving a finite-horizon optimal control problem. Developing an analytical solution of that condition for any arbitrary loss function $l(\cdot)$ and state-equation $\bm{f}(\cdot)$ is usually intractable. However, there is a choice of loss function that, under a linear system, allows for a nice closed-form solution to the Hamilton-Jacobi-Bellman equation. This defines the popular class of optimal controllers known as the Linear Quadratic Regulators.

\begin{boxed-definition}{(Linear Quadratic Regulator)} \label{def:lqr}
    Given a linear State-Space system in the form:
    \begin{align}
    \begin{cases}
        \dot{\bm{x}}(t) = \bm{A} \bm{x}(t) + \bm{B} \bm{u}(t) \\
        \bm{y}(t) = \bm{C} \bm{x}(t) + \bm{D} \bm{u}(t)
    \end{cases}
    .\end{align}
    
    A \textit{Linear Quadratic Regulator} (LQR) for this system is an optimal controller defined by the quadratic cost function:
    \begin{equation}
        J(\bm{x}, \bm{u}, t_0) = \int_{t_0}^{T} \left( \bm{x}^T \bm{Q} \bm{x} + \bm{u}^T \bm{R} \bm{u} \right) dt + \bm{x}^T(T) \bm{Q}_f \bm{x}(T)
    ,\end{equation}
    
    \noindent where is assumed that $\bm{Q},\bm{Q}_f  \succ 0$ and $\bm{R} \succ 0$ are matrices penalizing, respectively, the state-vector magnitude and the control effort.
\end{boxed-definition}

The LQR optimal controller was first introduced in \textbf{[reference]} and has been a foundation of optimal control theory ever since. In a engineering point of view, this choice of cost function has the advantage that it breaks the controller design to simply choose matrices $\bm{Q}$ and $\bm{R}$ as a trade-off between performance and actuator restrictions. In a mathematical point of view, the cost function has the advantage of being a quadratic function of the state and input signals, which is nice since quadratic optimization problems are widely studied in the literature \textbf{[reference]}. Now, it is necessary to develop a solution for the Hamilton-Jacobi equation in the light of this formulation. First of all, consider the following theorem.

\begin{boxed-theorem}{} \label{th:costQuadraForm}
    Consider a continuous cost function $V : \mathbb{R}^{n \times r \times 1} \rightarrow \mathbb{R}$ given as the LQR cost function defined in Definition \ref{def:lqr} for a linear system. Then the optimal cost $V^*(\bm{x}, t)$ has the quadratic form:
    \begin{equation}
        V^*(\bm{x}, t) = \bm{x}^T(t) \bm{P}(t) \bm{x}(t)
    \end{equation}
    
\noindent for any (possibly symmetric) matrix $\bm{P}(t)$ of appropriate dimensions. More precisely, the optimal cost $V^*(\bm{x}, t)$ satisfies the necessary and sufficient conditions for a quadratic function given as, for any $\lambda \in \mathbb{R}$:
    \begin{align} 
        V^*(\lambda \bm{x}, t) &= \lambda^2 V^*(\bm{x}, t) \label{eq:costQuadraForm021} \\ 
        V^*(\bm{x}_1 , t) + V^*(\bm{x}_2 , t) &= \cfrac{1}{2} \left( V^*(\bm{x}_1 + \bm{x}_2 , t) + V^*(\bm{x}_1 - \bm{x}_2 , t) \right) \label{eq:costQuadraForm022}
    .\end{align}
\end{boxed-theorem}

\begin{proof}
    Since $V^*(\bm{x}, t)$ is the minimum value of $V(\bm{x}, \bm{u}^*, t)$ given an optimal input $\bm{u}^*(t)$ for $t \in [t, T]$, then any deviance $\lambda \in \mathbb{R}$ in this parameter will produce a greater value of the cost. Thus, a direct result from this and the fact that $V(\bm{x}, \bm{u}, t)$ is a quadratic function of $\bm{x}(t)$ and $\bm{u}(t)$ is:
    \begin{equation}
        V^*(\bm{x}, t) \leq V(\lambda \bm{x}, \lambda \bm{u}^*, t) = \lambda^2 V^*(\bm{x}, t) \leq \lambda^2 V(\bm{x}, \lambda^{-1} \bm{u}^*, t) = V^*(\bm{x}, t)
    ,\end{equation}
    
    \noindent or, simply:
    \begin{equation}
        V^*(\bm{x}, t) \leq \lambda^2 V^*(\bm{x}, t) \leq V^*(\bm{x}, t)
    ,\end{equation}
    
    \noindent which implies $V^*(\bm{x}, t) = \lambda^2 V^*(\bm{x}, t)$ and establishes \eqref{eq:costQuadraForm021}. Similarly:
    \begin{align}
    \begin{split}
        V^*(\bm{x}_1, t) + V^*(\bm{x}_2, t) &= \cfrac{1}{4} \left( V^*(2 \bm{x}_1, t) + V^*(2 \bm{x}_2, t) \right) \\
            & \leq \cfrac{1}{4} \left( V(2 \bm{x}_1, \bm{u}_{x_1+x_2}^* + \bm{u}_{x_1-x_2}^*, t) + V(2 \bm{x}_2, \bm{u}_{x_1+x_2}^* - \bm{u}_{x_1-x_2}^*, t) \right) \\
            & = \cfrac{1}{2} \left( V(\bm{x}_1 + \bm{x}_2, \bm{u}_{x_1+x_2}^*, t) + V(\bm{x}_1 - \bm{x}_2, \bm{u}_{x_1-x_2}^*, t) \right) \\
            & = \cfrac{1}{2} \left( V^*(\bm{x}_1 + \bm{x}_2, t) + V^*(\bm{x}_1 - \bm{x}_2, t) \right) \\
            & \leq V^*(\bm{x}_1, t) + V^*(\bm{x}_2, t)
    \end{split}
    ,\end{align}
    
    \noindent which implies $V^*(\bm{x}_1, t) + V^*(\bm{x}_2, t) = (1/2) \left( V^*(\bm{x}_1 + \bm{x}_2, t) + V^*(\bm{x}_1 - \bm{x}_2, t) \right)$ and establishes \eqref{eq:costQuadraForm022}. Therefore, the optimal cost function has a quadratic form $V^*(\bm{x}, t) = \bm{x}^T(t) \bm{P}(t) \bm{x}(t)$.
\end{proof}

Notice that only the fact that the LQR cost function is quadratic directly implies that the optimal cost is also quadratic. Therefore, it is possible to define the optimal action $\bm{u}^*(t)$ that produced this optimal cost by evaluating a relationship between it and the matrix $\bm{P}$. Since the quadratic cost just evaluated is defined for a finite-horizon optimal controller, it has to obey the condition imposed by the Hamilton-Jacobi equation, and the optimal controller can be solved as shown in the following theorem.

\begin{boxed-theorem}{(LQR Control Action)} \label{th:LQRAction}
    Given a Linear Quadratic Regulator as defined in Definition \ref{def:lqr}, the optimal action produced by this optimal controller at any time $t \in [t_0, T]$ is given by:
    \begin{equation}
        \bm{u}^*(t) = - \bm{R}^{-1} \bm{B}^T \bm{P}(t) \bm{x}(t)
    ,\end{equation}
    
    \noindent where $\bm{P}(t)$ is the solution of the matrix Riccati differential equation:
    \begin{equation}
        -\dot{\bm{P}}(t) = \bm{A}^T \bm{P}(t) + \bm{P}(t) \bm{A} - \bm{P}(t) \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P}(t) + \bm{Q}
    ,\end{equation}
    
    \noindent with terminal condition $\bm{P}(T) = \bm{Q}_f$.
\end{boxed-theorem}

\begin{proof}
    Consider the Hamilton-Jacobi equation from \eqref{eq:HJEquation} restated below for a quadratic loss function $l(\bm{x}, \bm{u}, t$ and a linear system with state equation $\bm{f}(t)$ as given by Definition \ref{def:lqr}:
    \begin{equation}
    \begin{split}
        \cfrac{\partial (\bm{x}^T \bm{P} \bm{x})}{\partial t} &=  - \min_{\bm{u}(t)} \left\{ \bm{x}^T \bm{Q} \bm{x} + \bm{u}^T \bm{R} \bm{u} + \left[ \cfrac{\partial (\bm{x}^T \bm{P} \bm{x})}{\partial \bm{x}} \right]^T \left( \bm{A} \bm{x} + \bm{B} \bm{u} \right) \right\} \\
        \bm{x}^T \dot{\bm{P}} \bm{x} &= - \min_{\bm{u}(t)} \left\{ \bm{x}^T \bm{Q} \bm{x} + \bm{u}^T \bm{R} \bm{u} + (2 \bm{x}^T \bm{P}) \left( \bm{A} \bm{x} + \bm{B} \bm{u} \right) \right\}
    \end{split}
    ,\end{equation}
    
    \noindent where, without loss of generalization, $\bm{P}(t)$ was assumed to be symmetric. Because of the quadratic nature, it is possible to calculate the minimum of the right-hand side of the equation by taking the derivative with respect to the control action and evaluate it for zero:
    \begin{equation}
    \begin{split}
        0 &= \cfrac{\partial}{\partial \bm{u}(t)} \left( \bm{x}^T \bm{Q} \bm{x} + \bm{u}^T \bm{R} \bm{u} + (2 \bm{x}^T \bm{P}) \left( \bm{A} \bm{x} + \bm{B} \bm{u} \right) \right) \\
        0 &= 2 \bm{u}^T(t) \bm{R} + (2 \bm{x}^T(t) \bm{P}(t)) \bm{B} \\
        \bm{u}(t) &= - \bm{R}^{-1}  \bm{B}^T \bm{P}(t) \bm{x}(t)
    \end{split}
    .\end{equation}
    
    To solve for $\bm{P}$ first note that the following identity can be found by completing the squares:
    \begin{equation}
    \begin{split}
        \bm{x}^T \bm{Q} \bm{x} + \bm{u}^T \bm{R} \bm{u} + (2 \bm{x}^T \bm{P}) \left( \bm{A} \bm{x} + \bm{B} \bm{u} \right) = & (u + \bm{R}^{-1} \bm{B}^T \bm{P} x)^T \bm{R} (u + \bm{R}^{-1} \bm{B}^T \bm{P} x) \\ & + \bm{x}^T (\bm{A}^T \bm{P} + \bm{P} \bm{A} - \bm{P} \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P} + \bm{Q}) \bm{x}
    \end{split}
    .\end{equation}
    
    Thus, plugging this identity and substituting the optimal control action results in the matrix Riccati equation:
    \begin{equation}
    \begin{split}
    \bm{x}^T \dot{\bm{P}} \bm{x} &= \bm{x}^T (\bm{A}^T \bm{P} + \bm{P} \bm{A} - \bm{P} \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P} + \bm{Q}) \bm{x} \\
    \dot{\bm{P}} &= \bm{A}^T \bm{P} + \bm{P} \bm{A} - \bm{P} \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P} + \bm{Q}
    \end{split}
    .\end{equation}
    
    Finally, the terminal condition is direct from the fact that $V^*(\bm{x}, T) = \bm{x}^T(T) \bm{Q}_f \bm{x}(T)$.
\end{proof}

A closed-form solution for the LQR problem makes this controller a very appealing solution in several applications, since not only it is an optimal controller but the controller action can be calculated in real-time. Several others optimal controllers may pose performance improvements, but usually depends on iterative optimization procedures, that can be hard to compute in high-dimensional systems (or even in critical low-dimensional systems). Furthermore, the time complexity of the LQR control action computation is governed by the solution of the Riccati differential equation which can be done very efficiently \textbf{[reference]}.

Now, consider the optimal control action $\bm{u}^*(t)$. Applying this action to a linear system in State-Space representation results in the following state-equation:
\begin{equation}
\begin{split}
    \dot{\bm{x}}(t) &= \bm{A} \bm{x}(t) + \bm{B} \left( -\bm{R}^{-1} \bm{B}^T \bm{P}(t) \bm{x}(t) \right) \\
        & = \left( \bm{A} - \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P}(t) \right) \bm{x}(t) \\
        & = \left( \bm{A} - \bm{B} \bm{K}(t) \right) \bm{x}(t)
\end{split}
.\end{equation}

This formulation indicates that the Linear Quadratic Regulator solution follows the exact same form of a standard state-feedback regulator, represented by the linear but time-variant state-feedback gain $\bm{K}(t) = \bm{R}^{-1} \bm{B}^T \bm{P}(t)$. Therefore, the past results of state-feedback regulators are directly applied to the operation of this class of optimal controllers. Notice, also, that both the computation of $\bm{K}(t)$ and $\bm{P}(t)$ does not explicitly depends on $\bm{x}(t)$, meaning that, for a specific control horizon, they can be calculated off-line and then provided to the controller actuator for the on-line operation. For this reason, despite being a closed-loop controller with corrective action, the LQR can be considered a open-loop optimizer, since the optimization solution itself does not depend on the state signal.

Now, as discussed in the previous chapter, the class of regulation controllers are broad but still restricted to a zero-state reference signal. To expand the range of application for the optimal controller just derived, it is possible to introduce integral action to the feedback, just as in the previous case.

\begin{boxed-definition}{(Linear Quadratic Servo)} \label{def:lqServo}
    Given a linear State-Space system represented by matrices $(\bm{A}, \bm{B}, \bm{C}, \bm{D})$, augmented with state $\dot{\bm{x}}_a(t) = \bm{r}(t) - \bm{C} \bm{x}(t)$:
    \begin{align} 
    \begin{cases}
        \begin{bmatrix}
            \dot{\bm{x}}(t) \\
            \dot{\bm{x}}_a(t)
        \end{bmatrix} &= \underbrace{\begin{bmatrix}
            \bm{A}  & \bm{0} \\ - \bm{C} & \bm{0}
        \end{bmatrix}}_{\tilde{\bm{A}}} \underbrace{\begin{bmatrix}
            \bm{x}(t) \\
            \bm{x}_a(t)
        \end{bmatrix}}_{\tilde{\bm{x}}(t)} + \underbrace{\begin{bmatrix}
            \bm{B} \\
            \bm{0}
        \end{bmatrix}}_{\tilde{\bm{B}}} \bm{u}(t) + \begin{bmatrix}
            \bm{0} \\
            \bm{I}
        \end{bmatrix} \bm{r}(t)
        \\
        \hfill \bm{y}(t) &= \begin{bmatrix}
            \bm{C} & \bm{0}
        \end{bmatrix} \begin{bmatrix}
            \bm{x}(t) \\
            \bm{x}_a(t)
        \end{bmatrix}
    \end{cases}
    .\end{align}
    
    A \textit{Linear Quadratic Servo} (LQ-Servo) for this system is an optimal controller defined by the quadratic cost function:
    \begin{equation}
        J(\bm{x}, \bm{u}, t_0) = \int_{t_0}^{T} \left( \tilde{\bm{x}}^T \tilde{\bm{Q}} \tilde{\bm{x}} + \bm{u}^T \bm{R} \bm{u} \right) dt +\tilde{\bm{x}} \tilde{\bm{Q}}_f \tilde{\bm{x}}(T)
    ,\end{equation}
    
    \noindent where is assumed that $\tilde{\bm{Q}}, \tilde{\bm{Q}}_f \succ 0$ and $\bm{R} \succ 0$ are matrices penalizing, respectively, the state-vector magnitude and the control effort.
\end{boxed-definition}

As before, the integral action basically turns the tracking problem into a regulation problem. In this case, the optimal controller will try to optimize a zero-state for $\bm{x}_a(t)$, producing the reference tracking. The new $p$ diagonal entries of the augmented matrix $\tilde{\bm{Q}}$ can be interpreted as weights penalizing the state deviance from the reference signal. The solution for this controller is the same as the one derived in Theorem \ref{th:LQRAction}, but making $\bm{A} = \tilde{\bm{A}}$ and $\bm{B} = \tilde{\bm{B}}$, and the resulting control action can be equated as:
\begin{equation}
    \bm{u}(t) = \tilde{\bm{K}}(t)\tilde{\bm{x}}(t) = \begin{bmatrix} \bm{K}(t) & \bm{K}_a(t) \end{bmatrix} \begin{bmatrix} \bm{x}(t) \\ \bm{x}_a(t) \end{bmatrix}
.\end{equation}

To finish the section, it is also worth mentioning the fact that, for the LQR cost function, it is possible to determine an optimal controller for an infinite-horizon operation, that is, when $T \to \infty$. The fact that the control horizon is now infinite results in a stationary state-feedback gain $\bm{K}$, as stated below.

\begin{boxed-theorem}{(Infinite-Horizon LQR)} \label{th:infiniteLQR}
    Consider a Linear Quadratic Regulator as defined in Definition \ref{def:lqr}, but with $T = \infty$. The optimal control action produced by this optimal controller at any time $t \in [t_0, \infty]$ is given by:
    \begin{equation}
        \bm{u}^*(t) = - \bm{K} \bm{x}(t) = - \bm{R}^{-1} \bm{B}^T \bm{P} \bm{x}(t)
    ,\end{equation}
    
    \noindent where $\bm{K} = \bm{R}^{-1} \bm{B}^T \bm{P}$ and $\bm{P}(t)$ is the solution of the matrix algebraic Riccati equation:
    \begin{equation}
        0 = \bm{A}^T \bm{P} + \bm{P} \bm{A} - \bm{P} \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P} + \bm{Q}
    .\end{equation}
\end{boxed-theorem}

A detailed proof of this result can be found in \textbf{[reference]}. The possibility for an infinite-horizon LQR is desirable in the sense that it doesn't restrict the operation to a fixed time interval, however, the resulting optimal controller suffers from a worst performance when compared to finite-time horizon controllers.

\section{Optimal State Estimators}

As already discussed, the state-vector $\bm{x}(t)$ for a time $t$ is a necessary information to calculate $\bm{u}(t)$ in state-feedback controllers, but is also inaccessible in practice. In the last chapter, a deterministic observer, namely the Luenberger observer, was derived to deal with this problem by the design of a observer gain $\bm{L}$. Despite being possible to utilize the Luenberger observer together with optimal controllers, this method suffers from the same design issues that the Pole-Placement method does, since they are dual problems. Therefore, this section develops an optimal state estimation approach which, from the estimation nature of the problem, relies on a statistical interpretation of the dynamical system and its observations.

First of all, consider a system perturbed by disturbances in the state-response and in the measurements, respectively $\bm{w} : \bm{R} \rightarrow \bm{R}^{n}$ and $\bm{v} : \bm{R} \rightarrow \bm{R}^{p}$. This configuration is illustrated at Fig. \ref{fig:optSE01}. If these disturbances are assumed to be white Gaussian noises, i.e., $\bm{w}_k \sim \mathcal{N}(0, \bm{Q}_{kf})$ and $\bm{v}_k \sim \mathcal{N}(0, \bm{R}_{kf})$ where $\bm{Q}_{kf}$ and $\bm{R}_{kf}$ are covariances matrices of appropriate sizes, then it is possible to motivate a stochastic formulation of the linear State-Space model.

\begin{boxed-definition}{(Stochastic Discrete State-Space)} \label{def:stochSS}
    Given an additive process noise $\bm{w}_k \sim \mathcal{N}(\bm{0}, \bm{Q}_{kf})$ with covariance $\bm{Q}_{kf} \in \mathbb{R}^{n \times n}$ and an additive measurement noise $\bm{v}_k \sim \mathcal{N}(\bm{0}, \bm{R}_{kf})$ with covariance $\bm{R}_{kf} \in \mathbb{R}^{p \times p}$, the stochastic version of a discrete-time State-Space model is given by the equations:
    \begin{align}
    \begin{cases}
        \hfill \bm{x}_{k+1} &= \bm{A}_d \bm{x}_k + \bm{B}_d \bm{u}_k + \bm{w}_k \\
        \hfill \bm{y}_k &= \bm{C}_d \bm{x}_k + \bm{v}_k 
    \end{cases}
    .\end{align}
\end{boxed-definition}


\begin{figure}[ht]
    \centering
    \resizebox{!}{!}{
    \begin{tikzpicture}[auto, node distance=2cm,>=latex', scale=0.2]
        % We start by placing the blocks
        \node [input, name=input] {};
        \node [block, right of=input] (inputMatrix) {$\bm{B}$};
        \node [sum, right of=inputMatrix, node distance=4em] (stateSum) {};
        \node [block, right of=stateSum, node distance=6em] (integral) {$\bm{z}^{-1}$};        
        \node [input, above of=stateSum, node distance=4em] (wInput) {};
        \node [text width=1em, above of=wInput, node distance=1em] (wText) {$w_k$};
        \node [block, right of=integral, node distance=8em] (outputMatrix) {$\bm{C}$};
        \node [sum, right of=outputMatrix, node distance=4em] (vSum) {};
        \node [input, above of=vSum, node distance=4em] (vInput) {};
        \node [text width=1em, above of=vInput, node distance=1em] (vText) {$v_k$};
        \node [output, right of=vSum] (output) {};
        \node [block, below of=integral] (stateMatrix) {$\bm{A}$};
        
        \begin{scope}[on background layer]
            \node [fit=(inputMatrix) (stateMatrix) (vSum), fill= myBlue!10, rounded corners, inner sep=.4cm, label={[xshift=1.2em, myBlue!90]below left:Open-Loop System}] {};
        \end{scope}     
        
        % Once the nodes are placed, connecting them is easy.       
        \draw [draw,->] (input) -- node[pos=0.1] {$\bm{u}_k$} (inputMatrix);
        \draw [->] (inputMatrix) -- node[pos=0.8]{$+$} (stateSum);
        \draw [->] (stateSum) -- node[pos=0.6]{$\bm{x}_{k+1}$} (integral);
        \draw [->] (integral) -- node[name=bk1,pos=0.6]{$\bm{x}_{k}$} (outputMatrix);
        \draw [->] (wInput) -- node[pos=0.8]{$+$} (stateSum);
        \draw [->] (outputMatrix) -- node[pos=0.8]{$+$} (vSum);
        \draw [->] (vInput) -- node[pos=0.8]{$+$} (vSum);
        \draw [->] (vSum) -- node[pos=0.8]{$\bm{y}_k$} (output);
        \draw [->] (bk1) |- (stateMatrix);
        \draw [->] (stateMatrix) -| node[pos=0.95]{$+$} (stateSum);
    \end{tikzpicture} 
    }
    \caption{Block diagram of a stochastic discrete State-Space system.}
    \label{fig:optSE01}
\end{figure}

With this formulation, the noises can actually represent both the effect of external disturbances and the uncertainty about the model accuracy in respect to the physical system. This is important since the dynamical models, even those developed by the first-principle methodology, are not guaranteed to be the absolute true representation of a dynamical system or process. The motivation for a discussion over a discrete State-Space model is to facilitate the interpretation of the statistical properties over finite data sample collections. Since it is always possible to convert a continuous-time State-Space into a discrete-time State-Space \textbf{[reference]}, this discussion can be easily extended to more general cases, and results for continuous-time systems will be addressed. 

A direct result from Definition \ref{def:stochSS} is that, since $\bm{w}_k$ and $\bm{v}_k$ are random variables, $\bm{x}_k$ and $\bm{y}_k$ are also random variables. More precisely, since the noises are Gaussian, both variables $\bm{x}_k \sim \mathcal{N}(\mu_{x_k}, \Sigma_{x_k})$ and $\bm{y}_k \sim \mathcal{N}(\mu_{y_k}, \Sigma_{y_k})$ are Gaussian, since this distribution is closed to linear operations \textbf{[reference]}. For this reason, a state and output trajectory over a discrete time interval is a collection of random variables following the Gaussian distribution, i.e., a stochastic process known as a Gaussian Process (GP) \textbf{[reference]}.

\begin{boxed-definition}{(Gaussian Process)} \label{def:gaussianProcess}
    Given a Gaussian process defined as a time-series collection of random variables between a discrete time interval:
    \begin{equation}
        \bm{X} = \left\{ \bm{x}_k \sim \mathcal{N}(\mu_{\bm{x}_k}, \Sigma_{\bm{x}_k}) ; k \in [0, K] \right\}
    .\end{equation}
    
    If this data is obtained from a causal system, the joint probability of the random variables is modeled as:
    \begin{equation}
        p(\bm{X}) = p(\bm{x}_0, \bm{x}_1, \cdots, \bm{x}_K) = \prod_{k=0}^K p(\bm{x}_k | \bm{x}_{k-1}, \cdots, \bm{x}_1, \bm{x}_0)
    .\end{equation}
\end{boxed-definition}

Using this definition, the dynamic response $bm{x}_{k+1}$ of a system for a time instant $k+1$, given its trajectory until the actual state $bm{x}_{k}$, is given by the conditional probability $p(\bm{x}_{k+1} | \bm{x}_{k}, \cdots, \bm{x}_1, \bm{x}_0)$. One can assume, however, that an actual state stores enough information from the past states to unequivocally determine the state response, so that the conditional probability can be restated as:
\begin{equation}
    p(\bm{x}_{k+1} | \bm{x}_{k}, \cdots, \bm{x}_1, \bm{x}_0) =  p(\bm{x}_{k+1} | \bm{x}_{k})
.\end{equation}

This assumption is known as the \textit{Markov property}, and a process that exhibits this property is known as a Markov process \textbf{[reference]}. With this formulation, is always possible to marginalize the probability distribution of a single random variable $\bm{x}_{k+1}$ given a probability distribution over the initial state $\bm{x}_0$, which are related through the discrete Chapman-Kolmogorov equation:
\begin{equation}
    p(\bm{x}_{k+1}) =  p(\bm{x}_{k+1} | \bm{x}_{k}) p(\bm{x}_{k} | \bm{x}_{k-1}) \cdots p(\bm{x}_{1} | \bm{x}_{0})p(\bm{x}_0) = p(\bm{x}_0) \prod_{i=0}^k p(\bm{x}_{i+1} | \bm{x}_{i})
.\end{equation}


Combining this assumption with Definition \ref{def:gaussianProcess}, in order to describe a Gaussian-Markov Process, and using the model from Definition \ref{def:stochSS} it is possible to completely define a stochastic dynamical system in respect to the random variables probabilities.

\begin{boxed-definition}{(Hidden Markov Model)} \label{def:HMM}
    Given a stochastic State-Space model as in Definition \ref{def:stochSS}, and assuming the Markov property, a \textit{Hidden Markov Model} for this dynamical system at a time instant $k$ is defined through the two distributions:
    \begin{align}
    \begin{cases}
        p(\bm{x}_{k} | \bm{x}_{k-1}, \bm{u}_{k-1}) & \text{(Transition Distribution)} \\
        p(\bm{y}_{k} | \bm{x}_{k}) & \text{(Emission Distribution)}
    \end{cases}
    .\end{align}
    
    Furthermore, since the noises $\bm{w}_k$ and $\bm{v}_k$ are white Gaussian noises, these distributions are modeled as the Normal conditional distributions:
    \begin{align}
    \begin{cases}
        p(\bm{x}_{k} | \bm{x}_{k-1}) &= \mathcal{N}(\bm{x}_k | \bm{A}\bm{x}_{k-1} + \bm{B}\bm{u}_{k-1}, \bm{Q}_{kf}) \\
        \hfill p(\bm{y}_{k} | \bm{x}_{k}) &= \mathcal{N}(\bm{y}_k | \bm{C}\bm{x}_{k}, \bm{R}_{kf})
    \end{cases}
    .\end{align}
\end{boxed-definition}

A Hidden-Markov Model is a statistical interpretation of dynamical system that is well-known in the literature \textbf{[reference?]}. An illustration of the dependence between these variables as a graphical model is depicted in Fig. \ref{fig:HMM01}. Notice that, in this formulation, the state-vector $\bm{x}_k$ at each time instance is a latent variable whose actual value is not known, but can be observed through the emission variable $\bm{y}_k$. This raises the problem of state estimation as a common inference problem in engineering applications known as the \textit{filtering}, which consists on infer the actual value of $\bm{x}_k$ given the entire history of observations $\bm{y}_0, \cdots, \bm{y}_k$ and control actions $\bm{u}_0, \cdots, \bm{u}_k$. Thus, the actual state can be obtained through the distribution:
\begin{equation}
    p(\bm{x}_k | \bm{y}_{k}, \bm{y}_{k-1}, \cdots, \bm{y}_0, \bm{u}_{k-1}, \bm{u}_{k-2}, \cdots, \bm{u}_0)
,\end{equation}

\noindent from where a representative value, such as the expected value $\mathbb{E}\left[ p(\bm{x}_k | \bm{y}_{k}, \cdots, \bm{y}_0, \bm{u}_{k-1}, \cdots, \bm{u}_0) \right]$ can be selected as the value used by the controller to determine an action.

\begin{figure}[ht]
    \centering
    \resizebox{!}{!}{
    \begin{tikzpicture}[auto, node distance=1.75cm,>=latex', scale=0.2]
        % Nodes
        \node [mcInput] (u0) {$u_0$};
        \node [mcCircle, below of=u0] (x0) {$x_0$};
        \node [mcCircle, below of=x0] (y0) {$y_0$};
        
        \node [mcInput, right of=u0] (u1) {$u_1$};
        \node [mcCircle, below of=u1] (x1) {$x_1$};
        \node [mcCircle, below of=x1] (y1) {$y_1$};
        
        \node [text width=2em, right of=x1] (cdots1) {$\cdots$};    
        
        \node [mcCircle, right of=cdots1] (xi) {$x_i$};     
        \node [mcInput, above of=xi] (ui) {$u_i$};
        \node [mcCircle, below of=xi] (yi) {$y_i$};
        
        \node [text width=2em, right of=xi] (cdots2) {$\cdots$};
        
        \node [mcCircle, right of=cdots2] (xk) {$x_k$};
        \node [mcCircle, below of=xk] (yk) {$y_k$};
        
        % Lines
        \draw [->] (u0) -- (x1);
        \draw [->] (u1) -- (cdots1);
        \draw [->] (ui) -- (cdots2);
        
        \draw [->] (x0) -- (x1);
        \draw [->] (x1) -- (cdots1);
        \draw [->] (cdots1) -- (xi);
        \draw [->] (xi) -- (cdots2);
        \draw [->] (cdots2) -- (xk);
        
        \draw [->] (x0) -- (y0);
        \draw [->] (x1) -- (y1);
        \draw [->] (xi) -- (yi);
        \draw [->] (xk) -- (yk);
        
    \end{tikzpicture} 
    }
    \caption{A Hidden Markov Model for a discrete-time system, where the shaded boxes indicates that the inputs are not random variables.}
    \label{fig:HMM01}
\end{figure}

There are several approaches to solve the filtering and other related problems in statistical state estimation \textbf{[reference-Sarkka]}, but the most popular method has been the Kalman filter \textbf{[reference]} which is discussed below for a discrete-time system.

\begin{boxed-theorem}{(Kalman Filter)} \label{th:kalmanFilter}
    Given a Hidden Markov Model as in Definition \ref{def:HMM} and initial state distribution $\bm{x}_0 \sim \mathcal{N}(\bm{m}_0, \bm{P}_0)$, the filtering distribution can be solved in closed-form as:
    \begin{equation}
        p(\bm{x}_k | \bm{y}_{k}, \cdots, \bm{y}_0, \bm{u}_{k-1}, \cdots, \bm{u}_0)  = \mathcal{N}(\hat{\bm{x}}_k, \bm{P}_k)
    ,\end{equation}   
    
    \noindent where the mean $\bm{m}_k$ and covariance $\bm{P}_k$ are recursively computed through the prediction and update steps below:
    \begin{align}
    \begin{matrix*}[l]
    \textbf{Predict Step:} & & & \textbf{Update Step:} \\ \\
    \hat{\bar{\bm{x}}}_k = \bm{A} \hat{\bm{x}}_{k-1} + \bm{B} \bm{u}_{k-1} & & & \bm{K}_k = \bar{\bm{P}}_k \bm{C}^T (\bm{R}_{kf} + \bm{C} \bar{\bm{P}}_k \bm{C}^T)^{-1} \\
    \bar{\bm{P}}_k = \bm{A} \bm{P}_{k-1} \bm{A}^T + \bm{Q_{kf}} & & & \hat{\bm{x}}_k = \hat{\bar{\bm{x}}}_k + \bm{K}_k(\bm{y}_k - \bm{C} \hat{\bar{\bm{x}}}_k)\ \\
    & & & \bm{P}_k = \bar{\bm{P}}_k - \bm{K}_k (\bm{R}_{kf} + \bm{C} \bar{\bm{P}}_k \bm{C}^T) \bm{K}_k^T
    \end{matrix*}   
    .\end{align}
\end{boxed-theorem}

\begin{proof}
    Consider the transition and emission distributions given as:
    \begin{align}
    \begin{cases}
        p(\bm{x}_{k} | \bm{x}_{k-1}) &= \mathcal{N}(\bm{x}_k | \bm{A}\bm{x}_{k-1} + \bm{B}\bm{u}_{k-1}, \bm{Q}_{kf}) \\
        \hfill p(\bm{y}_{k} | \bm{x}_{k}) &= \mathcal{N}(\bm{y}_k | \bm{C}\bm{x}_{k}, \bm{R}_{kf})
    \end{cases}
    .\end{align}
    
    Consider, now, the joint distribution of $\left( \bm{x}_{k-1}, \bm{x}_k \right)$ conditioned on measurement history $\left( \bm{y}_0, \bm{y}_1, \cdots, \bm{y}_{k-1} \right)$. Since the joint and condition distributions between two Gaussians distributions are themselves Gaussian, this density can be expressed as:
    \begin{align}
    \begin{split}
        p(\bm{x}_k, \bm{x}_{k-1} | \bm{y}_0, \bm{y}_1, \cdots, \bm{y}_{k-1}) &= p(\bm{x}_k | \bm{x}_{k-1}) p(\bm{x}_{k-1} | \bm{y}_0, \bm{y}_1, \cdots, \bm{y}_{k-1}) \\
        &= \mathcal{N}(\bm{x}_k | \bm{A}\bm{x}_{k-1}+\bm{B}\bm{u}_{k-1}, \bm{Q}_{kf}) \mathcal{N}(\bm{x}_{k-1} | \hat{\bm{x}}_{k-1}, \bm{P}_{k-1}) \\
        &= \mathcal{N}\left( \begin{bmatrix} \bm{x}_{k-1} \\ \bm{x}_{k} \end{bmatrix}    \right| \left. \begin{bmatrix} \hat{\bm{x}}_{k-1} \\ \bm{A}\bm{x}_{k-1}+\bm{B}\bm{u}_{k-1} \end{bmatrix}, \begin{bmatrix} \bm{P}_{k-1} & \bm{P}_{k-1} \bm{A}^T  \\ \bm{A} \bm{P}_{k-1} & \bm{A} \bm{P}_{k-1} \bm{A}^T + \bm{Q}_{kf} \end{bmatrix}  \right)
    \end{split}
    .\end{align}
    
    Marginalizing the joint distribution, it is possible to obtain the distribution of $\bm{x}_k$ conditioned in the measurement history as:
    \begin{equation}
        p(\bm{x}_k | \bm{y}_0, \bm{y}_1, \cdots, \bm{y}_{k-1}) = \mathcal{N}( \underbrace{\bm{A} \bm{x}_{k-1} + \bm{B} \bm{u}_{k-1}}_{\hat{\bar{\bm{x}}}_k}, \underbrace{\bm{A} \bm{P}_{k-1} \bm{A}^T + \bm{Q}_{kf}}_{\bar{\bm{P}}_k} )
    ,\end{equation}
    
    \noindent which completes the \textit{prediction step} of the Kalman filter. For the update step, consider the joint distribution between $(\bm{x}_k, \bm{y}_k)$, the actual state and actual measurement:
    \begin{align}
    \begin{split}
        p(\bm{x}_k, \bm{y}_{k} | \bm{y}_0, \bm{y}_1, \cdots, \bm{y}_{k-1}) &= p(\bm{y}_k | \bm{x}_{k}) p(\bm{x}_{k} | \bm{y}_0, \bm{y}_1, \cdots, \bm{y}_{k-1}) \\
        &= \mathcal{N}(\bm{y}_k | \bm{C}\bm{x}_{k}, \bm{R}_{kf}) \mathcal{N}(\bm{x}_{k} | \bar{\hat{\bm{x}}}_k, \bar{\bm{P}}_k) \\
        &= \mathcal{N}\left( \begin{bmatrix} \bm{x}_{k} \\ \bm{y}_{k} \end{bmatrix}  \right| \left. \begin{bmatrix} \bar{\hat{\bm{x}}}_k \\ \bm{C}\bar{\hat{\bm{x}}}_k \end{bmatrix}, \begin{bmatrix} \bar{\bm{P}}_k & \bar{\bm{P}}_k \bm{C}^T  \\ \bm{C} \bar{\bm{P}}_k & \bm{C} \bar{\bm{P}}_k \bm{C}^T + \bm{R}_{kf} \end{bmatrix}   \right)
    \end{split}
    .\end{align}
    
    Therefor, the filtering distribution can be obtained as:
    \begin{align}
        p(\bm{x}_k | \bm{y}_k, \bm{y}_0, \bm{y}_1, \cdots, \bm{y}_{k-1}) = \mathcal{N}(\bm{x}_k | \hat{\bm{x}}_k, \bm{P}_k)
    ,\end{align}

    \noindent where $\hat{\bm{x}}_k$ and $\bm{P}_k$ are the computations of the update step, given as:
    \begin{align}
    \begin{cases}
        \hat{\bm{x}}_k = \hat{\bar{\bm{x}}}_k + \bar{\bm{P}}_k \bm{C}^T (\bm{R}_{kf} + \bm{C} \bar{\bm{P}}_k \bm{C}^T)^{-1}(\bm{y}_k - \bm{C}\bm{x}_k) \\
        \bm{P}_k = \bar{\bm{P}}_k - \bar{\bm{P}}_k \bm{C}^T (\bm{R}_{kf} + \bm{C} \bar{\bm{P}}_k \bm{C}^T)^{-1} \bm{C} \bar{\bm{P}}_k
    \end{cases}
    .\end{align}
    
    Finally, making $\bm{K}_k = \bar{\bm{P}}_k \bm{C}^T (\bm{R}_{kf} + \bm{C} \bar{\bm{P}}_k \bm{C}^T)^{-1}$ establishes the theorem.
\end{proof}

Notice that the Kalman filter solution implies that an ``open-loop" estimation of the system, $\hat{\bar{\bm{x}}}_k$, can be corrected by the equation:
\begin{equation}
    \hat{\bm{x}}_k =\hat{\bar{\bm{x}}}_k + \bm{K}_k (\bm{y}_k - \bm{C} \hat{\bar{\bm{x}}}_k)
,\end{equation}

\noindent which, by expanding $\hat{\bar{\bm{x}}}_k$, results in a similar state-equation expression for a system with a deterministic observer as defined earlier, but for a time-varying gain $\bm{K}_k$:
\begin{equation}
\begin{split}
    \hat{\bm{x}}_k &= \bm{A} \hat{\bar{\bm{x}}}_k + \bm{B} \bm{u}_k + \bm{K}_k (\bm{y}_k - \bm{C} \hat{\bar{\bm{x}}}_k) \\
    &= \left( \bm{A} - \bm{K}_k \bm{C} \right) \hat{\bar{\bm{x}}}_k + \bm{B} \bm{u}_k + \bm{K}_k \bm{y}_k
\end{split}
.\end{equation}

Notice that, despite the similarity with Luenberger observers, the Kalman gain has the advantage that the gain matrix $\bm{K}_k$ is not resulting from a designer procedure but rather from solving an inference problem whose only additional information needed are the covariances $\bm{Q}_{kf}$ and $\bm{R}_{kf}$, which can be assumed or estimated. In addition to that, it can be shown that the recursive steps that solves the filtering problem are also responsible for minimizing the expectation of the error between the measurements and actual state response, given as:
\begin{equation}
    \bm{J} = \mathbb{E}\left[ (\bm{x}_k - \hat{\bm{x}}_k)(\bm{x}_k - \hat{\bm{x}}_k)^T \right]
.\end{equation}

Since this method minimizes some cost function, it is indeed an optimal estimator. In fact, the Kalman filter can be formulated through an optimal control formulation, which motivates the development of a continuous-time version of this estimator, as shown below.

\begin{boxed-theorem}{(Kalman-Bucy Filter)} \label{th:kalmanBucy}
    Consider a continuous-time State-Space linear system subject to additive process noise $\bm{w}_k \sim \mathcal{N}(\bm{0}, \bm{Q}_{kf})$ and measurement noise $\bm{v}_k \sim \mathcal{N}(\bm{0}, \bm{R}_{kf})$, where the covariances $\bm{Q}_{kf} \in \mathbb{R}^{n \times n}$  and $\bm{R}_{kf} \in \mathbb{R}^{p \times p}$ represents the \emph{power spectral density} of the noises.   In this case, for an estimated state $\hat{\bm{x}}(t)$ at time $t$, the error covariance:
    \begin{equation}
        \bm{J}(\bm{x}, \hat{\bm{x}}, t) = \mathbb{E}\left\{ [\bm{x}(t) - \hat{\bm{x}}(t)][\bm{x}(t) - \hat{\bm{x}}(t)]^T \right\}
    \end{equation}
    
    \noindent is minimized by $\hat{\bm{x}}(t)$ obtained through the system:
    \begin{equation}
        \dot{\hat{\bm{x}}}(t) = \bm{A} \hat{\bm{x}}(t) + \bm{B} \bm{u}(t) + \bm{K}_{e}(t) \left( \bm{y}(t) - \bm{C} \hat{\bm{x}}(t) \right)
    ,\end{equation}
    
    \noindent where $\bm{K}_e(t) = \bm{P}_e(t)\bm{C}\bm{R}^{-1}$, being $\bm{P}_e(t)$ the solution of the Riccati differential matrix equation:
    \begin{equation}
        \dot{\bm{P}_e}(t) = \bm{A} \bm{P}_e(t) + \bm{P}_e(t) \bm{A}^T - \bm{P}_e(t)\bm{C}^T\bm{R}_{kf}^{-1} \bm{C} \bm{P}_e(t) + \bm{Q}_{kf}
    ,\end{equation}
    
    \noindent with initial condition $\bm{P}_e(t_0) = \mathbb{E} \left\{ [\bm{x}(t_0) - \bar{\bm{x}}(t_0)][\bm{x}(t_0) - \bar{\bm{x}}(t_0)]^T \right\}$ for $t_0 > -\infty$.
\end{boxed-theorem}

A detailed proof of this theorem can be found in \textbf{[reference]}. The formulation of an optimal state estimator further emphasizes the duality between controllers and observers. Notice that the gain $\bm{K}_e(t)$ of a Kalman-Bucy filter depends on a matrix $\bm{P}_e(t)$ that solves the exact same Riccati differential equation as the one in Theorem \ref{th:LQRAction}, but forward in time for matrices $\bm{A}^T$ and $\bm{C}^T$ instead of $\bm{A}$ and $\bm{B}$. Notice, however, that these filters are not limited to a terminal time $T$, since the boundary condition only requires information on the initial time $t_0 > -\infty$, which implies that they could also be used in forward infinite-horizon operations. Another direct result of the duality is that is possible to derive a estimator considering $t_0 \to -\infty$, which, as in Theorem \ref{th:infiniteLQR}, results in a time-invariant filter with gain $K_e$ \textbf{[reference]}.

\section{Linear Quadratic Gaussian (LQG)}

This chapter presented formulation for controllers and state estimators that are optimal, in the sense that they minimize some cost function based on information about the model and measurements. In the previous chapter, the Theorem \ref{th:separationPrinciple} stated that a controller and a estimator can be designed separately, and a resulting control by feedback of estimated states is always feasible. In this section, a similar configuration is shown for optimal control and estimation operations, as depicted in Fig. \ref{fig:lqg01}. In this illustration, the open-loop system can represent either a mathematical model (possibly non-linear and time-varying) being simulated or a physical system whose output is only measured through some sensor device. 

\begin{figure}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}[auto, node distance=2cm,>=latex']
        % We start by placing the blocks
        \node [input, name=input] {};
        \node [sum, right of=input, node distance=3em] (intFbckSum) {};
        \node [block, right of=intFbckSum, node distance=4em] (integralAction) {$\int$};
        \node [block, right of=integralAction, node distance=6em] (intActGain) {$\bm{K}_a$};
        \node [sum, right of=intActGain, node distance=4.5em] (fbckSum) {};

        \node [block, fill=myBlue!20, minimum height=4em, right of=fbckSum, node distance=14em] (openloopSystem) {Open-Loop System};    
        \node [output, right of=outputMatrix, node distance=25em] (output) {};
        
        \node [block, below of=openloopSystem, node distance=12.5em] (integralLU) {$\int$};     
        \node [sum, left of=integralLU, node distance=4em] (stateSumLU) {};             
        \node [block, left of=stateSumLU, node distance=4em] (inputMatrixLU) {$\bm{B}$};
        \node [block, right of=integralLU, node distance=8em] (outputMatrixLU) {$\bm{C}$};
        \node [block, below of=integralLU] (stateMatrixLU) {$\bm{A}$};
        \node [block, above of=integralLU] (kalmanGain) {$\bm{K}_e$};       
        
        \draw [->] (openloopSystem) -- node[name=bk3,pos=0.5]{} node[name=bk5,pos=0.75]{} node[pos=0.9]{$\bm{y}$} (output);
        \node [sum, anchor=base] (fbckOutput) at (bk3.base |- kalmanGain) {};
        
        \node [block, below of=stateSumLU, node distance=11em, label={below:LQR Controller}] (fbckGain) {$\bm{K}$};     
        
        
        \begin{scope}[on background layer]
            \node [fit=(integralAction) (intActGain), fill= myGreen!10, rounded corners, inner sep=.4cm, label={[xshift=4.3em, myGreen!90]above left:Integral Action}] {};
        \end{scope} 
        
        \begin{scope}[on background layer]
            \node [fit=(inputMatrixLU) (kalmanGain) (stateMatrixLU) (fbckOutput), fill= myRed!10, rounded corners, inner sep=.4cm, label={[xshift=5.8em, myRed!90]above left:Kalman-Bucy Filter}] {};
        \end{scope} 
        
        % Once the nodes are placed, connecting them is easy.       
        \draw [draw,->] (input) -- node[pos=0.1] {$\bm{r}$} node[pos=0.8] {$+$}  (intFbckSum);
        \draw [->] (intFbckSum) -- (integralAction);
        \draw [->] (integralAction) -- node[pos=0.5]{$\bm{x}_a$} (intActGain);
        \draw [->] (intActGain) -- node[pos=0.8]{$-$} (fbckSum);        
        \draw [->] (fbckSum) -- node[pos=0.2]{$\bm{u}$} node[name=bku1, pos=0.2]{} (openloopSystem);
        

        \draw [->] (fbckGain) -| node[pos=0.98]{$-$} (fbckSum);
        \draw [->] (bk5) |- ++(0, -28em) -| node[pos=0.98]{$-$} (intFbckSum);
        
        
        \draw [->] (bku1) |- (inputMatrixLU);
        \draw [->] (inputMatrixLU) -- node[pos=0.8]{$+$} (stateSumLU);
        \draw [->] (stateSumLU) -- node[pos=0.5]{$\dot{\hat{\bm{x}}}$} (integralLU);
        \draw [->] (integralLU) -- node[name=bk4,pos=0.5]{} node[pos=0.5]{$\hat{\bm{x}}$} (outputMatrixLU);
        \draw [->] (outputMatrixLU) -| node[pos=0.95]{$-$} (fbckOutput);
        \draw [->] (bk4) |- (stateMatrixLU);
        \draw [->] (fbckOutput) -- (kalmanGain);
        \draw [->] (stateMatrixLU) -| node[pos=0.95]{$+$} (stateSumLU);
        \draw [->] (kalmanGain) -| node[pos=0.95]{$+$} (stateSumLU);
        \draw [->] (bk3) -- node[pos=0.95]{$+$} (fbckOutput);
        
        \draw [->] (bk4) |- (fbckGain);
    \end{tikzpicture} 
    }
    \caption{Block diagram of a Linear Quadratic Gaussian control configuration.}
    \label{fig:lqg01}
\end{figure}

The control configuration that connects a optimal control action of a Linear Quadratic Regulator together with the estimation states from a Kalman filter (or Kalman-Bucy filter) is know as a Linear Quadratic Gaussian (LQG) controller \textbf{[reference]}. Since the controller is independent of the estimator, it is also possible to include integral action to the configuration, as shown in the block diagram, presenting this architecture as a general configuration that can deal with a broad range of applications. Because of the stochastic nature of the estimation and the optimal procedure of the control, this configuration is central to the field of Stochastic Optimal Control \textbf{[reference?]}. A formal definition is given below.

\begin{boxed-definition}{(Linear Quadratic Gaussian)} \label{def:lqg}
    Consider a stochastic system in State-Space representation:
	\begin{align}
    \begin{cases}
        \hfill \dot{\bm{x}}(t) &= \bm{A} \bm{x}(t) + \bm{B} \bm{u}(t) + \bm{w}(t) \\
        \hfill \bm{y}(t) &= \bm{C} \bm{x}(t) + \bm{v}(t)
    \end{cases}
    ,\end{align}
    
     \noindent whose estimated state-vector $\hat{\bm{x}}(t)$ is reconstructed from a Kalman-Bucy filter and whose input signal $\bm{u}(t)$ is calculated through a finite-horizon LQR. The Linear Quadratic Gaussian (LQG) control for the horizon $t \in [t_0, T]$, with $-\infty < t_0 \leq T < \infty$, is defined as:
    \begin{align} \label{eq:lqg01}
        \dot{\hat{\bm{x}}}(t) = \left[\bm{A} - \bm{K}_e(t) \bm{C} - \bm{B} \bm{K}(t) \right] \hat{\bm{x}}(t) + \bm{K}_e(t) \bm{y}(t) 
    ,\end{align}
    
    \noindent where $\bm{K}(t) = \bm{R}^{-1}\bm{B}^T \bm{P}(t)$ and $\bm{K}_e(t) = \bm{P}_e(t) \bm{C} \bm{R}^{-1}$ are, respectively, the LQR and Kalman-Bucy gains for matrices $\bm{P}(t)$ and $\bm{P}_e(t)$ that solves the Riccati differential equations:
    \begin{align}
    \begin{cases}
        -\dot{\bm{P}}(t) = \bm{A}^T \bm{P}(t) + \bm{P}(t) \bm{A} - \bm{P}(t) \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P}(t) + \bm{Q} \\
        \phantom{-} \dot{\bm{P}_e}(t) = \bm{A} \bm{P}_e(t) + \bm{P}_e(t) \bm{A}^T - \bm{P}_e(t)\bm{C}^T\bm{R}_{kf}^{-1} \bm{C} \bm{P}_e(t) + \bm{Q}_{kf}
    \end{cases}
    \end{align}
    
    \noindent for boundary conditions $\bm{P}(T) = \bm{Q}_f$ and $\bm{P}_e(t_0) = \mathbb{E} \left\{ [\bm{x}(t_0) - \bar{\bm{x}}(t_0)][\bm{x}(t_0) - \bar{\bm{x}}(t_0)]^T \right\}$.
\end{boxed-definition}

Notice that another advantage of this controller-estimator configuration is that the solution for both Riccati differential equations does not depends on the states at any time $\bm{x}(t)$, with the exception of the boundary condition $\bm{P}_e(t_0)$ which can in any case be obtained before the system operation. Therefore, the values for matrices $\bm{P}(t)$ and $\bm{P}_e(t)$, and consequently $\bm{K}(t)$ and $\bm{K}_e(t)$, for $t \in [t_0, T]$, can be evaluated beforehand and then applied in the on-line operation.

The LQG controller can also be used for tracking non-constant references, given that the system can be augmented while maintaining controllability. The augmented system is used to calculate the controller gains $\bm{K}(t)$ using Definition \ref{def:lqServo}, while the estimator gains $\bm{K}_e(t)$ are optimized through the original system. Given that it can optimally solve both the regulator and tracking problems, while still accounting for uncertainty in the system, the Linear Quadratic Gaussian poses as a high performing multi-purpose controller. The resulting augmented system can be represented by the closed-loop state equation:
\begin{align}
\begin{cases}
        \begin{bmatrix}
            \dot{\hat{\bm{x}}}(t) \\
            \dot{\hat{\bm{x}}}_a(t)
        \end{bmatrix} &= \begin{bmatrix}
            \bm{A} - \bm{B} \bm{K}(t) - \bm{K}_e(t) \bm{C} & -\bm{B} \bm{K}_a(t) \\ - \bm{C} & \bm{0}
        \end{bmatrix} \begin{bmatrix}
            \hat{\bm{x}}(t) \\
            \hat{\bm{x}}_a(t)
        \end{bmatrix} + \begin{bmatrix}
            \bm{K}_e(t) & \bm{0}  \\
            \bm{0} & \bm{I} 
        \end{bmatrix} \begin{bmatrix}
	        \bm{y}(t) \\ \bm{r}(t)
        \end{bmatrix} \\
        \hfill \bm{y}(t) &= \begin{bmatrix}
            \bm{C} & \bm{0}
        \end{bmatrix} \begin{bmatrix}
            \bm{x}(t) \\
            \bm{x}_a(t)
        \end{bmatrix}
    \end{cases}
.\end{align}

For the sake of illustration, consider the same system as controlled in \eqref{eq:isoSys02Rep}. Consider, now, that it is perturbed by noises $\bm{w}_k \sim \mathcal{N}(\bm{0}, \bm{Q}_{kf})$ and $\bm{v}_k \sim \mathcal{N}(\bm{0}, \bm{R}_{kf})$, with covariances:
\begin{equation}
	\begin{matrix}
		\bm{Q}_{kf} = \begin{bmatrix}
			0.2741  &  0.0036 \\
		    0.0036  &  0.0024
		\end{bmatrix} & \bm{R}_{kf} = 0.233
	\end{matrix}
.\end{equation}

\noindent A pair of LQG controllers for tracking are obtained by augmenting the system and solving the controller Ricatti differential equation for weights in the form $\bm{Q} = diag(\alpha_1, \alpha_2, \alpha_3)$ and $\bm{R} = \beta$, for terminal weight $\bm{P}(T) = \bm{Q}$. The estimator is solved forward in time for initial condition $\bm{P}_e(t_0) = \bm{Q}_{kf}$. The resulting simulations are shown in Fig. \ref{fig:lqg02}, demonstrating how the controller can be more or less aggressive given the choice of weighting matrices.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{chapter4/report_ch4_1}
	\caption{Simulation of the LQG controllers showing the input signal (up) and correspondent state response (down). The markers denotes the observations from the real system. The control on the left was solved for $\bm{Q}^{(1)} = \text{diag}(20, 20, 5000)$ and $\bm{R}^{(1)} = 75$, and the controller on the right was solved for $\bm{Q}^{(2)} = \text{diag}(1, 1, 10000)$ and $\bm{R}^{(2)} = 20$.}
	\label{fig:lqg02}
\end{figure}

\section{Stability and Robustness Analysis}

In the last chapter, the poles of the feedback controlled system where directly assigned from the pole-placement method. In this chapter, however, the controllers were results of optimization procedures, so it is necessary to discuss whether or not these controllers are stable and robust given the criteria already presented. 

First of all, the stability of the closed-loop system via Linear Quadratic Regulator is discussed. One might conclude that, since this controller is obtained from a optimization process, the resulting controller can not be unstable. Consider, however, the popular counter-intuition: consider a scalar system $\dot{x}(t) = x(t) + u(t)$, with $R = 1$ and $Q = 0$, such that its associated LQR cost function is:
\begin{equation}
    J(x,u,t_0) = \int_{t_0}^{\infty} u^2(t) dt
.\end{equation}

In this case, the optimal control action is always $u(t) = 0$, for $t \in [t_0, \infty]$. However, this control action results in the system $\dot{x}(t) = x(t)$ which is clearly BIBO unstable. Therefore, there are conditions in whether the optimization process results in a stable or unstable closed-loop system, as shown by the following theorem.

\begin{boxed-theorem}{(LQR Assymptoptic Stability)} \label{th:lqrStab}
    Consider the infinite-horizon Linear Quadratic Regulator as defined in Theorem \ref{th:infiniteLQR}. If the system is observable, then the closed-loop matrix $\bm{A}_{cl} = (\bm{A} - \bm{B}\bm{K}) = (\bm{A} - \bm{B}\bm{R}^{-1}\bm{B}^T\bm{P})$ is BIBO stable.
\end{boxed-theorem}

\begin{proof}
    Consider, without loss of generalization, that $\bm{Q} = \bm{C}^T \bm{C}$, which is still positive definite. In this case $\bm{C} = \bm{Q}^{1/2}$, which is nonsingular and the observability assumption holds. Furthermore, consider the LQR cost function given an optimal input trajectory $\bm{u}^* : \mathbb{R} \rightarrow \mathbb{R}^r$:
    \begin{equation} \label{eq:lqrStab01}
    J(\bm{x},\bm{u}^*,t_0) = \int_{t_0}^{\infty} \left( \bm{x}^T (\bm{C}^T \bm{C}) \bm{x} + (\bm{u}^*)^T \bm{R} \bm{u}^* \right) dt = \int_{t_0}^{\infty} \left( \bm{y}^T \bm{y} + (\bm{u}^*)^T \bm{R} \bm{u}^* \right) dt
.\end{equation}

The optimal output trajectory resulting from this control action is given by $\bm{y}^*(t) = \bm{C} \bm{x}^*(t)$, where $\bm{x}^*(t)$ is the correspondent optimal state trajectory. Since $\bm{u}^*(t)$ was obtained by solving the algebraic Riccati equation on Theorem \ref{th:infiniteLQR}, a well-known result is that the optimal cost function is bounded:
\begin{equation}
    \bm{V}^*(\bm{x}, t_0) \leq \bm{x}^T(t_0) \bm{P} \bm{x}(t_0)
,\end{equation}

\noindent and, therefore, \eqref{eq:lqrStab01} must also be bounded. This is only possible if $\bm{y}^*(t) \to 0$ and $\bm{u}^*(t) \to 0$ as $t \to \infty$, which, since the system is observable, directly implies $\bm{x}^*(t) \to 0$ as $t \to \infty$, concluding that the system is BIBO stable.
\end{proof}

Notice that, in contrast to the Pole-Placement method where the stability must be explicitly determined by the designer, the LQR controller can guarantee a stable closed-loop system directly by the observability property, even when the open-loop system is unstable. Since the BIBO stability (Definition \ref{def:BIBOStab}) discusses the magnitude of a system response as $t \to \infty$, the results are derived for infinite-horizon LQR. Despite of this, it is also possible to interpret some notion of stable response for finite-horizon LQR, since the cost function is always bounded by the optimal cost as conditioned by the Hamilton-Jacobi equation.

Now, the discussion turns to the robustness properties of these optimal controllers. It can be shown that the LQR has a gain margin $GM = \infty$ and a phase margin $PM \geq \ang{60}$ \textbf{[reference]}. This assumption can be verified from a visual inspection of the Nyquist plot, as shown in Fig. \ref{fig:lqrProperties01}. The reason for these values comes from the fact that the Nyquist diagram of an optimal regulator never crosses the unit circle centered at $s = -1$ (implied by the Return Difference Equality). Therefore, the diagram never crosses the point $s = -1$ no matter how much the gain $K$ is changed, which establishes the infinite gain margin. In addition, the angle that the closest permissible point of a unit circle centered in $s = 0$ makes with the point $s = -1$ is exactly $\ang{60}$, which establishes the lower bound on the gain phase.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.65]{chapter4/report_ch4_2}
	\caption{Nyquist diagram of a single state system with a open-loop unstable pole but which is closed-loop stable. The gain calculated by a LQR stabilizing controller never crosses the unit circle centered at $s = -1 + j0$.}
	\label{fig:lqrProperties01}
\end{figure}

Of course, there is no physical support of saying that a controller device exhibits a infinite gain margin, since there is always uncertainty about the model, the instrumentation and the environment. However, the fact that the mathematical model displays infinite gain margin allows for the assumption that the physical controller will have an extremely large margin. In fact, the uncertainty just mentioned is the main motivation into using optimal state estimators to reconstruct the state-vector from data, and it was shown by \textbf{[reference]} that an optimal controller-estimator configuration, i.e., the Linear Quadratic Gaussian, has no guaranteed margins.

% 5 - Receding Horizon Optimal Control
% ---------------------------------------------------------------
\clearpage
\chapter{Receding Horizon Optimal Control}

The previous chapter introduced the concept of finite-horizon optimal control, presenting the Linear Quadratic Regulator as a solution to the condition imposed by the Hamilton-Jacobi equation. Despite being a popular solution with nice properties, there are some downsides of the LQR standard formulation. This chapter explores a optimal controller formulation based on a receding horizon control strategy. In the first section the motivation and definition of the moving horizon strategy are stated. The second sections explores the possibility of combining the receding horizon strategy with an iterative linearization procedure to yield an optimal operation in respect to the nonlinear system itself.

\section{Optimization in Moving Horizons}

A major disadvantage of the standard finite-horizon optimal controller is that the time in which the system will operate becomes restricted to the time horizon used for the optimization. This might becomes a problem in the case that the system is interrupted or subjected to strong disturbances, given that corrective action would require time and the desired state could not be achieved in the given horizon. Similarly, the restricted time horizon is also problematic in the case that the system should track a non-constant reference signal, since it also has be assumed of fixed length for the optimization to account for the entire signal. 

A possible solution to this issue is to perform a Receding Horizon Control (RHC) strategy \textbf{[references]}. In the context of optimal control, a receding horizon controller is also known in literature as the \textit{Model-Predictive Controller} \textbf{[references]}, although this name is usually associated with constrained optimization procedures. The strategy is defined as follows:

\begin{boxed-definition}{(Receding-Horizon Control)} \label{def:recedingHorizon}
	Considers a linear system in State-Space form and a control horizon of size $T \in \mathbb{R}^+$. The \textit{Receding-Horizon Control} strategy for an optimal controller of this system is represented as:
	\begin{equation}
        \dot{\bm{x}}(t) = \left( \bm{A} - \bm{B} \bm{K}(t) \right) \bm{x}(t)
    ,\end{equation} 
    
	\noindent where $\bm{K}(t) = - \bm{R}^{-1} \bm{B}^T \bm{P}(t)$ represents the optimal gain from the LQR cost function (Definition \ref{def:lqr}) for an iterative time interval $[t, t+T]$, and $\bm{P}(t)$ which solves the the Riccati differential equation:
    \begin{equation}
        -\dot{\bm{P}}(t) = \bm{A}^T \bm{P}(t) + \bm{P}(t) \bm{A} - \bm{P}(t) \bm{B} \bm{R}^{-1} \bm{B}^T \bm{P}(t) + \bm{Q}
    ,\end{equation}
    
    \noindent with terminal condition $\bm{P}(t+T) = \bm{Q}_f$.
\end{boxed-definition}

In the optimization community, this problem is also popular in the following notation, where the moving horizon becomes explicit:
\begin{align}
\begin{matrix*}[l]
    \textbf{minimize} & {\displaystyle \int_{t}^{t+T}  \left( \bm{x}^T \bm{Q} \bm{x} + \bm{u}^T \bm{R} \bm{u} \right) d\tau } \\
    \hfill \textbf{s.t.} & \left. \begin{matrix*}[l] 
    	\dot{\bm{x}}(\tau) = \bm{A}\bm{x}(\tau) + \bm{B}\bm{u}(\tau)  \\ 
    	\bm{u}(\tau) = \pi(\bm{x}(t), \cdots, \bm{x}(\tau))
    \end{matrix*} \right\} \forall \tau \in [t, t+T]
\end{matrix*}
.\end{align}

This definition basically states that the optimization to be performed is similar to the standard case, but, since the time horizon is now a moving window of size $T$, the gain matrices $\bm{K} : \mathbb{R} \rightarrow \mathbb{R}^{r \times n}$ changes together with time. This is a consequence for the fact the the Ricatti differential equation has to be solved backwards in time for each new horizon at each new time instance. Of course, this means that for each which time interval that is optimized, only the first action is actually applied to the system. The Fig. \ref{fig:rhc01} illustrates this control strategy.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{chapter5/recedingHorizon}
	\caption{Illustration of the Receding Horizon Control strategy.}
	\label{fig:rhc01}
\end{figure}

From the Separation Principle, it is always possible to increment this approach by introducing a optimal state estimator in a receding horizon fashion, as well. In this case, similarly to the controller, the optimal estimator gains are obtained for the entire horizon, but only the first gain is used to correct the state, before the horizon is updated.

\textbf{[properties?]}

\section{Linear Parameter-Varying Models}

The optimal controllers discussed so far suffers from a performance disadvantage when regarding the control of nonlinear systems. As shown in Theorem \ref{th:linearization}, a nonlinear system can be approximated by a linear model around a specific steady-state point. The problem, however, is that the optimization accounts for a nominal linear system that, in the case of a linearized model, will not represent the real dynamics of the nonlinear system as its states deviates from the steady-state value used. For this reason, the LQR solution will not provide an optimal solution in the light of the real system.

In this section, a solution to the nonlinear problematic is presented through an iterative linearization process, where the full dynamics are approximated by a series of linear models obtained for each time instance. This approach is referred in literature as Linear Switched Systems \textbf{[references]}. Before any further discussion, first consider the following definition.

\begin{boxed-definition}{(Linear Parameter-Varying System)} \label{def:lpv}
	A \textit{Linear Parameter-Varying} (LPV) State-Space representation describing a system with state vector $\bm{x} : \mathbb{R} \rightarrow \mathbb{R}^{n}$, output vector $\bm{y} : \mathbb{R} \rightarrow \mathbb{R}^{p}$ and input vector $\bm{u} : \mathbb{R} \rightarrow \mathbb{R}^{r}$ is given by the system of equations: 
    \begin{align} \label{eq:SSRepr04}
    \begin{cases}
        \dot{\bm{x}}(t) = \bm{A}(\bm{\theta}(t)) \bm{x}(t) + \bm{B}(\bm{\theta}(t)) \bm{u}(t) & \\
        \bm{y}(t) = \bm{C}(\bm{\theta}(t)) \bm{x}(t) + \bm{D}(\bm{\theta}(t)) \bm{u}(t) &
    \end{cases}
    ,\end{align}

    \noindent where $\bm{A}(\cdot) \in \mathbb{R}^{n \times n}$, $\bm{B}(\cdot) \in \mathbb{R}^{n \times r}$, $\bm{C}(\cdot) \in \mathbb{R}^{p \times n}$ and $\bm{D}(\cdot) \in \mathbb{R}^{p \times r}$ are appropriate functions of the parameter function $\bm{\theta} : \mathbb{R} \rightarrow \mathbb{R}^{z}$. 
\end{boxed-definition}

The LPV State-Space model is a linear realization of a system that can change with time through the result of a function $\bm{\theta}(\cdot)$. The first discussions on this representation for dynamical models were motivated from the Gain Scheduling control method \textbf{[references]}, where it serves as an linear embedding of the nonlinear model. From Theorem \ref{th:linearization} it is clear that a possible formulation for such a model is given by: 
\begin{equation}
\begin{cases}
        \Delta \dot{\bm{x}}(t)) =\left. \cfrac{\partial \bm{f}}{\partial \bm{x}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{x}(t) + \left. \cfrac{\partial \bm{f}}{\partial \bm{u}}\right\vert_{\bm{x}_o,  \bm{u}_o}  \Delta \bm{u}(t) \\ \\
        \hfill \bm{y}(t) = \left. \cfrac{\partial \bm{g}}{\partial \bm{x}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{x}(t) + \left. \cfrac{\partial \bm{g}}{\partial \bm{u}}\right\vert_{\bm{x}_o, \bm{u}_o} \Delta \bm{u}(t) \\
\end{cases}
,\end{equation}

\noindent in which case it is possible to set $\bm{\theta}(t) = \begin{bmatrix} \bm{x}(t-\delta t), \bm{u}(t-\delta t) \end{bmatrix}^T$ and denote the Jacobians as:
\begin{align} \label{eq:lpvJacobians}
\begin{matrix*}[l]
        \bm{A}(\bm{x}(t-\delta t), \bm{u}(t-\delta t)) = \left. \cfrac{\partial \bm{f}}{\partial \bm{x}}\right\vert_{\bm{x}(t-\delta t), \bm{u}(t-\delta t)}; & \bm{B}(\bm{x}(t-\delta t), \bm{u}(t-\delta t)) = \left. \cfrac{\partial \bm{f}}{\partial \bm{u}}\right\vert_{\bm{x}(t-\delta t), \bm{u}(t-\delta t)}; \\ \\ \bm{C}(\bm{x}(t-\delta t), \bm{u}(t-\delta t)) = \left. \cfrac{\partial \bm{g}}{\partial \bm{x}}\right\vert_{\bm{x}(t-\delta t), \bm{u}(t-\delta t)} ; & \bm{D}(\bm{x}(t-\delta t), \bm{u}(t-\delta t)) = \left. \cfrac{\partial \bm{g}}{\partial \bm{u}}\right\vert_{\bm{x}(t-\delta t), \bm{u}(t-\delta t)} 
\end{matrix*}
\end{align}

The formulation in \eqref{eq:lpvJacobians} might not be a perfect linearization of the system, since the values $\bm{x}(t)$ and $\bm{u}(t)$ are not constrained to yield $\bm{f}(\bm{x}(t), \bm{u}(t))$, i.e., they may be non-stationary points of the system. In the case that $\delta t > 0$, these matrices functions represents a causal switching law for linear systems \textbf{[reference]}, and the LPV system tends to a nonlinear system as $\delta t \to 0$. For some families of state and input trajectories it is possible to guarantee stability and other asymptotic system properties \textbf{[reference]}, which are heavily based on Lyapunov's theory \textbf{[reference]}. 

Consider the linearization Jacobians stated in \eqref{eq:isoReact01}. In the case that only the two first states are modelled, a LPV formulation for that system is given as:
\begin{align}   \label{eq:isoReactLPV}
\left\{ \begin{matrix*}[l]
    \Delta \dot{\bm{x}}(t) = \begin{bmatrix}
        -\theta^{(1)} - K_{AB} - 2K_{AC} \bm{\theta}^{(2)}_1 & 0 \\ K_{AB} & - \theta^{(1)} - K_{BC} 
    \end{bmatrix} \Delta \bm{x}(t) +  \begin{bmatrix}
        \rho_{in}^{(A)} - \bm{\theta}^{(2)}_1 \\ - \bm{\theta}^{(2)}_2
    \end{bmatrix} \Delta u(t) \\ \\
    \phantom{\Delta} \bm{y}(t) = \begin{bmatrix}
        1 & 0  \\ 0 & 1 
    \end{bmatrix} \Delta \bm{x}(t) + \begin{bmatrix}
        0 \\ 0 
    \end{bmatrix} \Delta u(t)
\end{matrix*} \right.
,\end{align}

\noindent where $\theta^{(1)} = u(t-\delta t)$ and $\bm{\theta}^{(2)} = \bm{x}(t-\delta t)$. Notice that the matrix $\bm{A}(\cdot)$ in this case should always result in a stable system, since the variables in the diagonal are constrained to be positive, given their physical meaning. A simulation is shown in Fig. \ref{fig:lpvSim01} for this system, given a sequence of different amplitude steps as inputs, to visualize how well this system response can approxima	te the nonlinear response.

[fig]

\section{Receding Horizon Control for Switched Systems}

Combining the receding horizon strategy together with the linear switching approach, it is possible to develop a method to controlling nonlinear systems by optimizing over a series of linearized systems. The receding horizon, in this case, becomes a necessity since the optimization has to be repeated every time that the matrices $\bm{A}(\cdot)$ and $\bm{B}(\cdot)$ changes. A formal definition of such controller is given below.

\begin{boxed-definition}{(RHC for LPV Systems)} \label{def:recedingHorizon}
	Considers a linear parameter-varying system in State-Space form, a control horizon of size $T \in \mathbb{R}^+$ and a switching time distance $\delta t$. The receding horizon control strategy for an optimal controller of this switched system is represented as:
	\begin{equation}
        \dot{\bm{x}}(t) = \left( \bm{A}(\bm{\theta}(t)) - \bm{B}(\bm{\theta}(t)) \bm{K}(t) \right) \bm{x}(t)
    ,\end{equation} 
    
	\noindent where $\bm{\theta}(t) = \begin{bmatrix} \bm{x}(t - \delta t), \bm{u}(t - \delta t) \end{bmatrix}$ is the parameter function and $\bm{K}(t) = - \bm{R}^{-1} \bm{B}^T(\bm{\theta}(t)) \bm{P}(t)$ represents the optimal gain for an iterative time interval $[t, t+T]$, given $\bm{P}(t)$ which solves the the Riccati differential equation:
    \begin{equation}
        -\dot{\bm{P}}(t) = \bm{A}^T(\bm{\theta}(t)) \bm{P}(t) + \bm{P}(t) \bm{A}(\bm{\theta}(t)) - \bm{P}(t) \bm{B}(\bm{\theta}(t)) \bm{R}^{-1} \bm{B}^T(\bm{\theta}(t)) \bm{P}(t) + \bm{Q}
    ,\end{equation}
    
    \noindent with terminal condition $\bm{P}(t+T) = \bm{Q}_f$.
\end{boxed-definition}

A simulation of controlling the system from \eqref{eq:isoReactLPV} is shown at Fig. \ref{fig:rhcLPV01}, demonstrating the quality of this approach to directly control the nonlinear system. For this configuration, the transient characteristics of the response can then be tuned by the design of $\bm{Q}$, $\bm{R}$ and the time parameters $\bm{T}$ and $\delta t$. 

[fig]

As stated earlier, it is also possible to apply this approach to stochastic systems, which are modelled as the nominal State-Space models perturbed by white Gaussian noise. The main difference is that the switching is done through the estimated states, making the approximation to the nonlinear response dependent on the estimator capabilities. A simulation of such configuration is shown in Fig. \ref{fig:rhcLPV02}. Notice how the action of noise made the closed-loop system exhibits some pseudo-periodic behavior, while still being able to follow the reference signal.

[fig]

% 6 - Methodology
% ---------------------------------------------------------------
\clearpage
\chapter{Methodology}

This chapter details the methodology used in order to attest the discussion presented until now. The experiments were performed in a simulated environment, using the MATLAB software \textbf{[reference]} as the framework to compute such simulations. The results obtained are presented and discussed in the next chapter.   

\section{Non-Isothermal Continuous Stirred Tank}

The chemical reactor used for the experiments was the non-isothermal Continuous Stirred Tank (CSTR) presented by \textbf{[reference-Engel]}. This system is a realization of a class of systems that characterize a wide range of industrial applications, hence being considered a classical benchmark for reactive systems. In particular, the system proposed for the experiment is a multiple-input multiple-output (MIMO) system which is highly nonlinear, with non-minimum phase behavior and unmeasurable states, thus makes it very challenging to control \textbf{[reference]}. This system is comprised by a tank containing a dilute solution of cyclopentadiene ($\textbf{C}_5 \textbf{H}_6$), which suffers reactions together with water molecules and together with the side-products of these reactor. The reaction scheme follows the same Van de Vusse scheme explored throughout this report:
\begin{align}
\left\{\ \begin{matrix*}[l]
	\overbrace{\textbf{C}_5 \textbf{H}_6}^{A} & \overset{K_1(T)}{\longrightarrow} & \overbrace{\textbf{C}_5 \textbf{H}_7 \textbf{OH}}^{B} & \overset{K_2(T)}{\longrightarrow} & \overbrace{\textbf{C}_5 \textbf{H}_8 (\textbf{OH})_2}^{C} \\ 
	\overbrace{2 \textbf{C}_5 \textbf{H}_6}^{2A} & \overset{K_3(T)}{\longrightarrow} & \overbrace{\textbf{C}_10 \textbf{H}_12}^{D}
\end{matrix*} \right.
.\end{align}

Since the reaction is non-isothermal, the kinetics rates $K_1(T)$, $K_2(T)$ and $K_3(T)$ are function of the temperature, and they are assumed to follow the Arrhenius equation from \eqref{eq:arrhenius1}. In order to control the temperature, ensuring some safety constraints to the process, a coolant system is coupled to the tank reactor to perform heat exchange by conduction. This scenario assumes that only the concentration of chemical $\textbf{C}_5 \textbf{H}_7 \textbf{OH}$, $\rho_B$ and the temperature inside the tank can be measured. Additionally, the liquid inflow into the tank carries only the chemical $\textbf{C}_5 \textbf{H}_6$ with a concentration $\rho^{(A)}_{in}$ and temperature $T_{in}$. The schematic of this operation is shown in Fig. \ref{fig:methodology01}. 

\begin{figure}[ht] \centering
	\includegraphics[scale=0.75]{chapter6/exoCSTR}
	\caption{The non-isothermal continuous stirred tank reactor proposed.}
	\label{fig:methodology01}
\end{figure}

A nonlinear dynamical model, which was obtained from the first principles approach, is proposed to represent the evolution of the important states:
\begin{align} \label{eq:engellModel}
\begin{cases}
	\hfill \dot{\rho_A} &= q (\rho^{(A)}_{in} - \rho_A) - \left( K_1(T) \rho_A + K_3(T) \rho^2_A  \right) \\
	\hfill \dot{\rho_B} &= - q \rho_B + K_1(T) \rho_A - K_2(T) \rho_B \\
	\hfill \dot{T} &= q(T_{in} - T) + \cfrac{k_W A_r}{\varrho C_p V_r} (T_C - T) \\ & \ \ \ \ \ \ -\cfrac{1}{\varrho C_p} \left( K_1(T) \rho_A \Delta H_{AB} + K_2(T) \rho_B \Delta H_{BC} + K_1(T) \rho_A^2 \Delta H_{AC} \right) \\
	\hfill \dot{T_C} &= \cfrac{1}{m_K C_{pK}} Q + \cfrac{k_W A_r}{m_K C_{pK}} (T - T_C) 
\end{cases}
.\end{align}

The meaning and value of each parameter is shown in Table \ref{table:exo_parameters}. Additionally, the real system has some constraints in respect to the manipulated variables, namely:
\begin{equation}
\begin{matrix}
	5 hr^{-1} \leq F \leq 35 h^{-1}; & -8500 \cfrac{kJ}{h} \leq Q \leq 0 \cfrac{kJ}{h}
\end{matrix}
.\end{equation}

\begin{table}[ht]
  \centering
	\begin{tabular}{l | c | c l }
	\textbf{System Variable} & \textbf{Symbol} & \textbf{Value} & \textbf{Unit} \\
	\hline
	Inflow concentration of $A$ & $\rho^{(A)}_{in}$ 	& $5.1$		& $\text{mol}/l$ \\
	Inflow liquid temperature & $T_{in}$ 	& $130$		& $º C$ \\
	Nominal kinetic rate for $A \rightarrow B$ & $K_{10}$	& $(1.287)\times 10^{12}$	& $h^{-1}$ \\
	Nominal kinetic rate for $B \rightarrow D$ & $K_{20}$	& $(1.287)\times 10^{12}$	& $h^{-1}$ \\
	Nominal kinetic rate for $2A \rightarrow D$ & $K_{30}$	& $(9.043)\times 10^{9}$	& $l\  \text{mol}^{-1} h $ \\
	Activation energy for $A \rightarrow B$ & $E_{1}/R$	& $9758.3$	& - \\
	Activation energy for $B \rightarrow C$ & $E_{2}/R$	& $9758.3$	& - \\
	Activation energy for $2A \rightarrow D$ & $E_{3}/R$	& $8560.0$	& - \\
	Enthalpy for $A \rightarrow B$ & $\Delta H_{AB}$	& $4.2$	& $kJ/mol$ \\
	Enthalpy for $B \rightarrow C$ & $\Delta H_{BC}$	& $-11.0$	& $kJ/mol$ \\
	Enthalpy for $2A \rightarrow D$ & $\Delta H_{AD}$	& $-41.85$	& $kJ/mol$ \\
	Total density of the reactor solution & $\varrho$ & $(0.9342)\times 10^{-4}$ & $kg/l$ \\
	Heat capacity of the solution & $C_p$ & $3.01$ & $kJ\ kg^{-1} K$ \\
	Heat capacity of the coolant & $C_{pK}$ & $2.0$ & $kJ\ kg^{-1} K^{-1}$ \\
	Surface area of the tank reactor & $A_R$ & $0.215$ & $m^2$ \\
	Volume of the tank reactor & $V_R$ & $10.01$ & $m^3$ \\
	Coolant mass & $m_K$ & $5.0$ & $kg$ \\
	Heat transfer coefficient & $k_W$ & $4032.0$ & $kJ\ h^{-1} m^2 K$ \\
  \end{tabular} 
  \caption{Model parameters and variable nomenclature.}
  \label{table:exo_parameters} 
\end{table} \vskip0.25cm


\section{Simulation}

The process and all the control structures were simulated using the MATLAB software. In the first part of the experiments, the process was analyzed in respect to the following characteristics and properties of its dynamical models:
\begin{enumerate}
	\item Nonlinear model natural and forced response simulation;
	\item Linearized State-Space model structure and response simulation;
	\item Analysis of the response in time domain;
	\item Assessment of the stability, controllability and observability properties;
\end{enumerate}

In respect to the control, the second part of the experiments focused on the design of controllers using the optimal control techniques. The controllers were developed and analyzed in the same order that they were presented in this document:
\begin{enumerate}
	\item Linear Quadratic Regulator, assuming a nonzero initial state and perfect measurement of the states;
	\item Linear Quadratic Regulator with Integral Action, for a non-constant reference signals and assuming perfect measurement of the states;
	\item Linear Quadratic Gaussian, assuming a nonzero initial state and reconstruction of the state-vector through observations from a stochastic linear system;
	\item Linear Quadratic Gaussian with Integral Action, for a non-constant reference and assuming a reconstruction of the state-vector through observations from a stochastic linear system;
\end{enumerate}

The simulation results are mainly analyzed through graphical visualizations of the data obtained. The experiments were focused on validating the modeling and control procedures for the system presented, instead of comparing and ranking the controllers based on the most performing one. For the sake of completeness, however, some metrics are used to evaluate each controller performance and relate them to the visualizations. The metrics used were the \textit{Integral of Squared Error} (ISE), \textit{Integral of Absolute Error} (IAE), \textit{Integral of Time-Weighted Squared Error} (ITSE) and \textit{Integral of Time-Weighted Absolute Error} (ITAE), which are given as:
\begin{align}
\begin{matrix*}[l] \displaystyle
	\text{IAE} = \displaystyle\int_{t_0}^{T} | \bm{r}(\tau) - \bm{y}(\tau) | d\tau; & & \text{ITAE} = \displaystyle\int_{t_0}^{T} \tau | \bm{r}(\tau) - \bm{y}(\tau) | d\tau;\\ \\
	\text{ISE} = \displaystyle\int_{t_0}^{T} \left[ \bm{r}(\tau) - \bm{y}(\tau) \right]^2 d\tau; & & \text{ITSE} = \displaystyle\int_{t_0}^{T} \tau \left[ \bm{r}(\tau) - \bm{y}(\tau) \right]^2 d\tau.
\end{matrix*}
\end{align}


% 7 - Results and Discussion
% ---------------------------------------------------------------
\clearpage
\chapter{Results and Discussion}

a

\section{Dynamical Analysis}

\subsection{Nonlinear model simulation}

The behavior of the process is accessed directly by simulating the nonlinear model presented at \eqref{eq:engellModel}. The main goal is to observe how the system responds to variations in each of its manipulated variables. In the first simulation, shown in Fig. \ref{fig:dynamics01}, the process is started at the steady-state condition, and then a pulse is applied to the liquid inflow to the tank. The response in each system is shown at the right side of the figure, where the states relating concentrations and the ones relating temperatures are grouped in different plots. The dashed lines represents the response of for compounds $C$ and $D$, which are not given by the proposed model, but can easily be equated following the first-principles approach.

\begin{figure}[ht] \centering
	\begin{tikzpicture}	[node distance=0.7cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/dynamics01}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=5cm, minimum width=0.91\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_A$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_B$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.9290, 0.6940, 0.1250};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_C$};		
		\draw[-, dashed, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.4940, 0.1840, 0.5560};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_D$};		
		\draw[-, dashed, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.6350, 0.0780, 0.1840};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.3010, 0.7450, 0.9330};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T_c$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	

	\caption{Non-linear simulation showing the manipulated variables (left) and correspondent response of the system variables (right) for a pulse change in the input flow-rate.}
	\label{fig:dynamics01}
\end{figure}

The simulation shows that both compounds $A$ and $B$ grows together when there is an increase in the input flow-rate, in which case the first exhibits a greater response than the other. Of course, this conclusion is direct from the fact that the input carries concentrations of this very same chemical. In respect to the temperatures, notice that, since the process is non-isothermal, the temperatures raised together with the increase in the reagents concentrations. Given that the temperature of the liquid inflow is actually lower than the tank temperature at the steady-state, it is possible to relate the increase in the temperature directly to the occurrence of the chemical reactions. This fact characterizes the process as a exothermic process, as it can be also noted from the first-principles model.

The second simulation was performed similarly, but applying a negative pulse to the jacket cooling capacity. The result is shown in Fig. \ref{fig:dynamics02}. As expected, the change in the cooling capacity affects the temperatures inside the reactor tank and inside the cooling jacket. The visualization also shows that the change in the temperature slightly changed the concentration of the substances inside the reactor. Therefore, it is possible to consider an indirect effect of this manipulated variable to these system variables.

\begin{figure}[ht] \centering
		\begin{tikzpicture}	[node distance=0.7cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/dynamics02}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=5cm, minimum width=0.91\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_A$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_B$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.9290, 0.6940, 0.1250};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_C$};		
		\draw[-, dashed, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.4940, 0.1840, 0.5560};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_D$};		
		\draw[-, dashed, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.6350, 0.0780, 0.1840};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.3010, 0.7450, 0.9330};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T_c$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	
	
	\caption{Non-linear simulation showing the manipulated variables (left) and correspondent response of the system variables (right) for a pulse change in the cooling capacity.}
	\label{fig:dynamics02}
\end{figure}

Now, the goal was to visualize if it is possible to increase the concentration of the chemicals through the reactions, while maintaining the tank temperature close to the steady-state value. For this reason, the last simulation considers a pulse to increase the input flow-rate while another pulse increases the magnitude of the cooling capacity. The result of this simulation is shown at Fig. \ref{fig:dynamics03}. Note that the tank temperature remained restricted to values closer to the steady-state point, whereas the concentrations were increased by the flow-rate input. Therefore, the possibility of increasing the production of a chemical without breaking safety constraints holds.

\begin{figure}[ht] \centering
		\begin{tikzpicture}	[node distance=0.7cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/dynamics03}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=5cm, minimum width=0.91\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_A$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_B$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.9290, 0.6940, 0.1250};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_C$};		
		\draw[-, dashed, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.4940, 0.1840, 0.5560};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_D$};		
		\draw[-, dashed, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.6350, 0.0780, 0.1840};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.3010, 0.7450, 0.9330};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T_c$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	
	
	\caption{Non-linear simulation showing the manipulated variables (left) and correspondent response of the system variables (right) for a change in the input flow-rate and cooling capacity.}
	\label{fig:dynamics03}
\end{figure}

\subsection{Linearized model realization and simulation}

The discussion throughout the document focused on control design methods for linear systems. For this reason, the nonlinear model can not directly be used to develop the controllers, and a linearization (Theorem \ref{th:linearization}) of this model is required. First of all, consider the following choice for the state, input and output variables:
\begin{equation}
\begin{matrix}
	\bm{x}(t) = \begin{bmatrix}
		\rho_A(t) \\ 
		\rho_B(t) \\ 
		T(t) \\ 
		T_c(t)
	\end{bmatrix}; & \bm{u}(t) = \begin{bmatrix}
		q(t) \\ 
		Q(t) \\
	\end{bmatrix}; & \bm{y}(t) = \begin{bmatrix}
		\rho_B(t) \\ 
		T(t)
	\end{bmatrix}
\end{matrix}
.\end{equation}

By evaluating the derivative of each differential state equation in relation to each state variable, the obtained Jacobian is

\begin{equation} \label{eq:exoLinSS01}
	\bm{A} = \begin{bmatrix}
		-u_{1o} - K_1(x_{3o}) - 2K_3(x_{3o}) x_{1o} & 0 & \cfrac{K_1(x_{3o})}{d x_{3o}} x_{1e} + \cfrac{K_3(x_{3o})}{d x_{3o}} x_{1o}^2 & 0 \\ \\
		K_1(x_{3o}) 					 & -u_{1o} - K_2(x_{3o}) & \cfrac{K_1(x_{3o})}{d x_{3o}} x_{1e} \cfrac{K_2(x_{3o})}{d x_{3o}} x_{2e} & 0 \\ \\
		\cfrac{\partial H(x_{1o},x_{2o},x_{3o})}{\partial x_{1o}} & \cfrac{\partial H(x_{1o},x_{2o},x_{3o})}{\partial x_{2o}} & -u_{1e} - \alpha + \cfrac{\partial H(x_{1o},x_{2o},x_{3o})}{\partial x_{3o}} & \alpha \\ \\
		0 & 0 & \beta & -\beta \\
	\end{bmatrix}
,\end{equation} 

\noindent where the parameters $\alpha$ and $\beta$ are given by
\begin{equation}
\begin{matrix}
	\alpha = \cfrac{k_W A_r}{\varrho C_p V_r}; & \beta = \cfrac{k_W A_r}{m_K C_{pK}}
\end{matrix}
,\end{equation}

\noindent and the derivatives are calculated as
\begin{equation}
\begin{matrix*}[l]
	\cfrac{d K_i}{d x_{3o}} = \cfrac{E_1/R}{(x_{3o} + 273.15)^2} K_{i0} e^{\frac{-E_i / R}{x_{3o} + 273.15}}; \\ 
	\cfrac{\partial H}{\partial x_{1o}} = \cfrac{-1}{m_K C_{pK}} \left( K_1(x_{3o}) \Delta H_{AB} + 2 K_3(x_{3o}) x_{1o} \Delta H_{AD} \right); \\ \cfrac{\partial H}{\partial x_{2o}} = \cfrac{-1}{m_K C_{pK}} \left( K_2(x_{3o}) \Delta H_{BC} \right); \\
	\cfrac{\partial H}{\partial x_{3o}} = \cfrac{-1}{m_K C_{pK}} \left( \cfrac{d K_1}{d x_{3o}} x_{1o} \Delta H_{AB} + \cfrac{d K_2}{d x_{3o}} x_{2o} \Delta H_{BC} + \cfrac{d K_3}{d x_{3o}} x^2_{1o} \Delta H_{AC} \right).
\end{matrix*}
\end{equation}

Similarly, the derivative of each differential equation in relation to each input variable is evaluated, which results in the simpler Jacobian
\begin{equation} \label{eq:exoLinSS02}
	\bm{B} = \begin{bmatrix}
		\rho^{(A)}_{in} - x_{1o} & 0 \\
		-x_{2o}			& 0 \\
		T_{in} - x_{3o}    & 0 \\
		0 			    & \cfrac{1}{m_K C_{pK}}
	\end{bmatrix}
.\end{equation} 

Regarding the matrices $\bm{C}$ and $\bm{D}$, recall that only the variables $\rho_B$ and $T$ are measured in the process. The measurement, however, is assumed to be perfect, so that these matrices can be defined as
\begin{equation} \label{eq:exoLinSS03}
\begin{matrix}
	\bm{C} = \begin{bmatrix}
		0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0
	\end{bmatrix}; & \bm{D} = \bm{0}.
\end{matrix}
\end{equation} 

The linear model presented in \eqref{eq:exoLinSS01} is an approximation of the nonlinear model for any point in the state space, given that it is a steady-state point. To obtain feasible linearization points, the differential equation system is evaluated for all points where $\dot{\rho_A} = \dot{\rho_B} =\dot{T} =\dot{T_c} = 0$. To solve this system, the input variables are selected to be the interval defined by the physical variable restrictions. The result is shown in Fig. \ref{fig:ssValues} for the two measured variables. In this work, it is assumed the steady-state point given by $\bm{x}_o = [1.235, 0.90, 134.14, 128.95]^T$ and $\bm{x}_o = [18.83, -4495.7]^T$, indicated in the figure by a white cross.

\begin{figure}[ht] \centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{chapter7/ssValues01}
	\end{subfigure} \hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{chapter7/ssValues02}
	\end{subfigure}
	
	\caption{Visualization of the steady-state values for the linearized model.}
	\label{fig:ssValues}
\end{figure}

Using the steady-state values just mentioned, a realization for the linearized model is obtained:
\begin{align}   \label{eq:exoReact01}
\left\{ \begin{matrix*}[l]
    \bm{A} = \begin{bmatrix*}[r]
		  -86.0962 &        0 &  -4.2010 &        0 \\
		   50.6146 & -69.4446 &   0.9958 &        0 \\
		  172.2263 & 197.9985 & -36.7597 &  30.7977 \\
		         0 &       0  &  86.6880 & -86.6880
    \end{bmatrix*} \hfill & &
    \bm{B} = \begin{bmatrix*}[r]
		 3.865  &       0 \\
		-0.900  &       0 \\
		-4.140  &       0 \\
		      0  &  0.100
    \end{bmatrix*} \hfill \\  \\ 
    \bm{C} = \begin{bmatrix}
        0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0
    \end{bmatrix} \hfill & &
    \bm{D} = \begin{bmatrix}
        0 & 0 \\ 0 & 0 \\ 0 & 0 \\ 0 & 0
    \end{bmatrix} \hfill
\end{matrix*} \right.
.\end{align}

In order to demonstrate that the linear model is a valid approximation of the nonlinear model, both systems were simulated in parallel for the same input signal. The simulation, as shown in Fig. \ref{fig:linear01}, starts at the steady-state values and then apply a sinusoidal change in the input to drag the system away from this initial point. As expected, the results shows that the linearized model is an accurate representation of the system, except when it is far from the steady-state values.

\begin{figure}[ht] \centering
		\begin{tikzpicture}	[node distance=1cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/linear01}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=5.5cm, minimum width=0.7\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_A$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$\rho_B$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.6350, 0.0780, 0.1840};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.3010, 0.7450, 0.9330};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$T_c$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	
	
	\caption{Simulation of both the nonlinear and linearized State-Space models. The nonlinear response is represented by the dashed lines.}
	\label{fig:linear01}
\end{figure}

%a
%
%\begin{figure}[ht] \centering
%	\includegraphics[width=\textwidth]{chapter7/matrixExp01}
%	\caption{a}
%	\label{fig:matrixExp01}
%\end{figure}	


\subsection{Linearized model properties}

Considering a State-Space realization, it is possible to access the system properties. First of all, the BIBO stability of the system was determined through the poles location in the complex plane. In the case of \eqref{eq:exoReact01}, the eigenvalues of $\bm{A}$ were calculated as $\bm{\lambda} = [-16.79, -54.84, -86.33, -121.01]$, which are all in right-hand side of the plane, i.e., $\text{Re}[\lambda_i]$, $\forall i \in [1,4]$. It is possible to conclude that, in the light of this model, the system is stable and does not require a special design for stabilizing controllers.

The next property to be accessed consists in the controllability of the system. Using the definition, the controllability matrix of this model was calculated as:
\begin{equation}
	\bm{\mathcal{C}} = \begin{bmatrix*}[r]
	 3.8650  &      0 & -315.3696  &       0   &  24465 & -12.9 & 2271  \\
   -0.9000   &      0 &  254.0027  &       0   & -32964 & 3.1  & -1250 \\
   -4.1400  &      0  &  639.6409  &  3.0798   & -38589 & -380.2  & 43720 \\
         0  &  0.1000 & -358.8883  &   -8.6688 &  86561 & 1018.5  & -121250 
	\end{bmatrix*}
.\end{equation}

\noindent This matrix has a row rank equal to 4, which is exactly the number of state variables. Therefore, it is concluded that this system is indeed controllable, meaning that it is possible to drive the system to any position in the space using a specific input signal. It is worth mentioning that this matrix exhibits some entries with high values, which can become numerically unstable in the case of augmenting the system.

Moreover, the last property of the system to be accessed concerns the observability of the system. Similarly to the controllability case, the observability matrix was calculated as:
\begin{equation}
	\bm{\mathcal{O}} = \begin{bmatrix*}[r]
         0 &   1.0000 &        0  &       0 \\	
         0 &        0 &   1.0000  &       0 \\ 
   50.6146 & -69.4446 &   0.9958  &       0 \\
  172.2263 & 197.9985 & -36.7597  & 30.7977 \\
   -7701.1 &   5019.7 &  -318.4  &  30.7 \\
   -11137 &  -21028 &   3495 &  -3802 \\
   862270  & -411630  &  51710  & -1246 \\
   496400  &  2152200 &  -432200  &  437200
	\end{bmatrix*}
.\end{equation}

\noindent This matrix has a column rank equal to 4, which, again, is exactly the number of state variables. Thus, it is possible to conclude that the system is observable, meaning that the information of any state can be reconstructed given only the outputs. 

\section{Optimal Control}

\subsection{Linear Quadratic Regulator}

The results for the controllers arrangements are now presented. The first controller simulated was the Linear Quadratic Regulator (LQR), as shown in Fig. \ref{fig:lqr01} for 4 different realizations of this controller. The controllers differs only on the choice of the weights $Q$ and $R$. In these simulations, the system was initiated at a non-zero initial state, specifically, at $20\%$ above the first and $5\%$ below the second steady-state value. The goal was to visualize how the controllers change the dynamic responses of the systems as they drive it to the zero-state reference. 

The simulations shows that the choice of weights can produce very distinct closed-loop systems. In the case of controller $LQR_4$, for instance, the 

\begin{figure}[ht] \centering
	\begin{tikzpicture}	[node distance=1cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/lqr01}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=5.4cm, minimum width=0.7\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_1$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_2$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.9290, 0.6940, 0.1250};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_3$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.4940, 0.1840, 0.5560};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_4$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	
	
	\caption{a}
	\label{fig:lqr01}
\end{figure}

a

\begin{figure}[ht] \centering
	\begin{tikzpicture}	[node distance=1cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/lqr02}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=4.3cm, minimum width=0.7\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_1$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_2$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.9290, 0.6940, 0.1250};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_3$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.4940, 0.1840, 0.5560};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQR_4$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	
	
	\caption{a}
	\label{fig:lqr02}
\end{figure}	

\subsection{Linear Quadratic Servo}

a

\begin{figure}[ht] \centering
	\begin{tikzpicture}	[node distance=1cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/lqri01}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=5.4cm, minimum width=0.7\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_1$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_2$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.9290, 0.6940, 0.1250};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_3$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.4940, 0.1840, 0.5560};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_4$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	
	\caption{a}
	\label{fig:lqri01}
\end{figure}

a

\begin{figure}[ht] \centering
	\begin{tikzpicture}	[node distance=1cm]
		\node (plot) {\includegraphics[width=\textwidth]{chapter7/lqri02}};
		
		\begin{scope}[on background layer]
			\node[block, above of=plot, xshift=0.65cm, node distance=4.3cm, minimum width=0.7\textwidth] (legendBox) {};
		\end{scope}
		
		\definecolor{linecolor}{rgb}{0, 0.4470, 0.7410};
		\node at (legendBox.west) [xshift=0.6cm]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_1$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.8500,0.3250,0.0980};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_2$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.9290, 0.6940, 0.1250};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_3$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
		
		\definecolor{linecolor}{rgb}{0.4940, 0.1840, 0.5560};
		\node[right of=label]  (legendStart) {};
		\node[right of=legendStart, node distance=1.5cm] (label) {$LQRI_4$};		
		\draw[-, linecolor, line width=1.5pt] (legendStart) -- (label);
	\end{tikzpicture}	
	
	\caption{a}
	\label{fig:lqri02}
\end{figure}	

\subsection{Linear Quadratic Gaussian}

a

\begin{figure}[ht] \centering
	\includegraphics[width=\textwidth]{chapter7/lqg01}
	\caption{a}
	\label{fig:lqg01}
\end{figure}

a

\begin{figure}[ht] \centering
	\includegraphics[width=\textwidth]{chapter7/lqg02}
	\caption{a}
	\label{fig:lqg02}
\end{figure}	

a

\begin{figure}[ht] \centering
	\includegraphics[width=\textwidth]{chapter7/lqg03}
	\caption{a}
	\label{fig:lqg03}
\end{figure}

a

\begin{figure}[ht] \centering
	\includegraphics[width=\textwidth]{chapter7/lqg04}
	\caption{a}
	\label{fig:lqg04}
\end{figure}	


\subsection{Linear Quadratic Gaussian Servo}

a

\begin{figure}[ht] \centering
	\includegraphics[width=\textwidth]{chapter7/lqgi01}
	\caption{a}
	\label{fig:lqgi01}
\end{figure}

a

\begin{figure}[ht] \centering
	\includegraphics[width=\textwidth]{chapter7/lqgi02}
	\caption{a}
	\label{fig:lqgi02}
\end{figure}	

a


% 8 - Conclusion
% ---------------------------------------------------------------
\clearpage
\chapter{Conclusion}

a

% Bibliography
% ---------------------------------------------------------------
\clearpage
\addcontentsline{toc}{chapter}{Bibliography}

\bibliographystyle{apalike}
\bibliography{citations}

% A - Apêndice A
% ---------------------------------------------------------------
\clearpage
\addcontentsline{toc}{chapter}{Appendix A - Proof of Theorems}
\chapter*{Appendix A - Proof of Theorems}

\section*{Chapter 2}

\begin{proof}{\textbf{(Theorem \ref{th:exoReactSys01})}}
    Considering that the reactions obeys the Arrhenius equation, the dynamical model for the non-isothermal reactor is a direct application of Theorem \ref{th:isoReactSys01} when assuming that the kinetics are in the form:
    \begin{equation}
        K_{XY} = K_{XY}(T) = K_{XY} e^{-\frac{E_{XY}}{T}} 
    ,\end{equation}
    
    \noindent with $T$ being the temperature in Kelvins and $E_{XY}$ being the activation energy needed for a reaction $X \rightarrow Y$ to occur.

    In respect to the change in temperatures, the model is obtained by a analysis of the conservation of heat, an energy that obeys this principle: 
    \begin{equation} \label{eq:jacket01}
        \begin{pmatrix}
            \text{Accumulation} \\ \text{of heat}
        \end{pmatrix} = \begin{pmatrix}
            \text{Heat entering} \\ \text{the System}
        \end{pmatrix} - \begin{pmatrix}
            \text{Heat leaving} \\ \text{the System}
        \end{pmatrix}
    .\end{equation}
    
    Consider the temperature inside the reactor. Since the system is closed, and involved by the heating/cooling system, this quantity is a result of a change in heat given by the flow of fluid entering and leaving the system, the direct transfer of heat by conduction from the contact with the cooling/heating system and the entropy contribution from the reactions. In this case, the conservation law becomes:  
    \begin{equation}
    \begin{split}
        \begin{pmatrix}
            \text{Accumulation} \\ \text{of heat}
        \end{pmatrix} &= \begin{pmatrix}
            \text{Heat entering} \\ \text{the System}
        \end{pmatrix} - \begin{pmatrix}
            \text{Heat leaving} \\ \text{the System}
        \end{pmatrix} \\
         &= \begin{pmatrix}
            \text{Heat transfer} \\ \text{from mass-flow}
        \end{pmatrix} + \begin{pmatrix}
            \text{Heat transfer} \\ \text{from regulator}
        \end{pmatrix} + \begin{pmatrix}
            \text{Entropy} \\ \text{contribution} \\ \text{from reactions}
        \end{pmatrix}
    \end{split}
    .\end{equation}
    
    From Fourier's law \textbf{[seek reference]}, the transfer of heat by convection between the fluids, $h_{conv}$ , and by the conduction between the systems, $h_{cond}$, obeys the proportionality:
    \begin{equation}
        \begin{matrix}
            h_{conv} \sim T_{in} - T_{out} & & h_{cond} \sim T_C - T
        \end{matrix}
    .\end{equation}      
    
    The entropy contribution from a reaction $\alpha X \rightarrow \beta Y$, denoted as $S_{XY}$, is proportional to the concentration of $\rho_X$ consumed multiplied by the energy that it liberates or absorbs:
    \begin{equation}
        S_{XY} \sim K_{XY} e^{-\frac{E_{XY}}{T}} (\rho_X)^{\alpha} \Delta H_{XY}
    .\end{equation}

    All the proportionality can be turned into equalities by imposing real constant factors that are calculated without the dynamical variables. In the case of the heat transfer by convection, the change in temperature is directly given by the ratio of volume entering the system. Plugging all together, and summing the entropy contribution from each reaction, the  accumulation of heat is modeled as:
    \begin{equation}
        \cfrac{d(T)}{dt} = q(T_{in} - T_{out}) + \eta (T_C - T) + \delta \sum_{\alpha A \rightarrow \beta X} K_{AX} e^{-\frac{E_{AX}}{T}} (\rho_A)^{\alpha} \Delta H_{AX}
    .\end{equation}
    
    Consider the heating/cooling system involving the reactor system. From the choice of design of this apparatus, it is possible to directly manipulate the temperature of its material using a cooling/heating capacity $Q$ up to a real factor of $\gamma$. Similar to the temperature inside the container, there is the conduction of heat between the healing/cooling system and the walls of the container for the reactor. Therefore, the model of heat accumulation for this quantity is given by:
    \begin{equation}
        \cfrac{d(T_C)}{dt} = \gamma Q + \beta (T - T_C)
    .\end{equation}
\end{proof}



\begin{proof}{\textbf{(Theorem \ref{th:transFun01})}}
    Applying the Laplace transform in both sides of the equation, and using some properties of this operator, the result is:

    \begin{equation} 
    \begin{split}
        \mathcal{L} \left\{\alpha_n \cfrac{d^n y(t)}{dt^n} + \cdots + \alpha_{1} \cfrac{d y(t)}{dt} + \alpha_{0} y(t) \right\}  &= \mathcal{L} \left\{ \beta_m \cfrac{d^m u(t)}{dt^m} + \cdots + \beta_{1} \cfrac{d u(t)}{dt} + \beta_{0} u(t) \right \}  \\
        \alpha_0 Y(s) + \sum_{i=1}^{n} \alpha_i \left(s^i Y(s) - \sum^{i-1}_{j = 0} s^j \cfrac{d^j y(0^-)}{dt^j} \right) &= \beta_0 U(s) + \sum_{k=1}^{m} \beta_i \left(s^i U(s) - \sum^{k-1}_{l = 0} s^l \cfrac{d^l u(0^-)}{dt^l} \right)
    \end{split}
    .\end{equation}
    
    By the assumption that all the initial conditions are zero, $y(0^-) = u(0^-) = 0$: 
    \begin{equation} 
        \alpha_0 Y(s) + \sum_{i=1}^{n} \alpha_i s^i Y(s) = \beta_0 U(s) + \sum_{k=1}^{m} \beta_i s^i U(s)
    .\end{equation}
    
    Factoring the left side by $\bm{Y}(s)$ and the right side by $\bm{U}(s)$, and doing some basic algebra, the transfer function $G(s)$, between these variables is:
    \begin{equation} 
     G(s) = \cfrac{Y(s)}{U(s)} = \cfrac{\beta_m s^m + \beta_{m-1} s^{m-1} + \cdots + \beta_{1} s + \beta_0}{\alpha_n s^n + \alpha_{n-1} s^{n-1} + \cdots + \alpha_{1} s + \alpha_0}
    .\end{equation}
\end{proof}

% ---------------------------------------------------------------
% End document
% ---------------------------------------------------------------

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Drafts:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Figure:
% \begin{figure}[ht]
%   \centering
%   \includegraphics[trim={0cm 0cm 0cm 0cm},clip,scale=1]{nameFigure}
%   \caption{Caption of the figure.}
%   \label{fig:nameFigure}
% \end{figure} \vskip0.25cm
%
%%%%% Equation:
% \begin{equation} \label{eq:nameEquation}
% \begin{split}
%    X = 1 + 1
% \end{split}
% \end{equation} \vskip0.25cm
%
%%%% Table:
% \begin{table}[hp]
%   \centering
%   \begin{tabular}{l | c c }
%   Principal & Coluna1 & Coluna2 \\
%   \hline 
%   ABC & 1 & 2 \\
%   DFG & 3 & 4 \\
%   HIJ & 5 & 6 \\
%   \end{tabular} 
%   \caption{Caption of the table.}
%   \label{table:nameTable} 
% \end{table} \vskip0.25cm

%\begin{figure}
%    \centering
%    \begin{minipage}[t][6cm][t]{.48\textwidth}
%	\begin{tikzpicture}
%	  \begin{axis}[enlargelimits=false, scale only axis, axis on top, width=0.8\textwidth,
%	  	xlabel={$u$ unemployment}, ylabel={$\pi$ inflation}]]
%		\addplot graphics [xmin=0,xmax=96,ymin=0,ymax=96] {chapter3/report_ch3_1_1};
%	  \end{axis}
%	 \end{tikzpicture}
%	\end{minipage}%
%	\hfill
%	\begin{minipage}[t][6cm][t]{0.48\textwidth}
%	\begin{tikzpicture}
%	  \begin{axis}[enlargelimits=false, scale only axis, axis on top, width=0.8\textwidth,
%	  	ylabel={$\pi$ inflation}]]
%		\addplot graphics [xmin=0,xmax=96,ymin=0,ymax=96] {chapter3/report_ch3_1_2};
%	  \end{axis}
%	\end{tikzpicture}%
%	\vspace{.6ex}
%	\begin{tikzpicture}
%		\begin{axis}[enlargelimits=false, scale only axis, axis on top, width=0.8\textwidth,
%	  	xlabel={$u$ unemployment}, ylabel={$\pi$ inflation}]]
%		\addplot graphics [xmin=0,xmax=96,ymin=0,ymax=96] {chapter3/report_ch3_1_3};
%		\end{axis}
%	\end{tikzpicture}
%	\end{minipage}%	
%    
%    \caption{A}
%    \label{fig:regulator01}
%\end{figure}